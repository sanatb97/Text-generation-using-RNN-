$ Generating Text with Recurrent Neural Networks Ilya Sutskever ILYA@CS UTORONTO CA James Martens JMARTENS@CS TORONTO EDU Geoffrey Hinton HINTON@CS TORONTO EDU University of Toronto, 6 King's College Rd , Toronto, ON M5S 3G4 CANADA Abstract Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly  #
$ Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems  #
$ In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks  #
$ The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or "gated") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next  #
$ After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling  a hierarchical non-parametric sequence model  #
$ To our knowledge this represents the largest recurrent neural network application to date  #
$ Introduction Recurrent Neural Networks (RNNs) form an expressive model family for sequence tasks  #
$ They are powerful because they have a high-dimensional hidden state with non-linear dynamics that enable them to remember and process past information  #
$ Furthermore, the gradients of the RNN are cheap to compute with backpropagation through time  #
$ Despite their attractive qualities, RNNs failed to become a Appearing in Proceedings of the 28th International Conference on Machine Learning, Bellevue, WA, USA, 2011  #
$ Copyright 2011 by the author(s)/owner(s)  #
$ mainstream tool in machine learning due to the difficulty of training them effectively  #
$ The cause of this difficulty is the very unstable relationship between the parameters and the dynamics of the hidden states, which manifests itself in the "vanishing/exploding gradients problem" (Bengio et al , 1994)  #
$ As a result, there has been surprisingly little research on standard RNNs in the last 20 years, and only a few successful applications using large RNNs (Robinson, 2002; Pollastri et al , 2002), including a recent notable application of RNNs as a word-level language model (Mikolov et al , 2010)  #
$ Recently, Martens (2010) developed a greatly improved variant of Hessian-Free optimization (HF) which was powerful enough to train very deep neural networks from random initializations  #
$ Since an RNN can be viewed as an extremely deep neural network with weight sharing across time, the same HF optimizer should be able to train RNNs  #
$ Fortunately, Martens & Sutskever (2011) were able to show that this is indeed the case, and that this form of non-diagonal, 2nd-order optimization provides a principled solution to the vanishing gradients problem in RNNs  #
$ Moreover, with the addition of a novel damping mechanism, Martens & Sutskever (2011) showed that the HF optimizer is robust enough to train RNNs, both on pathological synthetic datasets known to be impossible to learn with gradient descent, and on complex and diverse realworld sequence datasets  #
$ The goal of the paper is to demonstrate the power of large RNNs trained with the new Hessian-Free optimizer by applying them to the task of predicting the next character in a stream of text  #
$ This is an important problem because a better character-level language model could improve compression of text files (Rissanen & Langdon, 1979) and make it easier for people with physical disabilities to interact with computers (Ward et al , 2000)  #
$ More speculatively, achieving the asymptotic limit in text compression requires an understanding that is "equivalent to intelligence" (Hutter, 2006)  #
$ Good compression can be achieved by exploiting simple regularities such as the vocabulary and the syntax of the relevant languages and the shallow associations exemplified by the fact that the word "milk" often occurs soon after the word "cow", but beyond a certain point any improvement in performance must result from a deeper understanding of the text's meaning  #
$ Although standard RNNs are very expressive, we found that achieving competitive results on character-level language modeling required the development of a different type of RNN that was better suited to our application  #
$ This new "MRNN" architecture uses multiplicative connections to allow the current input character to determine the hidden-to-hidden weight matrix  #
$ We trained MRNNs on over a hundred of megabytes of text for several days using 8 Graphics Processing Units in parallel to perform significantly better than one of the best word-agnostic single character-level language models: the sequence memoizer (Wood et al , 2009; Gasthaus et al , 2010), which is a hierarchical nonparametric Bayesian method  #
$ It defines a prior process on the set of predictions at every conceivable context, with judiciously chosen details that make approximate inference computationally tractable  #
$ The memoizer induces dependencies between its predictions by making similar predictions at similar contexts  #
$ Although intelligent marginalization techniques are able to eliminate all but a relatively small number of the random variables (so the datastructures used scale linearly with the amount of data), its memory requirements are still prohibitively expensive for large datasets, which is a direct consequence of its nonparametric nature  #
$ While our method performs at the state of the art for pure character-level models, its compression performance falls short of the best models which have explicit knowledge of words, the most powerful of these being PAQ8hp12 (Mahoney, 2005)  #
$ PAQ is a mixture model of a large number of well-chosen context models whose mixing proportions are computed by a neural network whose weights are a function of the current context, and whose predictions are further combined with a neural-network like model  #
$ Unlike standard compression techniques, some of PAQ's context models not only consider contiguous contexts but also contexts with "gaps", allowing it to capture some types of longer range structures cheaply  #
$ More significantly, PAQ is not word-agnostic, because it uses a combination of character-level and word-level models  #
$ PAQ also preprocesses the data with a dictionary of common English words which we disabled, because it gave PAQ an unfair advantage over models that do not use such task-specific (and indeed, English-specific) explicit prior knowledge  #
$ The numerous mixture components of PAQ were chosen because they improved performance on a development set, so in this respect PAQ is similar in model complexity to the winning entry of the netflix prize (Bell et al , 2007)  #
$Figure 1. A Recurrent Neural Network is a very deep feedforward neural network whose weights are shared across time  #
$ The nonlinear activation function used by the hidden units is the source of the RNN's rich dynamics  #
$ Finally, language models can be used to "generate"  language, and to our surprise, the text generated by the MRNNs we trained exhibited a significant amount of interesting and high-level linguistic structure, featuring a large vocabulary, a considerable amount of grammatical structure, and a wide variety of highly plausible proper names that were not in the training set  #
$ Mastering the vocabulary of English did not seem to be a problem for the MRNN: it generated very few uncapitalized non-words, and those that it did generate were often very plausible, like "homosomalist" or "unameliary"  #
$ Of particular interest was the fact that the MRNN learned to balance parentheses and quotes over long distances (e g , 30 characters)  #
$ A character-level N-gram language model could only do this by modeling 31-grams, and neither Memoizer nor PAQ are representationally capable of balancing parentheses because of their need for exact context matches  #
$ In contrast, the MRNN's nonlinear dynamics enables it to extract higher level "knowledge" from the text, and there are no obvious limits to its representational power because of the ability of its hidden states to perform general computation  #
$ Recurrent Neural Networks A Recurrent Neural Network is a straightforward adaptation of the standard feed-forward neural network to allow it to model sequential data  #
$ At each timestep, the RNN receives an input, updates its hidden state, and makes a prediction (fig  #
$ 1)  #
$ The RNN's high dimensional hidden state and nonlinear evolution endow it with great expressive power, enabling the hidden state of the RNN to integrate information over many timesteps and use it to make accurate predictions  #
$ Even if the non-linearity used by each unit is quite simple, iterating it over time leads to very rich dynamics  #
$ The standard RNN is formalized as follows: Given a sequence of input vectors (x1, xT ), the RNN computes a sequence of hidden states (h1, hT ) and a sequence of outputs (o1,oT ) by iterating the following equations for t = 1 to T: ht = tanh(Whxxt + Whhht-1 + bh) (1) ot = Wohht + bo (2) In these equations, Whx is the input-to-hidden weight matrix, Whh is the hidden-to-hidden (or recurrent) weight matrix, Woh is the hidden-to-output weight matrix, and the vectors bh and bo are the biases  #
$ The undefined expression Whhht-1 at time t = 1 is replaced with a special initial bias vector, hinit, and the tanh nonlinearity is applied coordinate-wise  #
$ The gradients of the RNN are easy to compute via backpropagation through time (Rumelhart et al , 1986; Werbos, 1990)1 , so it may seem that RNNs are easy to train with gradient descent  #
$ In reality, the relationship between the parameters and the dynamics of the RNN is highly unstable which makes gradient descent ineffective  #
$ This intuition was formalized by Hochreiter (1991) and Bengio et al  #
$ (1994) who proved that the gradient decays (or, less frequently, blows up) exponentially as it is backpropagated through time, and used this result to argue that RNNs cannot learn long-range temporal dependencies when gradient descent is used for training  #
$ In addition, the occasional tendency of the backpropagated gradient to exponentially blow-up greatly increases the variance of the gradients and makes learning very unstable  #
$ As gradient descent was the main algorithm used for training neural networks at the time, these theoretical results and the empirical difficulty of training RNNs led to the near abandonment of RNN research  #
$ One way to deal with the inability of gradient descent to learn long-range temporal structure in a standard RNN is to modify the model to include "memory" units that are specially designed to store information over long time periods  #
$ This approach is known as "Long-Short Term Memory" (Hochreiter & Schmidhuber, 1997) and has been successfully applied to complex real-world sequence modeling tasks (e g , Graves & Schmidhuber, 2009)  #
$ Long Short Term Memory makes it possible to handle datasets which require long-term memorization and recall but even on these datasets it is outperformed by using a standard RNN trained with the HF optimizer (Martens & Sutskever, 2011)  #
$ Another way to avoid the problems associated with backpropagation through time is the Echo State Network (Jaeger & Haas, 2004) which forgoes learning the recurrent connections altogether and only trains the non-recurrent output weights  #
$ This is a much easier learning task and it works surprisingly well provided the recurrent connections 1 In contrast, Dynamic Bayes Networks (Murphy, 2002), the probabilistic analogues of RNNs, do not have an efficient algorithm for computing their gradients  #
$ Figure 2  #
$ An illustration of the significance of the multiplicative connections (the product is depicted by a triangle)  #
$ The presence of the multiplicative connections enables the RNN to be sensitive to conjunctions of context and character, allowing different contexts to respond in a qualitatively different manner to the same input character  #
$ are carefully initialized so that the intrinsic dynamics of the network exhibits a rich reservoir of temporal behaviours that can be selectively coupled to the output  #
$ The Multiplicative RNN Having applied a modestly-sized standard RNN architecture to the character-level language modeling problem (where the target output at each time step is defined as the the input character at the next time-step), we found the performance somewhat unsatisfactory, and that while increasing the dimensionality of the hidden state did help, the per-parameter gain in test performance was not sufficient to allow the method to be both practical and competitive with state-of-the-art approaches  #
$ We address this problem by proposing a new temporal architecture called the Multiplicative RNN (MRNN) which we will argue is better suited to the language modeling task  #
$ 3 1  #
$ The Tensor RNN The dynamics of the RNN's hidden states depend on the hidden-to-hidden matrix and on the inputs  #
$ In a standard RNN (as defined by eqs  #
$ 1-2), the current input xt is first transformed via the visible-to-hidden weight matrix Whx and then contributes additively to the input for the current hidden state  #
$ A more powerful way for the current input character to affect the hidden state dynamics would be to determine the entire hidden-to-hidden matrix (which defines the non-linear dynamics) in addition to providing an additive bias  #
$ One motivation for this approach came from viewing an RNN as a model of an unbounded tree in which each node is a hidden state vector and each edge is labelled by a character that determines how the parent node gives rise to the child node  #
$ This view emphasizes the resemblance of an RNN to a Markov model that stores familiar strings of characters in a tree, and it also makes it clear that the RNN tree is potentially much more powerful than the Markov model because the distributed representation of a node allows different nodes to share knowledge  #
$ For example, the character string "ing" is quite probable after "fix" and also quite probable after "break"  #
$ If the hidden state vectors that represent the two histories "fix" and "break" share a common representation of the fact that this could be the stem of a verb, then this common representation can be acted upon by the character "i" to produce a hidden state that predicts an "n"  #
$ For this to be a good prediction we require the conjunction of the verb-stem representation in the previous hidden state and the character "i"  #
$ One or other of these alone does not provide half as much evidence for predicting an "n": It is their conjunction that is important  #
$ This strongly suggests that we need a multiplicative interaction  #
$ To achieve this goal we modify the RNN so that its hidden-to-hidden weight matrix is a (learned) function of the current input xt: ht = tanh Whxxt + W (xt) hh ht-1 + bh (3) ot = Wohht + bo (4) These are identical to eqs  #
$ 1 and 2, except that Whh is replaced with W (xt) hh , allowing each character to specify a different hidden-to-hidden weight matrix  #
$ It is natural to define W (xt) hh using a tensor  #
$ If we store M matrices, W (1) hh ,   #
$ , W (M) hh , where M is the number of dimensions of xt, we could define W (xt) hh by the equation W (xt) hh = M m=1 x (m) t W (m) hh (5) where x (m) t is the m-th coordinate of xt  #
$ When the input xt is a 1-of-M encoding of a character, it is easily seen that every character has an associated weight matrix and W (xt) hh is the matrix assigned to the character represented by xt  #
$ 2 3 2  #
$ The Multiplicative RNN The above scheme, while appealing, has a major drawback: Fully general 3-way tensors are not practical because of their size  #
$ In particular, if we want to use RNNs with a large number of hidden units (say, 1000) and if the dimensionality of xt is even moderately large, then the storage required for the tensor W (xt) hh becomes prohibitive  #
$ It turns out we can remedy the above problem by factoring the tensor W (x) hh (e g , Taylor & Hinton, 2009)  #
$ This is done by introducing the three matrices Wfx, Whf , and Wfh, and reparameterizing the matrix W (xt) hh by the equation W (xt) hh = Whf  diag(Wfxxt)  Wfh (6) 2 The above model, applied to discrete inputs represented with their 1-of-M encodings, is the nonlinear version of the Observable Operator Model (OOM; Jaeger, 2000) whose linear nature makes it closely related to an HMM in terms of expressive power  #
$ Figure 3  #
$ The Multiplicative Recurrent Neural Network "gates" the recurrent weight matrix with the input symbol  #
$ Each triangle symbol represents a factor that applies a learned linear filter at each of its two input vertices  #
$ The product of the outputs of these two linear filters is then sent, via weighted connections, to all the units connected to the third vertex of the triangle  #
$ Consequently every input can synthesize its own hidden-to-hidden weight matrix by determining the gains on all of the factors, each of which represents a rank one hidden-to-hidden weight matrix defined by the outer-product of its incoming and outgoing weight-vectors to the hidden units  #
$ The synthesized weight matrices share "structure" because they are all formed by blending the same set of rank one matrices  #
$ In contrast, an unconstrained tensor model ensures that each input has a completely separate weight matrix  #
$ If the dimensionality of the vector Wfxxt, denoted by F, is sufficiently large, then the factorization is as expressive as the original tensor  #
$ Smaller values of F require fewer parameters while hopefully retaining a significant fraction of the tensor's expressive power  #
$ The Multiplicative RNN (MRNN) is the result of factorizing the Tensor RNN by expanding eq  #
$ 6 within eq  #
$ The MRNN computes the hidden state sequence (h1,   #
$ , hT ), an additional "factor state sequence" (f1,   #
$ , fT ), and the output sequence (o1,   #
$ , oT ) by iterating the equations ft = diag(Wfxxt)  Wfhht-1 (7) ht = tanh(Whf ft + Whxxt) (8) ot = Wohht + bo (9) which implement the neural network in fig  #
$ The tensor factorization of eq  #
$ 6 has the interpretation of an additional layer of multiplicative units between each pair of consecutive layers (i e , the triangles in fig  #
$ 3), so the MRNN actually has two steps of nonlinear processing in its hidden states for every input timestep  #
$ Each of the multiplicative units outputs the value ft of eq  #
$ 7 which is the product of the outputs of the two linear filters connecting the multiplicative unit to the previous hidden states and to the inputs  #
$ We experimentally verified the advantage of the MRNN over the RNN when the two have the same number of parameters  #
$ We trained an RNN with 500 hidden units and an MRNN with 350 hidden units and 350 factors (so the RNN has slightly more parameters) on the "machine learning" dataset (dataset 3 in the experimental section)  #
$ After extensive training, the MRNN achieved 1 56 bits per character and the RNN achieved 1 65 bits per character on the Generating Text with Recurrent Neural Networks test set  #
$ 3 3  #
$ The difficulty of learning multiplicative units In an MRNN, the effective weight W (c) ij 3 from hidden unit i to hidden unit j contributed by character c is given by: W (c) ij = f Wif WfcWfj (10) This product of parameters makes gradient descent learning difficult  #
$ If, for example, Wif is very small and Wfj is very large we get a very large deriviative for the very small weight and a very small derivative for the very large weight  #
$ Fortunately, this type of difficulty is exactly what second-order methods are good at handling, so multiplicative units should be better handled by a 2nd-order approach like the HF optimizer  #
$ The RNN as a Generative Model The goal of character-level language modeling is to predict the next character in a sequence  #
$ More formally, given a training sequence (x1,   #
$ , xT ), the RNN uses the sequence of its output vectors (o1,   #
$ , oT ) to obtain a sequence of predictive distributions P(xt+1|xt) = softmax(ot), where the softmax distribution is defined by P(softmax(ot) = j) = exp(o (j) t )/ k exp(o (k) t )  #
$ The language modeling objective is to maximize the total log probability of the training sequence T-1 t=0 log P(xt+1|xt), which implies that the RNN learns a probability distribution over sequences  #
$ Even though the hidden units are deterministic, we can sample from an MRNN stochastically because the states of its output units define the conditional distribution P(xt+1|xt)  #
$ We can sample from this conditional distribution to get the next character in a generated string and provide it as the next input to the RNN  #
$ This means that the RNN is a directed non-Markov model and, in this respect, it resembles the sequence memoizer (Wood et al , 2009)  #
$ The experiments The goal of our experiments is to demonstrate that the MRNN, when trained by HF, learns high-quality language models  #
$ We demonstrate this by comparing the MRNN to the sequence memoizer and to PAQ on three real-world language datasets  #
$ After splitting each dataset into a training and test set, we trained a large MRNN, a sequence memoizer4 , and PAQ, and report the bits per character (bpc) each model achieves on the test set  #
$ 3 We slightly abuse notation, using W (c) ij to stand for W (c) hh ij   #
$ 4 Which has no hyper-parameters and strictly speaking isn't 'trained' but rather conditioned the training set  #
$ Owing to its nonparametric nature and the nature of the data-structures it uses, the sequence memoizer is very memory intensive, so it can only be applied to training datasets of roughly 130MB on a machine with 32GB of RAM  #
$ In contrast, the MRNN can be applied to datasets of unlimited size although it typically requires considerably more total FLOPS to achieve good performance (but, unlike the memoizer, it is easily parallelized)  #
$ However, to make the experimental comparison fair, we train the MRNN, the memoizer, and PAQ on datasets of the same size  #
$ 5 1  #
$ The datasets We now describe the datasets  #
$ Each dataset is a long string of characters from an 86-character alphabet of about 100MB that includes digits and punctuation, together with a special symbol which indicates that the character in the original text was not one of the other 85 characters in our alphabet  #
$ The last 10 million characters of each dataset are used as a test set  #
$ The first dataset is a sequence of characters from the English Wikipedia  #
$ We removed the XML and the Wikipedia markup to clean the dataset  #
$ Since Wikipedia is extremely nonuniform, we randomly permuted its articles before partitioning it into a train and a test set  #
$ The second dataset is a collection of articles from the New York Times (Sandhaus, 2008)  #
$ The third dataset is a corpus of machine learning papers  #
$ We constructed this dataset by downloading every NIPS and JMLR paper, and converting them to plain text using the pdftotext utility  #
$ We then translated a large number of special characters to their ascii equivalents (including non-ascii punctuation, greek letters, and the "fi" and "if" symbol) to clean the dataset, and removed most of the unstructured text by using only sentences consisting of at least 70% alphanumeric characters  #
$ Finally, we randomly permuted the papers  #
$ The first two corpora are subsets of larger corpora (over 1GB large), but the semi-online nature of our optimizer makes it easy to train the MRNN on a dataset of any size  #
$ 5 2  #
$ Training details To compute the exact gradient of the log probability of the training set (eq  #
$ 4), the MRNN needs to process the entire training set sequentially and store the hidden state sequence in order to apply backpropagation-through-time  #
$ This is infeasible due to the size of the training set but it is also unnecessary: Training the MRNN on many shorter sequences is just as effective, provided they are several hundred characters or more long  #
$ If the sequences are too short, we fail to utilize the ability of the HF optimizer to capture long- #
$ This table shows the test bits per character for each ex- periment, with the training bits in brackets (where available)  #
$ The MRNN achieves lower bits per character than the sequence memoizer but higher than PAQ on each of the three datasets  #
$ The MRNN (full set) column refers to MRNNs trained on the larger (1GB) training corpora (except for the ML dataset which is not a subset of a larger corpus)  #
$ Note, also, that the improvement resulting from larger dataset is modest, implying that the an MRNN with 1500 units and factors is fairly well-trained with 100MB of text  #
$ DATA SET MEMOIZER PAQ MRNN MRNN (FULL SET) WIKI 1 66 1 51 1 60 (1 53) 1 55 (1 54) NYT 1 49 1 38 1 48 (1 44) 1 47 (1 46) ML 1 33 1 22 1 31 (1 27) term dependencies spanning hundreds of timesteps  #
$ An advantage of using a large number of relatively short sequences over using a single long sequence is that the former is much easier to parallelize  #
$ This is essential, since our preliminary experiments suggested that HF applied to MRNNs works best when the gradient is computed using millions of characters and the curvature-matrix vector products are computed using hundreds of thousands of characters  #
$ Using a highly parallel system (consisting of 8 high-end GPUs with 4GB of RAM each), we computed the gradient on 160300=48000 sequences of length 250, of which 8300=2400 sequences were used to compute the curvature-matrix vector products that are needed for the HF optimizer (Martens & Sutskever, 2011) (so each GPU processes 300 sequences at a time)  #
$ The first few characters of any sequence are much harder to predict because they do not have a sufficiently large context, so it is not beneficial to have the MRNN spend neural resources predicting these characters  #
$ We take this effect into account by having the MRNN predict only the last 200 timesteps of the 250-long training sequences, thus providing every prediction with at least 50 characters of context  #
$ The Hessian-Free optimizer (Martens, 2010) and its RNN-specialized variant (Martens & Sutskever, 2011) have a small number of meta-parameters that must be specified  #
$ We set the structural damping coefficient  to 0 1, and initialized  to 10 (see Martens & Sutskever (2011) for a description of these meta-parameters)  #
$ Our HF implementation uses a different subset of the training data at every iteration, so at a coarse temporal scale it is essentially online  #
$ In this setup, training lasted roughly 5 days for each dataset  #
$ We found that a total of 160150 weight updates was sufficient to adequately train an MRNN  #
$ More specifically, we used 160 steps of HF, with each of these steps using a maximum of 150 conjugate gradient iterations to approach the minimum of the quadratic Gauss-Newton-based approximation to the objective function, which remains fixed during the conjugate gradient iterations  #
$ The small number of weight updates, each requiring a massive amount of computation, makes the HF optimizer much easier to parallelize than stochastic gradient descent  #
$ In all our experiments we use MRNNs with 1500 hidden units and 1500 factors (F), which have 4,900,000 parameters  #
$ The MRNNs were initialized with sparse connections: each unit starts out with 15 nonzero connections to other units (see Martens & Sutskever, 2011)  #
$ Note that if we unroll the MRNN in time (as in fig  #
$ 3) we obtain a neural network with 500 layers of size 1500 if we view the multiplicative units ft as layers  #
$ This is arguably the deepest and largest neural network ever trained  #
$ 5 3  #
$ The results The main experimental results are shown in table 5 2  #
$ We see that the MRNN predicts the test set more accurately than the sequence memoizer but less accurately than the dictionary-free PAQ on the three datasets  #
$ 5 4  #
$ Debagging It is easy to convert a sentence into a bag of words, but it is much harder to convert a bag of words into a meaningful sentence  #
$ We name the latter the debagging problem  #
$ We perform an experiment where a character-level language model evaluates every possible ordering of the words in the bag, and returns and the ordering it deems best  #
$ To make the experiment tractable, we only considered bags of 7 words, giving a search space of size 5040  #
$ For our experiment, we used the MRNN and the memoizer5 to debag 500 bags of randomly chosen words from "Ana Karenina"  #
$ We use 11 words for each bag, where the first two and the last two words are used as context to aid debagging the middle seven words  #
$ We say that the model correctly debags a sentence if the correct ordering is assigned the highest log probability  #
$ We found that the wikipedia-trained MRNN recovered the correct ordering 34% of the time, and the wikipedia-trained memoizer did so 27% of the time  #
$ Given that the problem is "word-level", utilizing large character contexts is essential to achieve good performance  #
$ 5 We were unable to modify the implementation of PAQ to make debagging feasible  #
$ Generating Text with Recurrent Neural Networks 6  #
$ Qualitative experiments In this section we qualitatively investigate the nature of the models learned by the MRNN  #
$ 6 1  #
$ Samples from the models The simplest qualitative experiment is to inspect the samples generated by the three MRNNs  #
$ The most salient characteristic of the samples is the richness of their vocabularies  #
$ Further inspection reveals that the text is mostly grammatical, and that parentheses are usually balanced over many characters  #
$ The artifacts of the generated text, such as consecutive commas or quotes, are the result of the data preprocessing and are frequently found in the training set  #
$ 6 1 1  #
$ SAMPLES FROM THE WIKIPEDIA MODEL We now present a sample from the Wikipedia model  #
$ We use ? #
$ to indicate the "unknown" character  #
$ The sample below was obtained by running the MRNN less than 10 times and selecting the most intriguing sample  #
$ The beginning of the paragraph and the parentheses near the end are particularly interesting  #
$ The MRNN was initialized with the phrase "The meaning of life is": The meaning of life is the tradition of the ancient human reproduction: it is less favorable to the good boy for when to remove her bigger  #
$ In the show's agreement unanimously resurfaced  #
$ The wild pasteured with consistent street forests were incorporated by the 15th century BE  #
$ In 1996 the primary rapford undergoes an effort that the reserve conditioning, written into Jewish cities, sleepers to incorporate the  St Eurasia that activates the population  #
$ Mar? #
$ ?a Nationale, Kelli, Zedlat-Dukastoe, Florendon, Ptu's thought is  #
$ To adapt in most parts of North America, the dynamic fairy Dan please believes, the free speech are much related to the 6 1 2  #
$ SAMPLES FROM THE NYT MODEL Below is a sample from the model trained on the full NYT dataset, where the MRNN was initialized with a single space  #
$ The spaces surrounding the punctuation are an artifact of the preprocessing  #
$ while he was giving attention to the second advantage of school building a 2-for-2 stool killed by the Cultures saddled with a half- suit defending the Bharatiya Fernall 's office   #
$ Ms   #
$ Claire Parters will also have a history temple for him to raise jobs until naked Prodiena to paint baseball partners , provided people to ride both of Manhattan in 1978 , but what was largely directed to China in 1946 , focusing on the trademark period is the sailboat yesterday and comments on whom they obtain overheard within the 120th anniversary , where many civil rights defined , officials said early that forms , " said Bernard J  Marco Jr  of Pennsylvania , was monitoring New York 6 1 3  #
$ SAMPLES FORM THE ML MODEL Finally, we generate text from an MRNN trained on the ML corpus conditioned on the string "Recurrent"  #
$ This MRNN is also able to balance parentheses (e g , the third line of the sample): Recurrent network with the Stiefel information for logistic regression methods Along with either of the algorithms previously (two or more skewprecision) is more similar to the model with the same average mismatched graph  #
$ Though this task is to be studied under the reward transform, such as (c) and (C) from the training set, based on target activities for articles a ? #
$ 2(6) and (4 3)  #
$ The PHDPic (PDB) matrix of cav'va using the three relevant information contains for tieming measurements  #
$ Moreover, because of the therap tor, the aim is to improve the score to the best patch randomly, but for each initially four data sets  #
$ As shown in Figure 11, it is more than 100 steps, we used ?? #
$ \to \infty with 1000 6 2  #
$ Structured sentence completion In this section, we investigate the MRNN's response in various situations by sampling from the MRNN's distribution conditioned on a prefix  #
$ The goal is to see whether the MRNN is able to generate "plausible" continuations to the initial strings  #
$ In our first experiment, we use the Wikipedia MRNN to complete the string "England, Spain, France, Germany,": England, Spain, France, Germany, and Massachusetts  #
$ England, Spain, France, Germany, cars, and direct schools England, Spain, France, Germany, , or New Orleans and Uganda  #
$ England, Spain, France, Germany, , Westchester, Jet State, Springfield, Athleaves and Sorvinhee In the above completions, the MRNN correctly interpreted the string to be a list of locations, so the generated text was also a part of a list  #
$ Next, we performed a similar experiment using the ML model and the pair of strings "(ABC et al" and "ABC et al"  #
$ The system has never seen the string "(ABC et al" in its training set (simply because there is no machine learning author named ABC, and its capitalization is particularly uncommon for a citation), so the MRNN needed to generalize over an entirely new author name: (ABC et al , 2003), ?13?, and for a supervised Mann-Whitnaguing (ABC et al , 2002), based on Lebanon and Haussler, 1995b) ABC et al  #
$ (2003b), or Penalization of Information ABC et al  #
$ (2008) can be evaluated and motivated by providing optimal estimate This example shows that the MRNN is sensitive to the initial bracket before "ABC", illustrating its representational power  #
$ The above effect is extremely robust  #
$ In contrast, both N-gram models and the sequence memoizer cannot make such predictions unless these exact strings (e g , "(ABC et al , 2003)") occur in the training set, which cannot be counted on  #
$ In fact, any method which is based on precise context matches is fundamentally incapable of utilizing long contexts, because the probability that a long context occurs more than once is vanishingly small  #
$ We experimentally verified that neither the sequence memoizer Generating Text with Recurrent Neural Networks nor PAQ are not sensitive to the initial bracket  #
$ Discussion Modeling language at the character level seems unnecessarily difficult because we already know that morphemes are the appropriate units for making semantic and syntactic predictions  #
$ Converting large databases into sequences of morphemes, however, is non-trivial compared with treating them as character strings  #
$ Also, learning which character strings make words is a relatively easy task compared with discovering the subtleties of semantic and syntactic struc- ture  #
$ So, given a powerful learning system like an MRNN, the convenience of using characters may outweigh the extra work of having to learn the words All our experiments show that an MRNN finds it very easy to learn words  #
$ With the exception of proper names, the generated text contains very few non-words  #
$ At the same time, the MRNN also assigns probability to (and occasionally generates) plausible words that do not appear in the training set (e g , "cryptoliation", "homosomalist", or "unameliary")  #
$ This is a desirable property which enabled the MRNN to gracefully deal with real words that it nonetheless didn't see in the training set  #
$ Predicting the next word by making a sequence of character predictions avoids having to use a huge softmax over all known words and this is so advantageous that some word-level language models actually make up binary "spellings" of words so that they can predict them one bit at a time (Mnih & Hinton, 2009)  #
$ MRNNs already learn surprisingly good language models using only 1500 hidden units, and unlike other approaches such as the sequence memoizer and PAQ, they are easy to extend along various dimensions  #
$ If we could train much bigger MRNNs with millions of units and billions of connections, it is possible that brute force alone would be sufficient to achieve an even higher standard of performance  #
$ But this will of course require considerably more computational power  #






$ GloVe: Global Vectors for Word Representation Jeffrey Pennington, Richard Socher, Christopher D  Manning Computer Science Department, Stanford University, Stanford, CA 94305 jpennin@stanford edu, richard@socher org, manning@stanford edu Abstract Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque  #
$ We analyze and make explicit the model properties needed for such regularities to emerge in word vectors  #
$ The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods  #
$ Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus  #
$ The model produces a vector space with meaningful sub- structure, as evidenced by its performance of 75% on a recent word analogy task  #
$ It also outperforms related models on similarity tasks and named entity recognition  #
$ 1 Introduction Semantic vector space models of language represent each word with a real-valued vector  #
$ These vectors can be used as features in a variety of applications, such as information retrieval (Manning et al , 2008), document classification (Sebastiani, 2002), question answering (Tellex et al , 2003), named entity recognition (Turian et al , 2010), and parsing (Socher et al , 2013)  #
$ Most word vector methods rely on the distance or angle between pairs of word vectors as the primary method for evaluating the intrinsic quality of such a set of word representations  #
$ Recently, Mikolov et al  #
$ (2013c) introduced a new evaluation scheme based on word analogies that probes the finer structure of the word vector space by examining not the scalar distance between word vectors, but rather their various dimensions of difference  #
$ For example, the analogy "king is to queen as man is to woman" should be encoded in the vector space by the vector equation king - queen = man - woman  #
$ This evaluation scheme favors models that produce dimensions of meaning, thereby capturing the multi-clustering idea of distributed representations (Bengio, 2009)  #
$ The two main model families for learning word vectors are: 1) global matrix factorization methods, such as latent semantic analysis (LSA) (Deerwester et al , 1990) and 2) local context window methods, such as the skip-gram model of Mikolov et al  #
$ (2013c)  #
$ Currently, both families suffer significant drawbacks  #
$ While methods like LSA efficiently leverage statistical information, they do relatively poorly on the word analogy task, indicating a sub-optimal vector space structure  #
$ Methods like skip-gram may do better on the analogy task, but they poorly utilize the statistics of the corpus since they train on separate local context windows instead of on global co-occurrence counts  #
$ In this work, we analyze the model properties necessary to produce linear directions of meaning and argue that global log-bilinear regression models are appropriate for doing so  #
$ We propose a specific weighted least squares model that trains on global word-word co-occurrence counts and thus makes efficient use of statistics  #
$ The model produces a word vector space with meaningful sub-structure, as evidenced by its state-of-the-art performance of 75% accuracy on the word analogy dataset  #
$ We also demonstrate that our methods outperform other current methods on several word similarity tasks, and also on a common named entity recognition (NER) benchmark  #
$ We provide the source code for the model as well as trained word vectors at http://nlp  #
$ stanford edu/projects/glove/  #
$ 2 Related Work Matrix Factorization Methods  #
$ Matrix factorization methods for generating low-dimensional word representations have roots stretching as far back as LSA  #
$ These methods utilize low-rank approximations to decompose large matrices that capture statistical information about a corpus  #
$ The particular type of information captured by such matrices varies by application  #
$ In LSA, the matrices are of "term-document" type, i e , the rows correspond to words or terms, and the columns correspond to different documents in the corpus  #
$ In contrast, the Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), for example, utilizes matrices of "term-term" type, i e , the rows and columns correspond to words and the entries correspond to the number of times a given word occurs in the context of another given word  #
$ A main problem with HAL and related methods is that the most frequent words contribute a disproportionate amount to the similarity measure: the number of times two words co-occur with the or and, for example, will have a large effect on their similarity despite conveying relatively little about their semantic relatedness  #
$ A number of techniques exist that addresses this shortcoming of HAL, such as the COALS method (Rohde et al , 2006), in which the co-occurrence matrix is first transformed by an entropy-or correlation-based normalization  #
$ An advantage of this type of transformation is that the raw co-occurrence counts, which for a reasonably sized corpus might span 8 or 9 orders of magnitude, are compressed so as to be distributed more evenly in a smaller interval  #
$ A variety of newer models also pursue this approach, including a study (Bullinaria and Levy, 2007) that indicates that positive pointwise mutual information (PPMI) is a good transformation  #
$ More recently, a square root type transformation in the form of Hellinger PCA (HPCA) (Lebret and Collobert, 2014) has been suggested as an effective way of learning word representations  #
$ Shallow Window-Based Methods  #
$ Another approach is to learn word representations that aid in making predictions within local context windows  #
$ For example, Bengio et al  #
$ (2003) introduced a model that learns word vector representations as part of a simple neural network architecture for language modeling  #
$ Collobert and Weston (2008) decoupled the word vector training from the downstream training objectives, which paved the way for Collobert et al  #
$ (2011) to use the full context of a word for learning the word representations, rather than just the preceding context as is the case with language models  #
$ Recently, the importance of the full neural network structure for learning useful word representations has been called into question  #
$ The skip-gram and continuous bag-of-words (CBOW) models of Mikolov et al  #
$ (2013a) propose a simple single-layer architecture based on the inner product between two word vectors  #
$ Mnih and Kavukcuoglu (2013) also proposed closely-related vector log-bilinear models, vLBL and ivLBL, and Levy et al  #
$ (2014) proposed explicit word embeddings based on a PPMI metric  #
$ In the skip-gram and ivLBL models, the objective is to predict a word's context given the word itself, whereas the objective in the CBOW and vLBL models is to predict a word given its context  #
$ Through evaluation on a word analogy task, these models demonstrated the capacity to learn linguistic patterns as linear relationships between the word vectors  #
$ Unlike the matrix factorization methods, the shallow window-based methods suffer from the disadvantage that they do not operate directly on the co-occurrence statistics of the corpus  #
$ Instead, these models scan context windows across the entire corpus, which fails to take advantage of the vast amount of repetition in the data  #
$ 3 The GloVe Model The statistics of word occurrences in a corpus is the primary source of information available to all unsupervised methods for learning word representations, and although many such methods now exist, the question still remains as to how meaning is generated from these statistics, and how the resulting word vectors might represent that meaning  #
$ In this section, we shed some light on this question  #
$ We use our insights to construct a new model for word representation which we call GloVe, for Global Vectors, because the global corpus statistics are captured directly by the model  #
$ First we establish some notation  #
$ Let the matrix of word-word co-occurrence counts be denoted by X, whose entries Xi j tabulate the number of times word j occurs in the context of word i  #
$ Let Xi = k Xik be the number of times any word appears in the context of word i  #
$ Finally, let Pi j = P(j|i) = Xi j/Xi be the probability that word j appear in the Table 1: Co-occurrence probabilities for target words ice and steam with selected context words from a 6 billion token corpus  #
$ Only in the ratio does noise from non-discriminative words like water and fashion cancel out, so that large values (much greater than 1) correlate well with properties specific to ice, and small values (much less than 1) correlate well with properties specific of steam  #
$ Probability and Ratio k = solid k = gas k = water k = fashion P(k|ice) 1 9  10-4 6 6  10-5 3 0  10-3 1 7  10-5 P(k|steam) 2 2  10-5 7 8  10-4 2 2  10-3 1 8  10-5 P(k|ice)/P(k|steam) 8 9 8 5  10-2 1 36 0 96 context of word i  #
$ We begin with a simple example that showcases how certain aspects of meaning can be extracted directly from co-occurrence probabilities  #
$ Consider two words i and j that exhibit a particular aspect of interest; for concreteness, suppose we are interested in the concept of thermodynamic phase, for which we might take i = ice and j = steam  #
$ The relationship of these words can be examined by studying the ratio of their co-occurrence probabilities with various probe words, k  For words k related to ice but not steam, say k = solid, we expect the ratio Pik/Pjk will be large  #
$ Similarly, for words k related to steam but not ice, say k = gas, the ratio should be small  #
$ For words k like water or fashion, that are either related to both ice and steam, or to neither, the ratio should be close to one  #
$ Table 1 shows these probabilities and their ratios for a large corpus, and the numbers confirm these expectations  #
$ Compared to the raw probabilities, the ratio is better able to distinguish relevant words (solid and gas) from irrelevant words (water and fashion) and it is also better able to discriminate between the two relevant words  #
$ The above argument suggests that the appropriate starting point for word vector learning should be with ratios of co-occurrence probabilities rather than the probabilities themselves  #
$ Noting that the ratio Pik/Pjk depends on three words i, j, and k, the most general model takes the form, F(wi,wj, ~ wk ) = Pik Pjk , (1) where w  Rd are word vectors and ~ w  Rd are separate context word vectors whose role will be discussed in Section 4 2  #
$ In this equation, the right-hand side is extracted from the corpus, and F may depend on some as-of-yet unspecified parameters  #
$ The number of possibilities for F is vast, but by enforcing a few desiderata we can select a unique choice  #
$ First, we would like F to encode the information present the ratio Pik/Pjk in the word vector space  #
$ Since vector spaces are inherently linear structures, the most natural way to do this is with vector differences  #
$ With this aim, we can restrict our consideration to those functions F that depend only on the difference of the two target words, modifying Eqn  #
$ (1) to, F(wi - wj, ~ wk ) = Pik Pjk   #
$ (2) Next, we note that the arguments of F in Eqn  #
$ (2) are vectors while the right-hand side is a scalar  #
$ While F could be taken to be a complicated function parameterized by, e g , a neural network, doing so would obfuscate the linear structure we are trying to capture  #
$ To avoid this issue, we can first take the dot product of the arguments, F (wi - wj )T ~ wk = Pik Pjk , (3) which prevents F from mixing the vector dimensions in undesirable ways  #
$ Next, note that for word-word co-occurrence matrices, the distinction between a word and a context word is arbitrary and that we are free to exchange the two roles  #
$ To do so consistently, we must not only exchange w  ~ w but also X  XT   #
$ Our final model should be invariant under this relabeling, but Eqn  #
$ (3) is not  #
$ However, the symmetry can be restored in two steps  #
$ First, we require that F be a homomorphism between the groups (R,+) and (R>0, ), i e , F (wi - wj )T ~ wk = F(wT i ~ wk ) F(wT j ~ wk ) , (4) which, by Eqn  #
$ (3), is solved by, F(wT i ~ wk ) = Pik = Xik Xi   #
$ (5) The solution to Eqn  #
$ (4) is F = exp, or, wT i ~ wk = log(Pik ) = log(Xik ) - log(Xi )   #
$ (6) Next, we note that Eqn  #
$ (6) would exhibit the exchange symmetry if not for the log(Xi ) on the right-hand side  #
$ However, this term is independent of k so it can be absorbed into a bias bi for wi  #
$ Finally, adding an additional bias ~ bk for ~ wk restores the symmetry, wT i ~ wk + bi + ~ bk = log(Xik )   #
$ (7) Eqn  #
$ (7) is a drastic simplification over Eqn  #
$ (1), but it is actually ill-defined since the logarithm diverges whenever its argument is zero  #
$ One resolution to this issue is to include an additive shift in the logarithm, log(Xik )  log(1 + Xik ), which maintains the sparsity of X while avoiding the divergences  #
$ The idea of factorizing the log of the co-occurrence matrix is closely related to LSA and we will use the resulting model as a baseline in our experiments  #
$ A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never  #
$ Such rare co-occurrences are noisy and carry less information than the more frequent ones -- yet even just the zero entries account for 7595% of the data in X, depending on the vocabulary size and corpus  #
$ We propose a new weighted least squares regression model that addresses these problems  #
$ Casting Eqn  #
$ (7) as a least squares problem and introducing a weighting function f (Xi j ) into the cost function gives us the model J = V i, j=1 f Xi j wT i ~ wj + bi + ~ bj - log Xi j 2 , (8) where V is the size of the vocabulary  #
$ The weighting function should obey the following properties: 1  f (0) = 0  #
$ If f is viewed as a continuous function, it should vanish as x  0 fast enough that the limx0 f (x) log2 x is finite  #
$ 2  f (x) should be non-decreasing so that rare co-occurrences are not overweighted  #
$ 3  f (x) should be relatively small for large values of x, so that frequent co-occurrences are not overweighted  #
$ Of course a large number of functions satisfy these properties, but one class of functions that we found to work well can be parameterized as, f (x) = (x/xmax) if x < xmax 1 otherwise   #
$ (9) 0 2 0 4 0 6 0 8 1 0 0 0 Figure 1: Weighting function f with  = 3/4  #
$ The performance of the model depends weakly on the cutoff, which we fix to xmax = 100 for all our experiments  #
$ We found that  = 3/4 gives a modest improvement over a linear version with  = 1  #
$ Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolov et al , 2013a)  #
$ 3 1 Relationship to Other Models Because all unsupervised methods for learning word vectors are ultimately based on the occurrence statistics of a corpus, there should be com- monalities between the models  #
$ Nevertheless, certain models remain somewhat opaque in this regard, particularly the recent window-based methods like skip-gram and ivLBL  #
$ Therefore, in this subsection we show how these models are related to our proposed model, as defined in Eqn  #
$ (8)  #
$ The starting point for the skip-gram or ivLBL methods is a model Qi j for the probability that word j appears in the context of word i  #
$ For concreteness, let us assume that Qi j is a softmax, Qi j = exp(wT i ~ wj ) V k=1 exp(wT i ~ wk )   #
$ (10) Most of the details of these models are irrelevant for our purposes, aside from the the fact that they attempt to maximize the log probability as a context window scans over the corpus  #
$ Training proceeds in an online, stochastic fashion, but the implied global objective function can be written as, J = - icorpus jcontext(i) log Qi j   #
$ (11) Evaluating the normalization factor of the softmax for each term in this sum is costly  #
$ To allow for efficient training, the skip-gram and ivLBL models introduce approximations to Qi j  #
$ However, the sum in Eqn  #
$ (11) can be evaluated much more efficiently if we first group together those terms that have the same values for i and j, J = - V i=1 V j=1 Xi j log Qi j , (12) where we have used the fact that the number of like terms is given by the co-occurrence matrix X  Recalling our notation for Xi = k Xik and Pi j = Xi j/Xi, we can rewrite J as, J = - V i=1 Xi V j=1 Pi j log Qi j = V i=1 Xi H(Pi,Qi ) , (13) where H(Pi,Qi ) is the cross entropy of the distributions Pi and Qi, which we define in analogy to Xi  #
$ As a weighted sum of cross-entropy error, this objective bears some formal resemblance to the weighted least squares objective of Eqn  #
$ (8)  #
$ In fact, it is possible to optimize Eqn  #
$ (13) directly as opposed to the online training methods used in the skip-gram and ivLBL models  #
$ One could interpret this objective as a "global skip-gram" model, and it might be interesting to investigate further  #
$ On the other hand, Eqn  #
$ (13) exhibits a number of undesirable properties that ought to be addressed before adopting it as a model for learning word vectors  #
$ To begin, cross entropy error is just one among many possible distance measures between probability distributions, and it has the unfortunate property that distributions with long tails are often modeled poorly with too much weight given to the unlikely events  #
$ Furthermore, for the measure to be bounded it requires that the model distribution Q be properly normalized  #
$ This presents a computational bottleneck owing to the sum over the whole vocabulary in Eqn  #
$ (10), and it would be desirable to consider a different distance measure that did not require this property of Q  #
$ A natural choice would be a least squares objective in which normalization factors in Q and P are discarded, ^ J = i, j Xi ^ Pi j - ^ Qi j 2 (14) where ^ Pi j = Xi j and ^ Qi j = exp(wT i ~ wj ) are the unnormalized distributions  #
$ At this stage another problem emerges, namely that Xi j often takes very large values, which can complicate the optimization  #
$ An effective remedy is to minimize the squared error of the logarithms of ^ P and ^ Q instead, ^ J = i, j Xi log ^ Pi j - log ^ Qi j 2 = i, j Xi wT i ~ wj - log Xi j 2   #
$ (15) Finally, we observe that while the weighting factor Xi is preordained by the online training method inherent to the skip-gram and ivLBL models, it is by no means guaranteed to be optimal  #
$ In fact, Mikolov et al  #
$ (2013a) observe that performance can be increased by filtering the data so as to reduce the effective value of the weighting factor for frequent words  #
$ With this in mind, we introduce a more general weighting function, which we are free to take to depend on the context word as well  #
$ The result is, ^ J = i, j f (Xi j ) wT i ~ wj - log Xi j 2 , (16) which is equivalent1 to the cost function of Eqn  #
$ (8), which we derived previously  #
$ 3 2 Complexity of the model As can be seen from Eqn  #
$ (8) and the explicit form of the weighting function f (X), the computational complexity of the model depends on the number of nonzero elements in the matrix X  #
$ As this number is always less than the total number of entries of the matrix, the model scales no worse than O(|V |2)  #
$ At first glance this might seem like a substantial improvement over the shallow window-based approaches, which scale with the corpus size, |C|  #
$ However, typical vocabularies have hundreds of thousands of words, so that |V |2 can be in the hundreds of billions, which is actually much larger than most corpora  #
$ For this reason it is important to determine whether a tighter bound can be placed on the number of nonzero elements of X  #
$ In order to make any concrete statements about the number of nonzero elements in X, it is necessary to make some assumptions about the distribution of word co-occurrences  #
$ In particular, we will assume that the number of co-occurrences of word i with word j, Xi j, can be modeled as a power-law function of the frequency rank of that word pair, ri j: Xi j = k (ri j )   #
$ (17) 1We could also include bias terms in Eqn  #
$ (16)  #
$ The total number of words in the corpus is proportional to the sum over all elements of the co- occurrence matrix X, |C| i j Xi j = |X | r=1 k r = kH|X |, , (18) where we have rewritten the last sum in terms of the generalized harmonic number Hn,m  #
$ The upper limit of the sum, |X|, is the maximum frequency rank, which coincides with the number of nonzero elements in the matrix X  #
$ This number is also equal to the maximum value of r in Eqn  #
$ (17) such that Xi j  1, i e , |X| = k1/  #
$ Therefore we can write Eqn  #
$ (18) as, |C|  |X| H|X |,   #
$ (19) We are interested in how |X| is related to |C| when both numbers are large; therefore we are free to expand the right hand side of the equation for large |X|  #
$ For this purpose we use the expansion of generalized harmonic numbers (Apostol, 1976), Hx,s = x1-s 1 - s + (s) + O(x-s) if s > 0, s 1 , (20) giving, |C| |X| 1 - + () |X| + O(1) , (21) where (s) is the Riemann zeta function  #
$ In the limit that X is large, only one of the two terms on the right hand side of Eqn  #
$ (21) will be relevant, and which term that is depends on whether  > 1, |X| = O(|C|) if  < 1, O(|C|1/) if  > 1  #
$ (22) For the corpora studied in this article, we observe that Xi j is well-modeled by Eqn  #
$ (17) with  = 1 25  #
$ In this case we have that |X| = O(|C|0 8)  #
$ Therefore we conclude that the complexity of the model is much better than the worst case O(V2), and in fact it does somewhat better than the online window-based methods which scale like O(|C|)  #
$ 4 Experiments 4 1 Evaluation methods We conduct experiments on the word analogy task of Mikolov et al  #
$ (2013a), a variety of word similarity tasks, as described in (Luong et al , 2013), and on the CoNLL-2003 shared benchmark Table 2: Results on the word analogy task, given as percent accuracy  #
$ Underlined scores are best within groups of similarly-sized models; bold scores are best overall  #
$ HPCA vectors are publicly available2; (i)vLBL results are from (Mnih et al , 2013); skip-gram (SG) and CBOW results are from (Mikolov et al , 2013a,b); we trained SG and CBOW using the word2vec tool3  #
$ See text for details and a description of the SVD models  #
$ Model Dim  #
$ Size Sem  #
$ Syn  #
$ Tot  #
$ Word analogies  #
$ The word analogy task consists of questions like, "a is to b as c is to ?" #
$ The dataset contains 19,544 such questions, divided into a semantic subset and a syntactic subset  #
$ The semantic questions are typically analogies about people or places, like "Athens is to Greece as Berlin is to ?"  #
$ The syntactic questions are typically analogies about verb tenses or forms of adjectives, for example "dance is to dancing as fly is to ?"  #
$ To correctly answer the question, the model should uniquely identify the missing term, with only an exact correspondence counted as a correct match  #
$ We answer the question "a is to b as c is to ?" #
$ by finding the word d whose representation wd is closest to wb - wa + wc according to the cosine similarity 4 2http://lebret ch/words/ 3http://code google com/p/word2vec/ 4Levy et al  #
$ (2014) introduce a multiplicative analogy evaluation, 3COSMUL, and report an accuracy of 68 24% on 0 100 200 300 400 500 600 20 30 40 50 60 70 80 Vector Dimension Accuracy [%] Semantic Syntactic Overall (a) Symmetric context 2 4 6 8 10 40 50 55 60 65 70 45 Window Size Accuracy [%] Semantic Syntactic Overall (b) Symmetric context 2 4 6 8 10 40 50 55 60 65 70 45 Window Size Accuracy [%] Semantic Syntactic Overall (c) Asymmetric context Figure 2: Accuracy on the analogy task as function of vector size and window size/type  #
$ All models are trained on the 6 billion token corpus  #
$ In (a), the window size is 10  #
$ In (b) and (c), the vector size is 100  #
$ Word similarity  #
$ While the analogy task is our primary focus since it tests for interesting vector space substructures, we also evaluate our model on a variety of word similarity tasks in Table 3  #
$ These include WordSim-353 (Finkelstein et al , 2001), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), SCWS (Huang et al , 2012), and RW (Luong et al , 2013)  #
$ Named entity recognition  #
$ The CoNLL-2003 English benchmark dataset for NER is a collection of documents from Reuters newswire articles, annotated with four entity types: person, location, organization, and miscellaneous  #
$ We train models on CoNLL-03 training data on test on three datasets: 1) ConLL-03 testing data, 2) ACE Phase 2 (2001-02) and ACE-2003 data, and 3) MUC7 Formal Run test set  #
$ We adopt the BIO2 annota- tion standard, as well as all the preprocessing steps described in (Wang and Manning, 2013)  #
$ We use a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model (Finkel et al , 2005)  #
$ A total of 437,905 discrete features were generated for the CoNLL-2003 training dataset  #
$ In addition, 50-dimensional vectors for each word of a five-word context are added and used as continuous features  #
$ With these features as input, we trained a conditional random field (CRF) with exactly the same setup as the CRFjoin model of (Wang and Manning, 2013)  #
$ 4 2 Corpora and training details We trained our model on five corpora of varying sizes: a 2010 Wikipedia dump with 1 billion tokens; a 2014 Wikipedia dump with 1 6 billion tokens; Gigaword 5 which has 4 3 billion tokens; the combination Gigaword5 + Wikipedia2014, which the analogy task  #
$ This number is evaluated on a subset of the dataset so it is not included in Table 2  #
$ 3COSMUL performed worse than cosine similarity in almost all of our experiments  #
$ has 6 billion tokens; and on 42 billion tokens of web data, from Common Crawl5  #
$ We tokenize and lowercase each corpus with the Stanford tokenizer, build a vocabulary of the 400,000 most frequent words6, and then construct a matrix of co-occurrence counts X  #
$ In constructing X, we must choose how large the context window should be and whether to distinguish left context from right context  #
$ We explore the effect of these choices below  #
$ In all cases we use a decreasing weighting function, so that word pairs that are d words apart contribute 1/d to the total count  #
$ This is one way to account for the fact that very distant word pairs are expected to contain less relevant information about the words' relationship to one another  #
$ For all our experiments, we set xmax = 100, = 3/4, and train the model using AdaGrad (Duchi et al , 2011), stochastically sampling non-zero elements from X, with initial learning rate of 0 05  #
$ We run 50 iterations for vectors smaller than 300 dimensions, and 100 iterations otherwise (see Section 4 6 for more details about the convergence rate)  #
$ Unless otherwise noted, we use a context of ten words to the left and ten words to the right  #
$ The model generates two sets of word vectors, W and ~ W  When X is symmetric, W and ~ W are equivalent and differ only as a result of their random initializations; the two sets of vectors should perform equivalently  #
$ On the other hand, there is evidence that for certain types of neural networks, training multiple instances of the network and then combining the results can help reduce overfitting and noise and generally improve results (Ciresan et al , 2012)  #
$ With this in mind, we choose to use 5To demonstrate the scalability of the model, we also trained it on a much larger sixth corpus, containing 840 billion tokens of web data, but in this case we did not lowercase the vocabulary, so the results are not directly comparable  #
$ 6For the model trained on Common Crawl data, we use a larger vocabulary of about 2 million words  #
$ the sum W + ~ W as our word vectors  #
$ Doing so typically gives a small boost in performance, with the biggest increase in the semantic analogy task  #
$ We compare with the published results of a variety of state-of-the-art models, as well as with our own results produced using the word2vec tool and with several baselines using SVDs  #
$ With word2vec, we train the skip-gram (SG) and continuous bag-of-words (CBOW) models on the 6 billion token corpus (Wikipedia 2014 + Gigaword 5) with a vocabulary of the top 400,000 most frequent words and a context window size of 10  #
$ We used 10 negative samples, which we show in Section 4 6 to be a good choice for this corpus  #
$ For the SVD baselines, we generate a truncated matrix Xtrunc which retains the information of how frequently each word occurs with only the top 10,000 most frequent words  #
$ This step is typical of many matrix-factorization-based methods as the extra columns can contribute a disproportionate number of zero entries and the methods are otherwise computationally expensive  #
$ The singular vectors of this matrix constitute the baseline "SVD"  #
$ We also evaluate two related baselines: "SVD-S" in which we take the SVD of Xtrunc, and "SVD-L" in which we take the SVD of log(1+Xtrunc)  #
$ Both methods help compress the otherwise large range of values in X 7 4 3 Results We present results on the word analogy task in Table 2  #
$ The GloVe model performs significantly better than the other baselines, often with smaller vector sizes and smaller corpora  #
$ Our results using the word2vec tool are somewhat better than most of the previously published results  #
$ This is due to a number of factors, including our choice to use negative sampling (which typically works better than the hierarchical softmax), the number of negative samples, and the choice of the corpus  #
$ We demonstrate that the model can easily be trained on a large 42 billion token corpus, with a substantial corresponding performance boost  #
$ We note that increasing the corpus size does not guarantee improved results for other models, as can be seen by the decreased performance of the SVD- 7We also investigated several other weighting schemes for transforming X; what we report here performed best  #
$ Many weighting schemes like PPMI destroy the sparsity of X and therefore cannot feasibly be used with large vocabularies  #
$ With smaller vocabularies, these information-theoretic transformations do indeed work well on word similarity measures, but they perform very poorly on the word analogy task  #
$ Table 3: Spearman rank correlation on word similarity tasks  #
$ All vectors are 300-dimensional  #
$ The CBOW vectors are from the word2vec website and differ in that they contain phrase vectors  #
$ Model Size WS353 MC RG SCWS RW SVD 6B 35 3 35 1 42 5 38 3 25 6 SVD-S 6B 56 5 71 5 71 0 53 6 34 7 SVD-L 6B 65 7 72 7 75 1 56 5 37 0 CBOW 6B 57 2 65 6 68 2 57 0 32 5 SG 6B 62 8 65 2 69 7 58 1 37 2 GloVe 6B 65 8 72 7 77 8 53 9 38 1 SVD-L 42B 74 0 76 4 74 1 58 3 39 9 GloVe 42B 75 9 83 6 82 9 59 6 47 8 CBOW 100B 68 4 79 6 75 4 59 4 45 5 L model on this larger corpus  #
$ The fact that this basic SVD model does not scale well to large corpora lends further evidence to the necessity of the type of weighting scheme proposed in our model  #
$ Table 3 shows results on five different word similarity datasets  #
$ A similarity score is obtained from the word vectors by first normalizing each feature across the vocabulary and then calculating the cosine similarity  #
$ We compute Spearman's rank correlation coefficient between this score and the human judgments  #
$ CBOW denotes the vectors available on the word2vec website that are trained with word and phrase vectors on 100B words of news data  #
$ GloVe outperforms it while using a corpus less than half the size  #
$ Table 4 shows results on the NER task with the CRF-based model  #
$ The L-BFGS training terminates when no improvement has been achieved on the dev set for 25 iterations  #
$ Otherwise all configurations are identical to those used by Wang and Manning (2013)  #
$ The model labeled Discrete is the baseline using a comprehensive set of discrete features that comes with the standard distribution of the Stanford NER model, but with no word vector features  #
$ In addition to the HPCA and SVD models discussed previously, we also compare to the models of Huang et al  #
$ (2012) (HSMN) and Collobert and Weston (2008) (CW)  #
$ We trained the CBOW model using the word2vec tool8  #
$ The GloVe model outperforms all other methods on all evaluation metrics, except for the CoNLL test set, on which the HPCA method does slightly better  #
$ We conclude that the GloVe vectors are useful in downstream NLP tasks, as was first 8We use the same parameters as above, except in this case we found 5 negative samples to work slightly better than 10  #
$ Table 4: F1 score on NER task with 50d vectors  #
$ Discrete is the baseline without word vectors  #
$ We use publicly-available vectors for HPCA, HSMN, and CW  #
$ See text for details  #
$ Model Dev Test ACE MUC7 Discrete 91 0 85 4 77 4 73 4 SVD 90 8 85 7 77 3 73 7 SVD-S 91 0 85 5 77 6 74 3 SVD-L 90 5 84 8 73 6 71 5 HPCA 92 6 88 7 81 7 80 7 HSMN 90 5 85 7 78 7 74 7 CW 92 2 87 4 81 7 80 2 CBOW 93 1 88 2 82 2 81 1 GloVe 93 2 88 3 82 9 82 2 shown for neural vectors in (Turian et al , 2010)  #
$ 4 4 Model Analysis: Vector Length and Context Size In Fig  #
$ 2, we show the results of experiments that vary vector length and context window  #
$ A context window that extends to the left and right of a target word will be called symmetric, and one which extends only to the left will be called asymmetric  #
$ In (a), we observe diminishing returns for vectors larger than about 200 dimensions  #
$ In (b) and (c), we examine the effect of varying the window size for symmetric and asymmetric context windows  #
$ Performance is better on the syntactic subtask for small and asymmetric context windows, which aligns with the intuition that syntactic information is mostly drawn from the immediate context and can depend strongly on word order  #
$ Semantic information, on the other hand, is more frequently non-local, and more of it is captured with larger window sizes  #
$ 4 5 Model Analysis: Corpus Size In Fig  #
$ 3, we show performance on the word analogy task for 300-dimensional vectors trained on different corpora  #
$ On the syntactic subtask, there is a monotonic increase in performance as the corpus size increases  #
$ This is to be expected since larger corpora typically produce better statistics  #
$ Interestingly, the same trend is not true for the semantic subtask, where the models trained on the smaller Wikipedia corpora do better than those trained on the larger Gigaword corpus  #
$ This is likely due to the large number of city- and country- based analogies in the analogy dataset and the fact that Wikipedia has fairly comprehensive articles for most such locations  #
$ Moreover, Wikipedia's 50 55 60 65 70 75 80 85 Overall Syntactic Semantic Wiki2010 1B tokens Accuracy [%] Wiki2014 1 6B tokens Gigaword5 4 3B tokens Gigaword5 + Wiki2014 6B tokens Common Crawl 42B tokens Figure 3: Accuracy on the analogy task for 300-dimensional vectors trained on different corpora  #
$ entries are updated to assimilate new knowledge, whereas Gigaword is a fixed news repository with outdated and possibly incorrect information  #
$ 4 6 Model Analysis: Runtime The total runtime is split between populating X and training the model  #
$ The former depends on many factors, including window size, vocabulary size, and corpus size  #
$ Though we did not do so, this step could easily be parallelized across multiple machines (see, e g , Lebret and Collobert (2014) for some benchmarks)  #
$ Using a single thread of a dual 2 1GHz Intel Xeon E5-2658 machine, populating X with a 10 word symmetric context window, a 400,000 word vocabulary, and a 6 billion token corpus takes about 85 minutes  #
$ Given X, the time it takes to train the model depends on the vector size and the number of iterations  #
$ For 300-dimensional vectors with the above settings (and using all 32 cores of the above machine), a single iteration takes 14 minutes  #
$ See Fig  #
$ 4 for a plot of the learning curve  #
$ 4 7 Model Analysis: Comparison with word2vec A rigorous quantitative comparison of GloVe with word2vec is complicated by the existence of many parameters that have a strong effect on performance  #
$ We control for the main sources of variation that we identified in Sections 4 4 and 4 5 by setting the vector length, context window size, corpus, and vocabulary size to the configuration mentioned in the previous subsection  #
$ The most important remaining variable to control for is training time  #
$ For GloVe, the relevant parameter is the number of training iterations  #
$ For word2vec, the obvious choice would be the number of training epochs  #
$ Unfortunately, the code is currently designed for only a single epoch: GloVe Skip-Gram Accuracy [%] Iterations (GloVe) Negative Samples (Skip-Gram) Training Time (hrs) (b) GloVe vs Skip-Gram Figure 4: Overall accuracy on the word analogy task as a function of training time, which is governed by the number of iterations for GloVe and by the number of negative samples for CBOW (a) and skip-gram (b)  #
$ In all cases, we train 300-dimensional vectors on the same 6B token corpus (Wikipedia 2014 + Gigaword 5) with the same 400,000 word vocabulary, and use a symmetric context window of size 10  it specifies a learning schedule specific to a single pass through the data, making a modification for multiple passes a non-trivial task  #
$ Another choice is to vary the number of negative samples  #
$ Adding negative samples effectively increases the number of training words seen by the model, so in some ways it is analogous to extra epochs  #
$ We set any unspecified parameters to their default values, assuming that they are close to opti- mal, though we acknowledge that this simplification should be relaxed in a more thorough analysis  #
$ In Fig  #
$ 4, we plot the overall performance on the analogy task as a function of training time  #
$ The two x-axes at the bottom indicate the corresponding number of training iterations for GloVe and negative samples for word2vec  #
$ We note that word2vec's performance actually decreases if the number of negative samples increases beyond about 10  #
$ Presumably this is because the negative sampling method does not approximate the target probability distribution well 9 For the same corpus, vocabulary, window size, and training time, GloVe consistently outperforms word2vec  #
$ It achieves better results faster, and also obtains the best results irrespective of speed  #
$ 5 Conclusion Recently, considerable attention has been focused on the question of whether distributional word representations are best learned from count-based 9In contrast, noise-contrastive estimation is an approximation which improves with more negative samples  #
$ In Table 1 of (Mnih et al , 2013), accuracy on the analogy task is a non-decreasing function of the number of negative samples  #
$ methods or from prediction-based methods  #
$ Currently, prediction-based models garner substantial support; for example, Baroni et al  #
$ (2014) argue that these models perform better across a range of tasks  #
$ In this work we argue that the two classes of methods are not dramatically different at a fundamental level since they both probe the underlying co-occurrence statistics of the corpus, but the efficiency with which the count-based methods capture global statistics can be advantageous  #
$ We construct a model that utilizes this main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec  #
$ The result, GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks  #
$ Acknowledgments We thank the anonymous reviewers for their valuable comments  #
$ Stanford University gratefully acknowledges the support of the Defense Threat Reduction Agency (DTRA) under Air Force Research Laboratory (AFRL) contract no  #
$ FA8650-10-C-7020 and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under AFRL contract no  #
$ FA8750-13-2-0040  #
$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DTRA, AFRL, DEFT, or the US government  #







$ A Critical Review of Recurrent Neural Networks for Sequence Learning Zachary C  Lipton zlipton@cs ucsd edu John Berkowitz jaberkow@physics ucsd edu Charles Elkan elkan@cs ucsd edu June 5th, 2015 Abstract Countless learning tasks require dealing with sequential data  #
$ Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences  #
$ In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences  #
$ Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities  #
$ Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes  #
$ Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window  #
$ Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them  #
$ In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition  #
$ In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models  #
$ When appropriate, we reconcile conflicting notation and nomenclature  #
$ Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research  #
$ 1 Introduction Neural networks are powerful learning models that achieve state-of-the-art reults in a wide range of supervised and unsupervised machine learning tasks  #
$ 1 arXiv:1506 00019v4 [cs LG] 17 Oct 2015 They are suited especially well for machine perception tasks, where the raw underlying features are not individually interpretable  #
$ This success is attributed to their ability to learn hierarchical representations, unlike traditional methods that rely upon hand-engineered features [Farabet et al , 2013]  #
$ Over the past several years, storage has become more affordable, datasets have grown far larger, and the field of parallel computing has advanced considerably  #
$ In the setting of large datasets, simple linear models tend to under-fit, and often under-utilize computing resources  #
$ Deep learning methods, in particular those based on deep belief networks (DNNs), which are greedily built by stacking restricted Boltzmann machines, and convolutional neural networks, which exploit the local dependency of visual information, have demonstrated record-setting results on many important applications  #
$ However, despite their power, standard neural networks have limitations  #
$ Most notably, they rely on the assumption of independence among the training and test examples  #
$ After each example (data point) is processed, the entire state of the network is lost  #
$ If each example is generated independently, this presents no problem  #
$ But if data points are related in time or space, this is unacceptable  #
$ Frames from video, snippets of audio, and words pulled from sentences, represent settings where the independence assumption fails  #
$ Additionally, standard networks generally rely on examples being vectors of fixed length  #
$ Thus it is desirable to extend these powerful learning tools to model data with temporal or sequential structure and varying length inputs and outputs, especially in the many domains where neural networks are already the state of the art  #
$ Recurrent neural networks (RNNs) are connectionist models with the ability to selectively pass information across sequence steps, while processing sequential data one element at a time  #
$ Thus they can model input and/or output consisting of sequences of elements that are not independent  #
$ Further, recurrent neural networks can simultaneously model sequential and time dependencies on multiple scales  #
$ In the following subsections, we explain the fundamental reasons why recurrent neural networks are worth investigating  #
$ To be clear, we are motivated by a desire to achieve empirical results  #
$ This motivation warrants clarification because recurrent networks have roots in both cognitive modeling and supervised machine learning  #
$ Owing to this difference of perspectives, many published papers have different aims and priorities  #
$ In many foundational papers, generally published in cognitive science and computational neuroscience journals, such as [Hopfield, 1982, Jordan, 1986, Elman, 1990], biologically plausible mechanisms are emphasized  #
$ In other papers [Schuster and Paliwal, 1997, Socher et al , 2014, Karpathy and Fei-Fei, 2014], biological inspiration is downplayed in favor of achieving empirical results on important tasks and datasets  #
$ This review is motivated by practical results rather than biological plausibility, but where appropriate, we draw connections to relevant concepts in neuroscience  #
$ Given the empirical aim, we now address three significant questions that one might reasonably want answered before reading further  #
$ 2 1 1 Why model sequentiality explicitly? #
$ In light of the practical success and economic value of sequence-agnostic models, this is a fair question  #
$ Support vector machines, logistic regression, and feedforward networks have proved immensely useful without explicitly modeling time  #
$ Arguably, it is precisely the assumption of independence that has led to much recent progress in machine learning  #
$ Further, many models implicitly capture time by concatenating each input with some number of its immediate predecessors and successors, presenting the machine learning model with a sliding window of context about each point of interest  #
$ This approach has been used with deep belief nets for speech modeling by Maas et al  #
$ [2012]  #
$ Unfortunately, despite the usefulness of the independence assumption, it precludes modeling long-range dependencies  #
$ For example, a model trained using a finite-length context window of length 5 could never be trained to answer the simple question, "what was the data point seen six time steps ago?" #
$ For a practical application such as call center automation, such a limited system might learn to route calls, but could never participate with complete success in an extended dialogue  #
$ Since the earliest conception of artificial intelligence, researchers have sought to build systems that interact with humans in time  #
$ In Alan Turing's groundbreaking paper Computing Machinery and Intelligence, he proposes an "imitation game" which judges a machine's intelligence by its ability to convincingly engage in dialogue [Turing, 1950]  #
$ Besides dialogue systems, modern interactive systems of economic importance include self-driving cars and robotic surgery, among others  #
$ Without an explicit model of sequentiality or time, it seems unlikely that any combination of classifiers or regressors can be cobbled together to provide this functionality  #
$ 1 2 Why not use Markov models? #
$ Recurrent neural networks are not the only models capable of representing time dependencies  #
$ Markov chains, which model transitions between states in an observed sequence, were first described by the mathematician Andrey Markov in 1906  #
$ Hidden Markov models (HMMs), which model an observed sequence as probabilistically dependent upon a sequence of unobserved states, were described in the 1950s and have been widely studied since the 1960s [Stratonovich, 1960]  #
$ However, traditional Markov model approaches are limited because their states must be drawn from a modestly sized discrete state space S  The dynamic programming algorithm that is used to perform efficient inference with hidden Markov models scales in time O(|S|2 ) [Viterbi, 1967]  #
$ Further, the transition table capturing the probability of moving between any two time-adjacent states is of size |S|2   #
$ Thus, standard operations become infeasible with an HMM when the set of possible hidden states grows large  #
$ Further, each hidden state can depend only on the immediately previous state  #
$ While it is possible to extend a Markov model to account for a larger context window by creating a new state space equal to the cross product of the possible states at each time in the window, this procedure grows the state space exponentially with the size of the 3 window, rendering Markov models computationally impractical for modeling long-range dependencies [Graves et al , 2014]  #
$ Given the limitations of Markov models, we ought to explain why it is reasonable that connectionist models, i e , artificial neural networks, should fare better  #
$ First, recurrent neural networks can capture long-range time dependencies, overcoming the chief limitation of Markov models  #
$ This point requires a careful explanation  #
$ As in Markov models, any state in a traditional RNN depends only on the current input as well as on the state of the network at the previous time step 1 However, the hidden state at any time step can contain information from a nearly arbitrarily long context window  #
$ This is possible because the number of distinct states that can be represented in a hidden layer of nodes grows exponentially with the number of nodes in the layer  #
$ Even if each node took only binary values, the network could represent 2N states where N is the number of nodes in the hidden layer  #
$ When the value of each node is a real number, a network can represent even more distinct states  #
$ While the potential expressive power of a network grows exponentially with the number of nodes, the complexity of both inference and training grows at most quadratically  #
$ 1 3 Are RNNs too expressive? #
$ Finite-sized RNNs with nonlinear activations are a rich family of models, capable of nearly arbitrary computation  #
$ A well-known result is that a finite-sized recurrent neural network with sigmoidal activation functions can simulate a universal Turing machine [Siegelmann and Sontag, 1991]  #
$ The capability of RNNs to perform arbitrary computation demonstrates their expressive power, but one could argue that the C programming language is equally capable of expressing arbitrary programs  #
$ And yet there are no papers claiming that the invention of C represents a panacea for machine learning  #
$ A fundamental reason is there is no simple way of efficiently exploring the space of C programs  #
$ In particular, there is no general way to calculate the gradient of an arbitrary C program to minimize a chosen loss function  #
$ Moreover, given any finite dataset, there exist countless programs which overfit the dataset, generating desired training output but failing to generalize to test examples  #
$ Why then should RNNs suffer less from similar problems? #
$ First, given any fixed architecture (set of nodes, edges, and activation functions), the recurrent neural networks with this architecture are differentiable end to end  #
$ The derivative of the loss function can be calculated with respect to each of the parameters (weights) in the model  #
$ Thus, RNNs are amenable to gradient-based training  #
$ Second, while the Turing-completeness of RNNs is an impressive property, given a fixed-size RNN with a specific architecture, it is not actually possible to reproduce any arbitrary program  #
$ Further, unlike a program composed in C, a recurrent neural network can be regularized via standard techniques that help 1 While traditional RNNs only model the dependence of the current state on the previous state, bidirectional recurrent neural networks (BRNNs) [Schuster and Paliwal, 1997] extend RNNs to model dependence on both past states and future states  #
$ 4 prevent overfitting, such as weight decay, dropout, and limiting the degrees of freedom  #
$ 1 4 Comparison to prior literature The literature on recurrent neural networks can seem impenetrable to the uninitiated  #
$ Shorter papers assume familiarity with a large body of background lit- erature, while diagrams are frequently underspecified, failing to indicate which edges span time steps and which do not  #
$ Jargon abounds, and notation is inconsistent across papers or overloaded within one paper  #
$ Readers are frequently in the unenviable position of having to synthesize conflicting information across many papers in order to understand just one  #
$ For example, in many papers subscripts index both nodes and time steps  #
$ In others, h simultaneously stands for a link function and a layer of hidden nodes  #
$ The variable t simultaneously stands for both time indices and targets, sometimes in the same equation  #
$ Many excellent research papers have appeared recently, but clear reviews of the recurrent neural network literature are rare  #
$ Among the most useful resources are a recent book on supervised sequence labeling with recurrent neural networks [Graves, 2012] and an earlier doctoral thesis [Gers, 2001]  #
$ A recent survey covers recurrent neural nets for language modeling [De Mulder et al , 2015]  #
$ Various authors focus on specific technical aspects; for example Pearlmutter [1995] surveys gradient calculations in continuous time recurrent neural networks  #
$ In the present review paper, we aim to provide a readable, intuitive, consistently notated, and reasonably comprehensive but selective survey of research on recurrent neural networks for learning with sequences  #
$ We emphasize architectures, algorithms, and results, but we aim also to distill the intuitions that have guided this largely heuristic and empirical field  #
$ In addition to concrete modeling details, we offer qualitative arguments, a historical perspective, and comparisons to alternative methodologies where appropriate  #
$ 2 Background This section introduces formal notation and provides a brief background on neural networks in general  #
$ 2 1 Sequences The input to an RNN is a sequence, and/or its target is a sequence  #
$ An input sequence can be denoted (x(1) , x(2) ,    , x(T ) ) where each data point x(t) is a real-valued vector  #
$ Similarly, a target sequence can be denoted (y(1) , y(2) ,    , y(T ) )  #
$ A training set typically is a set of examples where each example is an (input sequence, target sequence) pair, although commonly either the input or the output may be a single data point  #
$ Sequences may be of finite or countably infinite length  #
$ When they are finite, the maximum time index of the sequence 5 is called T  RNNs are not limited to time-based sequences  #
$ They have been used successfully on non-temporal sequence data, including genetic data [Baldi and Pollastri, 2003]  #
$ However, in many important applications of RNNs, the sequences have an explicit or implicit temporal aspect  #
$ While we often refer to time in this survey, the methods described here are applicable to non-temporal as well as to temporal tasks  #
$ Using temporal terminology, an input sequence consists of data points x(t) that arrive in a discrete sequence of time steps indexed by t  A target sequence consists of data points y(t)   #
$ We use superscripts with parentheses for time, and not subscripts, to prevent confusion between sequence steps and indices of nodes in a network  #
$ When a model produces predicted data points, these are labeled ^ y(t)   #
$ The time-indexed data points may be equally spaced samples from a continuous real-world process  #
$ Examples include the still images that comprise the frames of videos or the discrete amplitudes sampled at fixed intervals that comprise audio recordings  #
$ The time steps may also be ordinal, with no exact correspondence to durations  #
$ In fact, RNNs are frequently applied to domains where sequences have a defined order but no explicit notion of time  #
$ This is the case with natural language  #
$ In the word sequence "John Coltrane plays the saxophone", x(1) = John, x(2) = Coltrane, etc  #
$ 2 2 Neural networks Neural networks are biologically inspired models of computation  #
$ Generally, a neural network consists of a set of artificial neurons, commonly referred to as nodes or units, and a set of directed edges between them, which intuitively represent the synapses in a biological neural network  #
$ Associated with each neuron j is an activation function lj(), which is sometimes called a link function  #
$ We use the notation lj and not hj, unlike some other papers, to distinguish the activation function from the values of the hidden nodes in a network, which, as a vector, is commonly notated h in the literature  #
$ Associated with each edge from node j to j is a weight wjj   #
$ Following the convention adopted in several foundational papers [Hochreiter and Schmidhuber, 1997, Gers et al , 2000, Gers, 2001, Sutskever et al , 2011], we index neurons with j and j , and wjj denotes the "to-from" weight corresponding to the directed edge to node j from node j   #
$ It is important to note that in many references the indices are flipped and wj j = wjj denotes the "from-to" weight on the directed edge from the node j to the node j, as in lecture notes by Elkan [2015] and in Wikipedia [2015]  #
$ The value vj of each neuron j is calculated by applying its activation function to a weighted sum of the values of its input nodes (Figure 1): vj = lj j wjj  vj   #
$ For convenience, we term the weighted sum inside the parentheses the incoming 6 Figure 1: An artificial neuron computes a nonlinear function of a weighted sum of its inputs  #
$ activation and notate it as aj  #
$ We represent this computation in diagrams by depicting neurons as circles and edges as arrows connecting them  #
$ When appropriate, we indicate the exact activation function with a symbol, e g ,  for sigmoid  #
$ Common choices for the activation function include the sigmoid (z) = 1/(1 + e-z ) and the tanh function (z) = (ez - e-z)/(ez + e-z)  #
$ The latter has become common in feedforward neural nets and was applied to recurrent nets by Sutskever et al  #
$ [2011]  #
$ Another activation function which has become prominent in deep learning research is the rectified linear unit (ReLU) whose formula is lj(z) = max(0, z)  #
$ This type of unit has been demonstrated to improve the performance of many deep neural networks [Nair and Hinton, 2010, Maas et al , 2012, Zeiler et al , 2013] on tasks as varied as speech processing and object recognition, and has been used in recurrent neural networks by Bengio et al  #
$ [2013]  #
$ The activation function at the output nodes depends upon the task  #
$ For multiclass classification with K alternative classes, we apply a softmax nonlinearity in an output layer of K nodes  #
$ The softmax function calculates ^ yk = eak K k =1 eak for k = 1 to k = K  The denominator is a normalizing term consisting of the sum of the numerators, ensuring that the outputs of all nodes sum to one  #
$ For multilabel classification the activation function is simply a point-wise sigmoid, and for regression we typically have linear output  #
$ 7 Figure 2: A feedforward neural network  #
$ An example is presented to the network by setting the values of the blue (bottom) nodes  #
$ The values of the nodes in each layer are computed successively as a function of the prior layers until output is produced at the topmost layer  #
$ 2 3 Feedforward networks and backpropagation With a neural model of computation, one must determine the order in which computation should proceed  #
$ Should nodes be sampled one at a time and updated, or should the value of all nodes be calculated at once and then all updates applied simultaneously? #
$ Feedforward networks (Figure 2) are a restricted class of networks which deal with this problem by forbidding cycles in the directed graph of nodes  #
$ Given the absence of cycles, all nodes can be arranged into layers, and the outputs in each layer can be calculated given the outputs from the lower layers  #
$ The input x to a feedforward network is provided by setting the values of the lowest layer  #
$ Each higher layer is then successively computed until output is generated at the topmost layer ^ y  Feedforward networks are frequently used for supervised learning tasks such as classification and regression  #
$ Learning is accomplished by iteratively updating each of the weights to minimize a loss function, L(^ y, y), which penalizes the distance between the output ^ y and the target y  #
$ The most successful algorithm for training neural networks is backpropagation, introduced for this purpose by Rumelhart et al  #
$ [1985]  #
$ Backpropagation uses the chain rule to calculate the derivative of the loss function L with respect to each parameter in the network  #
$ The weights are then adjusted by gradient descent  #
$ Because the loss surface is non-convex, there is no assurance that backpropagation will reach a global minimum  #
$ Moreover, exact optimization is known to be an NP-hard problem  #
$ However, a large body of work on heuristic 8 pre-training and optimization techniques has led to impressive empirical success on many supervised learning tasks  #
$ In particular, convolutional neural networks, popularized by Le Cun et al  #
$ [1990], are a variant of feedforward neural network that holds records since 2012 in many computer vision tasks such as object detection [Krizhevsky et al , 2012]  #
$ Nowadays, neural networks are usually trained with stochastic gradient descent (SGD) using mini-batches  #
$ With batch size equal to one, the stochastic gradient update equation is w  w - wFi where  is the learning rate and wFi is the gradient of the objective function with respect to the parameters w as calculated on a single example (xi, yi)  #
$ Many variants of SGD are used to accelerate learning  #
$ Some popular heuristics, such as AdaGrad [Duchi et al , 2011], AdaDelta [Zeiler, 2012], and RMSprop [Tieleman and Hinton, 2012], tune the learning rate adaptively for each feature  #
$ AdaGrad, arguably the most popular, adapts the learning rate by caching the sum of squared gradients with respect to each parameter at each time step  #
$ The step size for each feature is multiplied by the inverse of the square root of this cached value  #
$ AdaGrad leads to fast convergence on convex error surfaces, but because the cached sum is monotonically increasing, the method has a monotonically decreasing learning rate, which may be undesirable on highly non-convex loss surfaces  #
$ RMSprop modifies AdaGrad by introducing a decay factor in the cache, changing the monotonically growing value into a moving average  #
$ Momentum methods are another common SGD variant used to train neural networks  #
$ These methods add to each update a decaying sum of the previous updates  #
$ When the momentum parameter is tuned well and the network is initialized well, momentum methods can train deep nets and recurrent nets competitively with more computationally expensive methods like the Hessian-free optimizer of Sutskever et al  #
$ [2013]  #
$ To calculate the gradient in a feedforward neural network, backpropagation proceeds as follows  #
$ First, an example is propagated forward through the network to produce a value vj at each node and outputs ^ y at the topmost layer  #
$ Then, a loss function value L(^ yk, yk) is computed at each output node k  Subsequently, for each output node k, we calculate k = L(^ yk, yk) ^ yk  lk(ak)  #
$ Given these values k, for each node in the immediately prior layer we calculate j = l (aj) k k  wkj  #
$ This calculation is performed successively for each lower layer to yield j for every node j given the  values for each node connected to j by an outgoing edge  #
$ Each value j represents the derivative L/aj of the total loss function with respect to that node's incoming activation  #
$ Given the values vj calculated 9 during the forward pass, and the values j calculated during the backward pass, the derivative of the loss L with respect a given parameter wjj is L wjj = jvj   #
$ Other methods have been explored for learning the weights in a neural network  #
$ A number of papers from the 1990s [Belew et al , 1990, Gruau et al , 1994] championed the idea of learning neural networks with genetic algorithms, with some even claiming that achieving success on real-world problems only by applying many small changes to the weights of a network was impossible  #
$ Despite the subsequent success of backpropagation, interest in genetic algorithms continues  #
$ Several recent papers explore genetic algorithms for neural networks, especially as a means of learning the architecture of neural networks, a problem not addressed by backpropagation [Bayer et al , 2009, Harp and Samad, 2013]  #
$ By architecture we mean the number of layers, the number of nodes in each, the connectivity pattern among the layers, the choice of activation functions, etc  #
$ One open question in neural network research is how to exploit sparsity in training  #
$ In a neural network with sigmoidal or tanh activation functions, the nodes in each layer never take value exactly zero  #
$ Thus, even if the inputs are sparse, the nodes at each hidden layer are not  #
$ However, rectified linear units (ReLUs) introduce sparsity to hidden layers [Glorot et al , 2011]  #
$ In this setting, a promising path may be to store the sparsity pattern when computing each layer's values and use it to speed up computation of the next layer in the network  #
$ Some recent work shows that given sparse inputs to a linear model with a standard regularizer, sparsity can be fully exploited even if regularization makes the gradient be not sparse [Carpenter, 2008, Langford et al , 2009, Singer and Duchi, 2009, Lipton and Elkan, 2015]  #
$ 3 Recurrent neural networks Recurrent neural networks are feedforward neural networks augmented by the inclusion of edges that span adjacent time steps, introducing a notion of time to the model  #
$ Like feedforward networks, RNNs may not have cycles among conventional edges  #
$ However, edges that connect adjacent time steps, called recurrent edges, may form cycles, including cycles of length one that are self-connections from a node to itself across time  #
$ At time t, nodes with recurrent edges receive input from the current data point x(t) and also from hidden node values h(t-1) in the network's previous state  #
$ The output ^ y(t) at each time t is calculated given the hidden node values h(t) at time t  Input x(t-1) at time t - 1 can influence the output ^ y(t) at time t and later by way of the recurrent connections  #
$ Two equations specify all calculations necessary for computation at each time step on the forward pass in a simple recurrent neural network as in Figure 3: h(t) = (Whxx(t) + Whhh(t-1) + bh) 10 Figure 3: A simple recurrent network  #
$ At each time step t, activation is passed along solid edges as in a feedforward network  #
$ Dashed edges connect a source node at each time t to a target node at each following time t + 1  #
$ ^ y(t) = softmax(Wyhh(t) + by)  #
$ Here Whx is the matrix of conventional weights between the input and the hidden layer and Whh is the matrix of recurrent weights between the hidden layer and itself at adjacent time steps  #
$ The vectors bh and by are bias parameters which allow each node to learn an offset  #
$ The dynamics of the network depicted in Figure 3 across time steps can be visualized by unfolding it as in Figure 4  #
$ Given this picture, the network can be interpreted not as cyclic, but rather as a deep network with one layer per time step and shared weights across time steps  #
$ It is then clear that the unfolded network can be trained across many time steps using backpropagation  #
$ This algorithm, called backpropagation through time (BPTT), was introduced by Werbos [1990]  #
$ All recurrent networks in common current use apply it  #
$ 3 1 Early recurrent network designs The foundational research on recurrent networks took place in the 1980s  #
$ In 1982, Hopfield introduced a family of recurrent neural networks that have pattern recognition capabilities [Hopfield, 1982]  #
$ They are defined by the values of the weights between nodes and the link functions are simple thresholding at zero  #
$ In these nets, a pattern is placed in the network by setting the values of the nodes  #
$ The network then runs for some time according to its update rules, and eventually another pattern is read out  #
$ Hopfield networks are useful for recovering a stored pattern from a corrupted version and are the forerunners of Boltzmann machines and auto-encoders  #
$ 11 Figure 4: The recurrent network of Figure 3 unfolded across time steps  #
$ An early architecture for supervised learning on sequences was introduced by Jordan [1986]  #
$ Such a network (Figure 5) is a feedforward network with a single hidden layer that is extended with special units 2 Output node values are fed to the special units, which then feed these values to the hidden nodes at the following time step  #
$ If the output values are actions, the special units allow the network to remember actions taken at previous time steps  #
$ Several modern architectures use a related form of direct transfer from output nodes; Sutskever et al  #
$ [2014] translates sentences between natural languages, and when generating a text sequence, the word chosen at each time step is fed into the network as input at the following time step  #
$ Additionally, the special units in a Jordan network are self-connected  #
$ Intuitively, these edges allow sending information across multiple time steps without perturbing the output at each intermediate time step  #
$ The architecture introduced by Elman [1990] is simpler than the earlier Jordan architecture  #
$ Associated with each unit in the hidden layer is a context unit  #
$ Each such unit j takes as input the state of the corresponding hidden node j at the previous time step, along an edge of fixed weight wj j = 1  #
$ This value then feeds back into the same hidden node j along a standard edge  #
$ This architecture is equivalent to a simple RNN in which each hidden node has a single self-connected recurrent edge  #
$ The idea of fixed-weight recurrent edges that make hidden nodes self-connected is fundamental in subsequent work on LSTM networks [Hochreiter and Schmidhuber, 1997]  #
$ Elman [1990] trains the network using backpropagation and demonstrates that the network can learn time dependencies  #
$ The paper features two sets of experiments  #
$ The first extends the logical operation exclusive or (XOR) to 2 Jordan [1986] calls the special units "state units" while Elman [1990] calls a corresponding structure "context units " #
$ In this paper we simplify terminology by using only "context units"  #
$ 12 Figure 5: A recurrent neural network as proposed by Jordan [1986]  #
$ Output units are connected to special units that at the next time step feed into themselves and into hidden units  #
$ the time domain by concatenating sequences of three tokens  #
$ For each three-token segment, e g  #
$ "011", the first two tokens ("01") are chosen randomly and the third ("1") is set by performing xor on the first two  #
$ Random guessing should achieve accuracy of 50%  #
$ A perfect system should perform the same as random for the first two tokens, but guess the third token perfectly, achieving accuracy of 66 7%  #
$ The simple network of Elman [1990] does in fact approach this maximum achievable score  #
$ 3 2 Training recurrent networks Learning with recurrent networks has long been considered to be difficult  #
$ Even for standard feedforward networks, the optimization task is NP-complete Blum and Rivest [1993]  #
$ But learning with recurrent networks can be especially challenging due to the difficulty of learning long-range dependencies, as described by Bengio et al  #
$ [1994] and expanded upon by Hochreiter et al  #
$ [2001]  #
$ The problems of vanishing and exploding gradients occur when backpropagating errors across many time steps  #
$ As a toy example, consider a network with a single input node, a single output node, and a single recurrent hidden node (Figure 7)  #
$ Now consider an input passed to the network at time  and an error calculated at time t, assuming input of zero in the intervening time steps  #
$ The tying of weights across time steps means that the recurrent edge at the hidden node j always has the same weight  #
$ Therefore, the contribution of the input at time to the output at time t will either explode or approach zero, exponentially fast, as t -  grows large  #
$ Hence the derivative of the error with respect to the input will either explode or vanish  #
$ 13 Figure 6: A recurrent neural network as described by Elman [1990]  #
$ Hidden units are connected to context units, which feed back into the hidden units at the next time step  #
$ Which of the two phenomena occurs depends on whether the weight of the recurrent edge |wjj| > 1 or |wjj| < 1 and on the activation function in the hidden node (Figure 8)  #
$ Given a sigmoid activation function, the vanishing gradient problem is more pressing, but with a rectified linear unit max(0, x), it is easier to imagine the exploding gradient  #
$ Pascanu et al  #
$ [2012] give a thorough mathematical treatment of the vanishing and exploding gradient problems, characterizing exact conditions under which these problems may occur  #
$ Given these conditions, they suggest an approach to training via a regularization term that forces the weights to values where the gradient neither vanishes nor explodes  #
$ Truncated backpropagation through time (TBPTT) is one solution to the exploding gradient problem for continuously running networks [Williams and Zipser, 1989]  #
$ With TBPTT, some maximum number of time steps is set along which error can be propagated  #
$ While TBPTT with a small cutoff can be used to alleviate the exploding gradient problem, it requires that one sacrifice the ability to learn long-range dependencies  #
$ The LSTM architecture described below uses carefully designed nodes with recurrent edges with fixed unit weight as a solution to the vanishing gradient problem  #
$ The issue of local optima is an obstacle to effective training that cannot be dealt with simply by modifying the network architecture  #
$ Optimizing even a single hidden-layer feedforward network is an NP-complete problem [Blum and Rivest, 1993]  #
$ However, recent empirical and theoretical studies suggest that in practice, the issue may not be as important as once thought  #
$ Dauphin et al  #
$ [2014] show that while many critical points exist on the error surfaces of large neural networks, the ratio of saddle points to true local minima increases exponentially with the size of the network, and algorithms can be designed to 14 Figure 7: A simple recurrent net with one input unit, one output unit, and one recurrent hidden unit  #
$ Figure 8: A visualization of the vanishing gradient problem, using the network depicted in Figure 7, adapted from Graves [2012]  #
$ If the weight along the recurrent edge is less than one, the contribution of the input at the first time step to the output at the final time step will decrease exponentially fast as a function of the length of the time interval in between  #
$ 15 escape from saddle points  #
$ Overall, along with the improved architectures explained below, fast implementations and better gradient-following heuristics have rendered RNN training feasible  #
$ Implementations of forward and backward propagation using GPUs, such as the Theano [Bergstra et al , 2010] and Torch [Collobert et al , 2011] packages, have made it straightforward to implement fast training algorithms  #
$ In 1996, prior to the introduction of the LSTM, attempts to train recurrent nets to bridge long time gaps were shown to perform no better than random guessing [Hochreiter and Schmidhuber, 1996]  #
$ However, RNNs are now frequently trained successfully  #
$ For some tasks, freely available software can be run on a single GPU and produce compelling results in hours [Karpathy, 2015]  #
$ Martens and Sutskever [2011] reported success training recurrent neural networks with a Hessian-free truncated Newton approach, and applied the method to a network which learns to generate text one character at a time in [Sutskever et al , 2011]  #
$ In the paper that describes the abundance of saddle points on the error surfaces of neural networks [Dauphin et al , 2014], the authors present a saddle-free version of Newton's method  #
$ Unlike Newton's method, which is attracted to critical points, including saddle points, this variant is specially designed to escape from them  #
$ Experimental results include a demonstration of improved performance on recurrent networks  #
$ Newton's method requires computing the Hessian, which is prohibitively expensive for large networks, scaling quadratically with the number of parameters  #
$ While their algorithm only approximates the Hessian, it is still computationally expensive compared to SGD  #
$ Thus the authors describe a hybrid approach in which the saddle-free Newton method is applied only in places where SGD appears to be stuck  #
$ 4 Modern RNN architectures The most successful RNN architectures for sequence learning stem from two papers published in 1997  #
$ The first paper, Long Short-Term Memory by Hochreiter and Schmidhuber [1997], introduces the memory cell, a unit of computation that replaces traditional nodes in the hidden layer of a network  #
$ With these memory cells, networks are able to overcome difficulties with training encountered by earlier recurrent networks  #
$ The second paper, Bidirectional Recurrent Neural Networks by Schuster and Paliwal [1997], introduces an architecture in which information from both the future and the past are used to determine the output at any point in the sequence  #
$ This is in contrast to previous networks, in which only past input can affect the output, and has been used successfully for sequence labeling tasks in natural language processing, among others  #
$ Fortunately, the two innovations are not mutually exclusive, and have been successfully combined for phoneme classification [Graves and Schmidhuber, 2005] and handwriting recognition [Graves et al , 2009]  #
$ In this section we explain the LSTM and BRNN and we describe the neural Turing machine (NTM), which extends RNNs with an addressable external memory [Graves et al , 2014]  #
$ 16 Figure 9: One LSTM memory cell as proposed by Hochreiter and Schmidhuber [1997]  #
$ The self-connected node is the internal state s  The diagonal line indicates that it is linear, i e  #
$ the identity link function is applied  #
$ The blue dashed line is the recurrent edge, which has fixed unit weight  #
$ Nodes marked  output the product of their inputs  #
$ All edges into and from  nodes also have fixed unit weight  #
$ 4 1 Long short-term memory (LSTM) Hochreiter and Schmidhuber [1997] introduced the LSTM model primarily in order to overcome the problem of vanishing gradients  #
$ This model resembles a standard recurrent neural network with a hidden layer, but each ordinary node (Figure 1) in the hidden layer is replaced by a memory cell (Figure 9)  #
$ Each memory cell contains a node with a self-connected recurrent edge of fixed weight one, ensuring that the gradient can pass across many time steps without vanishing or exploding  #
$ To distinguish references to a memory cell and not an ordinary node, we use the subscript c  The term "long short-term memory" comes from the following intuition  #
$ Simple recurrent neural networks have long-term memory in the form of weights  #
$ The weights change slowly during training, encoding general knowledge about the data  #
$ They also have short-term memory in the form of ephemeral activations, which pass from each node to successive nodes  #
$ The LSTM model introduces an intermediate type of storage via the memory cell  #
$ A memory cell is a composite unit, built from simpler nodes in a specific connectivity pattern, with the novel inclusion of multiplicative nodes, represented in diagrams by the letter   #
$ All elements of the LSTM cell are enumerated and described below  #
$ Note that when we use vector notation, we are referring to the values of the 17 nodes in an entire layer of cells  #
$ For example, s is a vector containing the value of sc at each memory cell c in a layer  #
$ When the subscript c is used, it is to index an individual memory cell  #
$ Input node: This unit, labeled gc, is a node that takes activation in the standard way from the input layer x(t) at the current time step and (along recurrent edges) from the hidden layer at the previous time step h(t-1)  #
$ Typically, the summed weighted input is run through a tanh activation function, although in the original LSTM paper, the activation function is a sigmoid  #
$ Input gate: Gates are a distinctive feature of the LSTM approach  #
$ A gate is a sigmoidal unit that, like the input node, takes activation from the current data point x(t) as well as from the hidden layer at the previous time step  #
$ A gate is so-called because its value is used to multiply the value of another node  #
$ It is a gate in the sense that if its value is zero, then flow from the other node is cut off  #
$ If the value of the gate is one, all flow is passed through  #
$ The value of the input gate ic multiplies the value of the input node  #
$ Internal state: At the heart of each memory cell is a node sc with linear activation, which is referred to in the original paper as the "internal state" of the cell  #
$ The internal state sc has a self-connected recurrent edge with fixed unit weight  #
$ Because this edge spans adjacent time steps with constant weight, error can flow across time steps without vanishing or exploding  #
$ This edge is often called the constant error carousel  #
$ In vector notation, the update for the internal state is s(t) = g(t) i(t) + s(t-1) where is pointwise multiplication  #
$ Forget gate: These gates fc were introduced by Gers et al  #
$ [2000]  #
$ They provide a method by which the network can learn to flush the contents of the internal state  #
$ This is especially useful in continuously running networks  #
$ With forget gates, the equation to calculate the internal state on the forward pass is s(t) = g(t) i(t) + f(t) s(t-1)   #
$ Output gate: The value vc ultimately produced by a memory cell is the value of the internal state sc multiplied by the value of the output gate oc  #
$ It is customary that the internal state first be run through a tanh activation function, as this gives the output of each cell the same dynamic range as an ordinary tanh hidden unit  #
$ However, in other neural network research, rectified linear units, which have a greater dynamic range, are easier to train  #
$ Thus it seems plausible that the nonlinear function on the internal state might be omitted  #
$ In the original paper and in most subsequent work, the input node is labeled g  We adhere to this convention but note that it may be confusing as g does 18 Figure 10: LSTM memory cell with a forget gate as described by Gers et al  #
$ [2000]  #
$ not stand for gate  #
$ In the original paper, the gates are called yin and yout but this is confusing because y generally stands for output in the machine learning literature  #
$ Seeking comprehensibility, we break with this convention and use i, f, and o to refer to input, forget and output gates respectively, as in Sutskever et al  #
$ [2014]  #
$ Since the original LSTM was introduced, several variations have been proposed  #
$ Forget gates, described above, were proposed in 2000 and were not part of the original LSTM design  #
$ However, they have proven effective and are standard in most modern implementations  #
$ That same year, Gers and Schmidhuber [2000] proposed peephole connections that pass from the internal state directly to the input and output gates of that same node without first having to be modulated by the output gate  #
$ They report that these connections improve performance on timing tasks where the network must learn to measure precise intervals between events  #
$ The intuition of the peephole connection can be captured by the following example  #
$ Consider a network which must learn to count objects and emit some desired output when n objects have been seen  #
$ The network might learn to let some fixed amount of activation into the internal state after each object is seen  #
$ This activation is trapped in the internal state sc by the constant error carousel, and is incremented iteratively each time another object is seen  #
$ When the nth object is seen, the network needs to know to let out content from the internal state so that it can affect the output  #
$ To accomplish this, the output gate oc must know the content of the internal state sc  #
$ Thus sc should be an input to oc  #
$ Put formally, computation in the LSTM model proceeds according to the 19 Figure 11: A recurrent neural network with a hidden layer consisting of two memory cells  #
$ The network is shown unfolded across two time steps  #
$ following calculations, which are performed at each time step  #
$ These equations give the full algorithm for a modern LSTM with forget gates: g(t) = (Wgxx(t) + Wghh(t-1) + bg) i(t) = (Wixx(t) + Wihh(t-1) + bi) f(t) = (Wfxx(t) + Wfhh(t-1) + bf ) o(t) = (Woxx(t) + Wohh(t-1) + bo) s(t) = g(t) i(i) + s(t-1) f(t) h(t) = (s(t)) o(t)   #
$ The value of the hidden layer of the LSTM at time t is the vector h(t) , while h(t-1) is the values output by each memory cell in the hidden layer at the previous time  #
$ Note that these equations include the forget gate, but not peephole connections  #
$ The calculations for the simpler LSTM without forget gates are obtained by setting f(t) = 1 for all t  We use the tanh function  for the input node g following the state-of-the-art design of Zaremba and Sutskever [2014]  #
$ However, in the original LSTM paper, the activation function for g is the sigmoid   #
$ Intuitively, in terms of the forward pass, the LSTM can learn when to let activation into the internal state  #
$ As long as the input gate takes value zero, no activation can get in  #
$ Similarly, the output gate learns when to let the 20 Figure 12: A bidirectional recurrent neural network as described by Schuster and Paliwal [1997], unfolded in time  #
$ value out  #
$ When both gates are closed, the activation is trapped in the memory cell, neither growing nor shrinking, nor affecting the output at intermediate time steps  #
$ In terms of the backwards pass, the constant error carousel enables the gradient to propagate back across many time steps, neither exploding nor vanishing  #
$ In this sense, the gates are learning when to let error in, and when to let it out  #
$ In practice, the LSTM has shown a superior ability to learn long-range dependencies as compared to simple RNNs  #
$ Consequently, the majority of state-of-the-art application papers covered in this review use the LSTM model  #
$ One frequent point of confusion is the manner in which multiple memory cells are used together to comprise the hidden layer of a working neural network  #
$ To alleviate this confusion, we depict in Figure 11 a simple network with two memory cells, analogous to Figure 4  #
$ The output from each memory cell flows in the subsequent time step to the input node and all gates of each memory cell  #
$ It is common to include multiple layers of memory cells [Sutskever et al , 2014]  #
$ Typically, in these architectures each layer takes input from the layer below at the same time step and from the same layer in the previous time step  #
$ 4 2 Bidirectional recurrent neural networks (BRNNs) Along with the LSTM, one of the most used RNN architectures is the bidirectional recurrent neural network (BRNN) (Figure 12) first described by Schuster and Paliwal [1997]  #
$ In this architecture, there are two layers of hidden nodes  #
$ Both hidden layers are connected to input and output  #
$ The two hidden layers are differentiated in that the first has recurrent connections from the past time 21 steps while in the second the direction of recurrent of connections is flipped, passing activation backwards along the sequence  #
$ Given an input sequence and a target sequence, the BRNN can be trained by ordinary backpropagation after unfolding across time  #
$ The following three equations describe a BRNN: h(t) = (Whxx(t) + Whhh(t-1) + bh) z(t) = (Wzxx(t) + Wzzz(t+1) + bz) ^ y(t) = softmax(Wyhh(t) + Wyzz(t) + by) where h(t) and z(t) are the values of the hidden layers in the forwards and backwards directions respectively  #
$ One limitation of the BRNN is that cannot run continuously, as it requires a fixed endpoint in both the future and in the past  #
$ Further, it is not an appropriate machine learning algorithm for the online setting, as it is implausible to receive information from the future, i e , to know sequence elements that have not been observed  #
$ But for prediction over a sequence of fixed length, it is often sensible to take into account both past and future sequence elements  #
$ Consider the natural language task of part-of-speech tagging  #
$ Given any word in a sentence, information about both the words which precede and those which follow it is useful for predicting that word's part-of-speech  #
$ The LSTM and BRNN are in fact compatible ideas  #
$ The former introduces a new basic unit from which to compose a hidden layer, while the latter concerns the wiring of the hidden layers, regardless of what nodes they contain  #
$ Such an approach, termed a BLSTM has been used to achieve state of the art results on handwriting recognition and phoneme classification [Graves and Schmidhuber, 2005, Graves et al , 2009]  #
$ 4 3 Neural Turing machines The neural Turing machine (NTM) extends recurrent neural networks with an addressable external memory [Graves et al , 2014]  #
$ This work improves upon the ability of RNNs to perform complex algorithmic tasks such as sorting  #
$ The authors take inspiration from theories in cognitive science, which suggest humans possess a "central executive" that interacts with a memory buffer [Baddeley et al , 1996]  #
$ By analogy to a Turing machine, in which a program directs read heads and write heads to interact with external memory in the form of a tape, the model is named a Neural Turing Machine  #
$ While technical details of the read/write heads are beyond the scope of this review, we aim to convey a high-level sense of the model and its applications  #
$ The two primary components of an NTM are a controller and memory matrix  #
$ The controller, which may be a recurrent or feedforward neural network, takes input and returns output to the outside world, as well as passing instructions to and reading from the memory  #
$ The memory is represented by a large matrix of N memory locations, each of which is a vector of dimension M  Additionally, a number of read and write heads facilitate the interaction between 22 the controller and the memory matrix  #
$ Despite these additional capabilities, the NTM is differentiable end-to-end and can be trained by variants of stochastic gradient descent using BPTT  #
$ Graves et al  #
$ [2014] select five algorithmic tasks to test the performance of the NTM model  #
$ By algorithmic we mean that for each task, the target output for a given input can be calculated by following a simple program, as might be easily implemented in any universal programming language  #
$ One example is the copy task, where the input is a sequence of fixed length binary vectors followed by a delimiter symbol  #
$ The target output is a copy of the input sequence  #
$ In another task, priority sort, an input consists of a sequence of binary vectors together with a distinct scalar priority value for each vector  #
$ The target output is the sequence of vectors sorted by priority  #
$ The experiments test whether an NTM can be trained via supervised learning to implement these common algorithms correctly and efficiently  #
$ Interestingly, solutions found in this way generalize reasonably well to inputs longer than those presented in the training set  #
$ In contrast, the LSTM without external memory does not generalize well to longer inputs  #
$ The authors compare three different architectures, namely an LSTM RNN, an NTM with a feedforward controller, and an NTM with an LSTM controller  #
$ On each task, both NTM architectures significantly outperform the LSTM RNN both in training set performance and in generalization to test data  #
$ 5 Applications of LSTMs and BRNNs The previous sections introduced the building blocks from which nearly all state-of-the-art recurrent neural networks are composed  #
$ This section looks at several application areas where recurrent networks have been employed successfully  #
$ Before describing state of the art results in detail, it is appropriate to convey a concrete sense of the precise architectures with which many important tasks can be expressed clearly as sequence learning problems with recurrent neural networks  #
$ Figure 13 demonstrates several common RNN architectures and associates each with corresponding well-documented tasks  #
$ In the following subsections, we first introduce the representations of natural language used for input and output to recurrent neural networks and the commonly used performance metrics for sequence prediction tasks  #
$ Then we survey state-of-the-art results in machine translation, image captioning, video captioning, and handwriting recognition  #
$ Many applications of RNNs involve processing written language  #
$ Some applications, such as image captioning, involve generating strings of text  #
$ Others, such as machine translation and dialogue systems, require both inputting and outputting text  #
$ Systems which output text are more difficult to evaluate empirically than those which produce binary predictions or numerical output  #
$ As a result several methods have been developed to assess the quality of translations and captions  #
$ In the next subsection, we provide the background necessary to understand how text is represented in most modern recurrent net applications  #
$ We then explain the commonly reported evaluation metrics  #
$ 23 Figure 13: Recurrent neural networks have been used successfully to model both sequential inputs and sequential outputs as well as mappings between single data points and sequences (in both directions)  #
$ This figure, based on a similar figure in Karpathy [2015] shows how numerous tasks can be modeled with RNNs with sequential inputs and/or sequential outputs  #
$ In each subfigure, blue rectangles correspond to inputs, red rectangles to outputs and green rectangles to the entire hidden state of the neural network  #
$ (a) This is the conventional independent case, as assumed by standard feedforward networks  #
$ (b) Text and video classification are tasks in which a sequence is mapped to one fixed length vector  #
$ (c) Image captioning presents the converse case, where the input image is a single non-sequential data point  #
$ (d) This architecture has been used for natural language translation, a sequence-to-sequence task in which the two sequences may have varying and different lengths  #
$ (e) This architecture has been used to learn a generative model for text, predicting at each step the following character  #
$ 24 5 1 Representations of natural language inputs and outputs When words are output at each time step, generally the output consists of a softmax vector y(t) RK where K is the size of the vocabulary  #
$ A softmax layer is an element-wise logistic function that is normalized so that all of its components sum to one  #
$ Intuitively, these outputs correspond to the probabilities that each word is the correct output at that time step  #
$ For application where an input consists of a sequence of words, typically the words are fed to the network one at a time in consecutive time steps  #
$ In these cases, the simplest way to represent words is a one-hot encoding, using binary vectors with a length equal to the size of the vocabulary, so "1000" and "0100" would represent the first and second words in the vocabulary respectively  #
$ Such an encoding is discussed by Elman [1990] among others  #
$ However, this encoding is inefficient, requiring as many bits as the vocabulary is large  #
$ Further, it offers no direct way to capture different aspects of similarity between words in the encoding itself  #
$ Thus it is common now to model words with a distributed representation using a meaning vector  #
$ In some cases, these meanings for words are learned given a large corpus of supervised data, but it is more usual to initialize the meaning vectors using an embedding based on word co-occurrence statistics  #
$ Freely available code to produce word vectors from these statistics include GloVe [Pennington et al , 2014], and word2vec [Goldberg and Levy, 2014], which implements a word embedding algorithm from Mikolov et al  #
$ [2013]  #
$ Distributed representations for textual data were described by Hinton [1986], used extensively for natural language by Bengio et al  #
$ [2003], and more recently brought to wider attention in the deep learning community in a number of papers describing recursive auto-encoder (RAE) networks [Socher et al , 2010, 2011a,b,c]  #
$ For clarity we point out that these recursive networks are not recurrent neural networks, and in the present survey the abbreviation RNN always means recurrent neural network  #
$ While they are distinct approaches, recurrent and recursive neural networks have important features in common, namely that they both involve extensive weight tying and are both trained end-to-end via backpropagation  #
$ In many experiments with recurrent neural networks [Elman, 1990, Sutskever et al , 2011, Zaremba and Sutskever, 2014], input is fed in one character at a time, and output generated one character at a time, as opposed to one word at a time  #
$ While the output is nearly always a softmax layer, many papers omit details of how they represent single-character inputs  #
$ It seems reasonable to infer that characters are encoded with a one-hot encoding  #
$ We know of no cases of paper using a distributed representation at the single-character level  #
$ 5 2 Evaluation methodology A serious obstacle to training systems well to output variable length sequences of words is the flaws of the available performance metrics  #
$ In the case of captioning or translation, there maybe be multiple correct translations  #
$ Further, 25 a labeled dataset may contain multiple reference translations for each example  #
$ Comparing against such a gold standard is more fraught than applying standard performance measure to binary classification problems  #
$ One commonly used metric for structured natural language output with multiple references is BLEU score  #
$ Developed in 2002, BLEU score is related to modified unigram precision [Papineni et al , 2002]  #
$ It is the geometric mean of the n-gram precisions for all values of n between 1 and some upper limit N  In practice, 4 is a typical value for N, shown to maximize agreement with human raters  #
$ Because precision can be made high by offering excessively short translations, the BLEU score includes a brevity penalty B  #
$ Where c is average the length of the candidate translations and r the average length of the reference translations, the brevity penalty is B = 1 if c > r e(1-r/c) if c  r   #
$ Then the BLEU score is BLEU = B  exp 1 N N n=1 log pn where pn is the modified n-gram precision, which is the number of n-grams in the candidate translation that occur in any of the reference translations, divided by the total number of n-grams in the candidate translation  #
$ This is called modified precision because it is an adaptation of precision to the case of multiple references  #
$ BLEU scores are commonly used in recent papers to evaluate both translation and captioning systems  #
$ While BLEU score does appear highly correlated with human judgments, there is no guarantee that any given translation with a higher BLEU score is superior to another which receives a lower BLEU score  #
$ In fact, while BLEU scores tend to be correlated with human judgement across large sets of translations, they are not accurate predictors of human judgement at the single sentence level  #
$ METEOR is an alternative metric intended to overcome the weaknesses of the BLEU score [Banerjee and Lavie, 2005]  #
$ METEOR is based on explicit word to word matches between candidates and reference sentences  #
$ When multiple references exist, the best score is used  #
$ Unlike BLEU, METEOR exploits known synonyms and stemming  #
$ The first step is to compute an F-score F = P  R  P + (1 - )  R based on single word matches where P is the precision and R is the recall  #
$ The next step is to calculate a fragmentation penalty M  c/m where c is the smallest number of chunks of consecutive words such that the words are adjacent in both the candidate and the reference, and m is the total number of matched unigrams yielding the score  #
$ Finally, the score is METEOR = (1 - M)  F   26 Empirically, this metric has been found to agree with human raters more than BLEU score  #
$ However, METEOR is less straightforward to calculate than BLEU  #
$ To replicate the METEOR score reported by another party, one must exactly replicate their stemming and synonym matching, as well as the calculations  #
$ Both metrics rely upon having the exact same set of reference translations  #
$ Even in the straightforward case of binary classification, without sequential dependencies, commonly used performance metrics like F1 give rise to optimal thresholding strategies which may not accord with intuition about what should constitute good performance [Lipton et al , 2014]  #
$ Along the same lines, given that performance metrics such as the ones above are weak proxies for true objectives, it may be difficult to distinguish between systems which are truly stronger and those which most overfit the performance metrics in use  #
$ 5 3 Natural language translation Translation of text is a fundamental problem in machine learning that resists solutions with shallow methods  #
$ Some tasks, like document classification, can be performed successfully with a bag-of-words representation that ignores word order  #
$ But word order is essential in translation  #
$ The sentences "Scientist killed by raging virus" and "Virus killed by raging scientist" have identical bag-of-words representations  #
$ Sutskever et al  #
$ [2014] present a translation model using two multilayered LSTMs that demonstrates impressive performance translating from English to French  #
$ The first LSTM is used for encoding an input phrase from the source language and the second LSTM for decoding the output phrase in the target language  #
$ The model works according to the following procedure (Figure 14):  The source phrase is fed to the encoding LSTM one word at a time, which does not output anything  #
$ The authors found that significantly better results are achieved when the input sentence is fed into the network in reverse order  #
$ When the end of the phrase is reached, a special symbol that indicates the beginning of the output sentence is sent to the decoding LSTM  #
$ Additionally, the decoding LSTM receives as input the final state of the first LSTM  #
$ The second LSTM outputs softmax probabilities over the vocabulary at each time step  #
$ At inference time, beam search is used to choose the most likely words from the distribution at each time step, running the second LSTM until the endof-sentence (EOS) token is reached  #
$ For training, the true inputs are fed to the encoder, the true translation is fed to the decoder, and loss is propagated back from the outputs of the decoder across the entire sequence to sequence model  #
$ The network is trained to maximize the likelihood of the correct translation of each sentence in the training set  #
$ At inference time, a left to right beam search is used to determine 27 Figure 14: Sequence to sequence LSTM model of Sutskever et al  #
$ [2014]  #
$ The network consists of an encoding model (first LSTM) and a decoding model (second LSTM)  #
$ The input blocks (blue and purple) correspond to word vectors, which are fully connected to the corresponding hidden state  #
$ Red nodes are softmax outputs  #
$ Weights are tied among all encoding steps and among all decoding time steps  #
$ 28 which words to output  #
$ A few among the most likely next words are chosen for expansion after each time step  #
$ The beam search ends when the network outputs an end-of-sentence (EOS) token  #
$ Sutskever et al  #
$ [2014] train the model using stochastic gradient descent without momentum, halving the learning rate twice per epoch, after the first five epochs  #
$ The approach achieves a BLEU score of 34 81, outperforming the best previous neural network NLP systems, and matching the best published results for non-neural network approaches, including systems that have explicitly programmed domain expertise  #
$ When their system is used to rerank candidate translations from another system, it achieves a BLEU score of 36 5  #
$ The implementation which achieved these results involved eight GPUS  #
$ Nevertheless, training took 10 days to complete  #
$ One GPU was assigned to each layer of the LSTM, and an additional four GPUs were used simply to calculate softmax  #
$ The implementation was coded in C++, and each hidden layer of the LSTM contained 1000 nodes  #
$ The input vocabulary contained 160,000 words and the output vocabulary contained 80,000 words  #
$ Weights were initialized uniformly randomly in the range between -0 08 and 0 08  #
$ Another RNN approach to language translation is presented by Auli et al  #
$ [2013]  #
$ Their RNN model uses the word embeddings of Mikolov and a lattice representation of the decoder output to facilitate search over the space of possible translations  #
$ In the lattice, each node corresponds to a sequence of words  #
$ They report a BLEU score of 28 5 on French-English translation tasks  #
$ Both papers provide results on similar datasets but Sutskever et al  #
$ [2014] only report on English to French translation while Auli et al  #
$ [2013] only report on French to English translation, so it is not possible to compare the performance of the two models  #
$ 5 4 Image captioning Recently, recurrent neural networks have been used successfully for generating sentences that describe photographs [Vinyals et al , 2015, Karpathy and Fei-Fei, 2014, Mao et al , 2014]  #
$ In this task, a training set consists of input images x and target captions y  #
$ Given a large set of image-caption pairs, a model is trained to predict the appropriate caption for an image  #
$ Vinyals et al  #
$ [2015] follow up on the success in language to language translation by considering captioning as a case of image to language translation  #
$ Instead of both encoding and decoding with LSTMs, they introduce the idea of encoding an image with a convolutional neural network, and then decoding it with an LSTM  #
$ Mao et al  #
$ [2014] independently developed a similar RNN image captioning network, and achieved then state-of-the-art results on the Pascal, Flickr30K, and COCO datasets  #
$ Karpathy and Fei-Fei [2014] follows on this work, using a convolutional network to encode images together with a bidirectional network attention mech- anism and standard RNN to decode captions, using word2vec embeddings as word representations  #
$ They consider both full-image captioning and a model that captures correspondences between image regions and text snippets  #
$ At 29 inference time, their procedure resembles the one described by Sutskever et al  #
$ [2014], where sentences are decoded one word at a time  #
$ The most probable word is chosen and fed to the network at the next time step  #
$ This process is repeated until an EOS token is produced  #
$ To convey a sense of the scale of these problems, Karpathy and Fei-Fei [2014] focus on three datasets of captioned images: Flickr8K, Flickr30K, and COCO, of size 50MB (8000 images), 200MB (30,000 images), and 750MB (328,000 images) respectively  #
$ The implementation uses the Caffe library [Jia et al , 2014], and the convolutional network is pretrained on ImageNet data  #
$ In a revised version, the authors report that LSTMs outperform simpler RNNs and that learning word representations from random initializations is often preferable to word2vec embeddings  #
$ As an explanation, they say that word2vec embeddings may cluster words like colors together in the embedding space, which can be not suitable for visual descriptions of images  #
$ 5 5 Further applications Handwriting recognition is an application area where bidirectional LSTMs have been used to achieve state of the art results  #
$ In work by Liwicki et al  #
$ [2007] and Graves et al  #
$ [2009], data is collected from an interactive whiteboard  #
$ Sensors record the (x, y) coordinates of the pen at regularly sampled time steps  #
$ In the more recent paper, they use a bidirectional LSTM model, outperforming an HMM model by achieving 81 5% word-level accuracy, compared to 70 1% for the HMM  #
$ In the last year, a number of papers have emerged that extend the success of recurrent networks for translation and image captioning to new domains  #
$ Among the most interesting of these applications are unsupervised video encoding [Srivastava et al , 2015], video captioning [Venugopalan et al , 2015], and program execution [Zaremba and Sutskever, 2014]  #
$ Venugopalan et al  #
$ [2015] demonstrate a sequence to sequence architecture that encodes frames from a video and decode words  #
$ At each time step the input to the encoding LSTM is the topmost hidden layer of a convolutional neural network  #
$ At decoding time, the network outputs probabilities over the vocabulary at each time step  #
$ Zaremba and Sutskever [2014] experiment with networks which read computer programs one character at a time and predict their output  #
$ They focus on programs which output integers and find that for simple programs, including adding two nine-digit numbers, their network, which uses LSTM cells in several stacked hidden layers and makes a single left to right pass through the program, can predict the output with 99% accuracy  #
$ 6 Discussion Over the past thirty years, recurrent neural networks have gone from models primarily of interest for cognitive modeling and computational neuroscience, to 30 powerful and practical tools for large-scale supervised learning from sequences  #
$ This progress owes to advances in model architectures, training algorithms, and parallel computing  #
$ Recurrent networks are especially interesting because they overcome many of the restrictions placed on input and output data by traditional machine learning approaches  #
$ With recurrent networks, the assumption of independence between consecutive examples is broken, and hence also the assumption of fixed-dimension inputs and outputs  #
$ While LSTMs and BRNNs have set records in accuracy on many tasks in recent years, it is noteworthy that advances come from novel architectures rather than from fundamentally novel algorithms  #
$ Therefore, automating exploration of the space of possible models, for example via genetic algorithms or a Markov chain Monte Carlo approach, could be promising  #
$ Neural networks offer a wide range of transferable and combinable techniques  #
$ New activation functions, training procedures, initializations procedures, etc  #
$ are generally transferable across networks and tasks, often conferring similar benefits  #
$ As the number of such techniques grows, the practicality of testing all combinations diminishes  #
$ It seems reasonable to infer that as a community, neural network researchers are exploring the space of model architectures and configurations much as a genetic algorithm might, mixing and matching techniques, with a fitness function in the form of evaluation metrics on major datasets of interest  #
$ This inference suggests two corollaries  #
$ First, as just stated, this body of research could benefit from automated procedures to explore the space of models  #
$ Second, as we build systems designed to perform more complex tasks, we would benefit from improved fitness functions  #
$ A BLEU score inspires less con- fidence than the accuracy reported on a binary classification task  #
$ To this end, when possible, it seems prudent to individually test techniques first with classic feedforward networks on datasets with established benchmarks before applying them to recurrent networks in settings with less reliable evaluation criteria  #
$ Lastly, the rapid success of recurrent neural networks on natural language tasks leads us to believe that extensions of this work to longer texts would be fruitful  #
$ Additionally, we imagine that dialogue systems could be built along similar principles to the architectures used for translation, encoding prompts and generating responses, while retaining the entirety of conversation history as contextual information  #






$ LONG SHORT-TERM MEMORY Neural Computation 9(8):1735{1780, 1997 Sepp Hochreiter Fakultat fur Informatik Technische Universitat Munchen 80290 Munchen, Germany hochreit@informatik tu-muenchen de http://www7 informatik tu-muenchen de/~hochreit Jurgen Schmidhuber IDSIA Corso Elvezia 36 6900 Lugano, Switzerland juergen@idsia ch http://www idsia ch/~juergen Abstract Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insu cient, decaying error backflow  #
$ We brie y review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, e cient, gradient-based method called \Long Short-Term Memory" (LSTM)  #
$ Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant errorflow through \constant error carrousels" within special units  #
$ Multiplicative gate units learn to open and close access to the constant errorflow  #
$ LSTM is local in space and time; its computational complexity per time step and weight is O(1)  #
$ Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations  #
$ In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster  #
$ LSTM also solves complex, artificial long time lag tasks that have never been solved by previous recurrent network algorithms  #
$ 1 INTRODUCTION Recurrent networks can in principle use their feedback connections to store representations of recent input events in form of activations (\short-term memory", as opposed to \long-term memory" embodied by slowly changing weights)  #
$ This is potentially significant for many applications, including speech processing, non-Markovian control, and music composition (e g , Mozer 1992)  #
$ The most widely used algorithms for learning what to put in short-term memory, however, take too much time or do not work well at all, especially when minimal time lags between inputs and corresponding teacher signals are long  #
$ Although theoretically fascinating, existing methods do not provide clear practical advantages over, say, backprop in feedforward nets with limited time windows  #
$ This paper will review an analysis of the problem and suggest a remedy  #
$ The problem  #
$ With conventional \Back-Propagation Through Time" (BPTT, e g , Williams and Zipser 1992, Werbos 1988) or \Real-Time Recurrent Learning" (RTRL, e g , Robinson and Fallside 1987), error signals \ flowing backwards in time" tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error exponentially depends on the size of the weights (Hochreiter 1991)  #
$ Case (1) may lead to oscillating weights, while in case (2) learning to bridge long time lags takes a prohibitive amount of time, or does not work at all (see section 3)  #
$ The remedy  #
$ This paper presents \Long Short-Term Memory" (LSTM), a novel recurrent network architecture in conjunction with an appropriate gradient-based learning algorithm  #
$ LSTM is designed to overcome these error back-flow problems  #
$ It can learn to bridge time intervals in excess of 1000 steps even in case of noisy, incompressible input sequences, without loss of short time lag capabilities  #
$ This is achieved by an e cient, gradient-based algorithm for an architecture 1 enforcing constant (thus neither exploding nor vanishing) errorflow through internal states of special units (provided the gradient computation is truncated at certain architecture-specific points | this does not a ect long-term errorflow though)  #
$ Outlineof paper  #
$ Section 2 will brie y review previous work  #
$ Section 3 begins with an outline of the detailed analysis of vanishing errors due to Hochreiter (1991)  #
$ It will then introduce a naive approach to constant error backprop for didactic purposes, and highlight its problems concerning information storage and retrieval  #
$ These problems will lead to the LSTM architecture as described in Section 4  #
$ Section 5 will present numerous experiments and comparisons with competing methods  #
$ LSTM outperforms them, and also learns to solve complex, artificial tasks no other recurrent net algorithm has solved  #
$ Section 6 will discuss LSTM's limitations and advantages  #
$ The appendix contains a detailed description of the algorithm (A 1), and explicit errorflow formulae (A 2)  #
$ 2 PREVIOUS WORK This section will focus on recurrent nets with time-varying inputs (as opposed to nets with stationary inputs and xpoint-based gradient calculations, e g , Almeida 1987, Pineda 1987)  #
$ Gradient-descent variants  #
$ The approaches of Elman (1988), Fahlman (1991), Williams (1989), Schmidhuber (1992a), Pearlmutter (1989), and many of the related algorithms in Pearlmutter's comprehensive overview (1995) su er from the same problems as BPTT and RTRL (see Sections 1 and 3)  #
$ Time-delays  #
$ Other methods that seem practical for short time lags only are Time-Delay Neural Networks (Lang et al  #
$ 1990) and Plate's method (Plate 1993), which updates unit activations based on a weighted sum of old activations (see also de Vries and Principe 1991)  #
$ Lin et al  #
$ (1995) propose variants of time-delay networks called NARX networks  #
$ Time constants  #
$ To deal with long time lags, Mozer (1992) uses time constants influencing changes of unit activations (deVries and Principe's above-mentioned approach (1991) may in fact be viewed as a mixture of TDNN and time constants)  #
$ For long time lags, however, the time constants need external ne tuning (Mozer 1992)  #
$ Sun et al  #
$ 's alternative approach (1993) updates the activation of a recurrent unit by adding the old activation and the (scaled) current net input  #
$ The net input, however, tends to perturb the stored information, which makes long-term storage impractical  #
$ Ring's approach  #
$ Ring (1993) also proposed a method for bridging long time lags  #
$ Whenever a unit in his network receives con icting error signals, he adds a higher order unit in uencing appropriate connections  #
$ Although his approach can sometimes be extremely fast, to bridge a time lag involving 100 steps may require the addition of 100 units  #
$ Also, Ring's net does not generalize to unseen lag durations  #
$ Bengio et al  #
$ 's approaches  #
$ Bengio et al  #
$ (1994) investigate methods such as simulated annealing, multi-grid random search, time-weighted pseudo-Newton optimization, and discrete error propagation  #
$ Their \latch" and \2-sequence" problems are very similar to problem 3a with minimal time lag 100(see Experiment 3)  #
$ Bengioand Frasconi(1994)also proposean EM approach for propagating targets  #
$ With n so-called \state networks", at a given time, their system can be in one of only n different states  #
$ See also beginning of Section 5  #
$ But to solve continuous problems such as the \adding problem" (Section 5 4), their system would require an unacceptable number of states (i e , state networks)  #
$ Kalman lters  #
$ Puskorius and Feldkamp (1994) use Kalman lter techniques to improve recurrent net performance  #
$ Since they use \a derivative discount factor imposed to decay exponentially the e ects of past dynamic derivatives," there is no reason to believe that their Kalman Filter Trained Recurrent Networks will be useful for very long minimal time lags  #
$ Second order nets  #
$ We will see that LSTM uses multiplicative units (MUs) to protect errorflow from unwanted perturbations  #
$ It is not the rst recurrent net method using MUs though  #
$ For instance, Watrous and Kuhn (1992) use MUs in second order nets  #
$ Some differences to LSTM are: (1) Watrous and Kuhn's architecture does not enforce constant errorflow and is not designed 2 to solve long time lag problems  #
$ (2) It has fully connected second-order sigma-pi units, while the LSTM architecture's MUs are used only to gate access to constant errorflow  #
$ (3) Watrous and Kuhn's algorithm costs O(W2) operations per time step, ours only O(W), where W is the number of weights  #
$ See also Miller and Giles (1993) for additional work on MUs  #
$ Simple weight guessing  #
$ To avoid long time lag problems of gradient-based approaches we may simply randomly initialize all network weights until the resulting net happens to classify all training sequences correctly  #
$ In fact, recently we discovered (Schmidhuber and Hochreiter 1996, Hochreiter and Schmidhuber 1996, 1997) that simple weight guessing solves many of the problems in (Bengio 1994, Bengio and Frasconi 1994, Miller and Giles 1993, Lin et al  #
$ 1995) faster than the algorithms proposed therein  #
$ This does not mean that weight guessing is a good algorithm  #
$ It just means that the problems are very simple  #
$ More realistic tasks require either many free parameters (e g , input weights) or high weight precision (e g , for continuous-valued parameters), such that guessing becomes completely infeasible  #
$ Adaptive sequence chunkers  #
$ Schmidhuber's hierarchical chunker systems (1992b, 1993) do have a capability to bridge arbitrary time lags, but only if there is local predictability across the subsequences causing the time lags (see also Mozer 1992)  #
$ For instance, in his postdoctoral thesis (1993), Schmidhuber uses hierarchical recurrent nets to rapidly solve certain grammar learning tasks involving minimal time lags in excess of 1000 steps  #
$ The performance of chunker systems, however, deteriorates as the noise level increases and the input sequences become less compressible  #
$ LSTM does not suffer from this problem  #
$ 3 CONSTANT ERROR BACKPROP 3 1 EXPONENTIALLY DECAYING ERROR Conventional BPTT (e g  #
$ Williams and Zipser 1992)  #
$ Output unit k's target at time t is denoted by dk(t)  #
$ Using mean squared error, k's error signal is #k(t) = f0 k(netk(t))(dk(t) ?yk(t)); where yi(t) = fi(neti(t)) is the activation of a non-input unit i with differentiable activation function fi, neti(t) = X j wijyj(t?1) is unit i's current net input, and wij is the weight on the connection from unit j to i  #
$ Some non-output unit j's backpropagated error signal is #j(t) = f0 j(netj(t)) X i wij#i(t + 1): The corresponding contribution to wjl's total weight update is #j(t)yl(t ? #
$ 1), where is the learning rate, and l stands for an arbitrary unit connected to unit j  #
$ Outline of Hochreiter's analysis (1991, page 19-21)  #
$ Suppose we have a fully connected net whose non-input unit indices range from 1 to n  Let us focus on local errorflow from unit u to unit v (later we will see that the analysis immediately extends to global errorflow)  #
$ The error occurring at an arbitrary unit u at time step t is propagated \back into time" for q time steps, to an arbitrary unit v  This will scale the error by the following factor: @#v(t?q) @#u(t) = ( f0 v(netv(t ?1))wuv q = 1 f0 v(netv(t ?q)) Pn l=1 @#l(t?q+1) @#u(t) wlv q > 1 : (1) 3 With lq = v and l0 = u, we obtain: @#v(t?q) @#u(t) = n X l1=1 ::: n X lq?1=1 q Y m=1 f0 lm(netlm(t ?m))wlmlm?1 (2) (proof by induction)  #
$ The sum of the nq?1 terms Qq m=1 f0 lm(netlm(t?m))wlmlm?1 determines the total error back flow (note that since the summation terms may have different signs, increasing the number of units n does not necessarily increase errorflow)  #
$ Intuitive explanation of equation (2)  #
$ If jf0 lm(netlm(t ?m))wlmlm?1j > 1:0 for all m (as can happen, e g , with linear flm ) then the largest product increases exponentially with q  #
$ That is, the error blows up, and con icting error signals arriving at unit v can lead to oscillating weights and unstable learning (for error blow-ups or bifurcations see also Pineda 1988, Baldi and Pineda 1991, Doya 1992)  #
$ On the other hand, if jf0 lm(netlm(t ?m))wlmlm?1j < 1:0 for all m, then the largest product decreases exponentially with q  #
$ That is, the error vanishes, and nothing can be learned in acceptable time  #
$ If flm is the logistic sigmoid function, then the maximal value of f0 lm is 0 25  #
$ If ylm?1 is constant and not equal to zero, then jf0 lm(netlm)wlmlm?1j takes on maximal values where wlmlm?1 = 1 ylm?1 coth(1 2netlm); goes to zero for jwlmlm?1j ! #
$ 1, and is less than 1:0 for jwlmlm?1j < 4:0 (e g , if the absolute max- imal weight value wmax is smaller than 4 0)  #
$ Hence with conventional logistic sigmoid activation functions, the errorflow tends to vanish as long as the weights have absolute values below 4 0, especially in the beginning of the training phase  #
$ In general the use of larger initial weights will not help though | as seen above, for jwlmlm?1j ! #
$ 1 the relevant derivative goes to zero \faster" than the absolute weight can grow (also, some weights will have to change their signs by crossing zero)  #
$ Likewise, increasing the learning rate does not help either | it will not change the ratio of long-range errorflow and short-range errorflow  #
$ BPTT is too sensitive to recent distractions  #
$ (A very similar, more recent analysis was presented by Bengio et al  #
$ 1994)  #
$ Global errorflow  #
$ The local errorflow analysis above immediately shows that global errorflow vanishes, too  #
$ To see this, compute X u: u output unit @#v(t ?q) @#u(t) : Weak upper bound for scaling factor  #
$ The following, slightly extended vanishing error analysis also takes n, the number of units, into account  #
$ m)]ij := f0 i(neti(t ? #
$ m)) otherwise  #
$ Here T is the transposition operator, A]ij is the element in the i-th column and j-th row of matrix A, and x]i is the i-th component of vector x  #
$ 4 Using a matrix norm k : kA compatible with vector norm k : kx, we de ne f0 max := maxm=1;:::;qfk F0(t?m) kAg: For maxi=1;:::;nfjxijg k x kx we get jxTyj n k x kx k y kx : Since jf0 v(netv(t ?q))j k F0(t?q) kA f0 max; we obtain the following inequality: j @#v(t ?q) @#u(t) j n (f0 max)q k Wv kx k WuT kx k W kq?2 A n(f0 max k W kA)q : This inequality results from k Wv kx = k Wev kx k W kA k ev kx k W kA and k WuT kx = k euW kx k W kA k eu kx k W kA; where ek is the unit vector whose components are 0 except for the k-th component, which is 1  #
$ Note that this is a weak, extreme case upper bound | it will be reached only if all k F0(t?m) kA take on maximal values, and if the contributions of all paths across which errorflows back from unit u to unit v have the same sign  #
$ Large k W kA, however, typically result in small values of k F0(t?m) kA, as con rmed by experiments (see, e g , Hochreiter 1991)  #
$ For example, with norms k W kA := maxr X s jwrsj and k x kx:= maxrjxrj; we have f0 max = 0:25 for the logistic sigmoid  #
$ We observe that if jwijj wmax < 4:0 n 8i;j; then k W kA nwmax < 4:0 will result in exponential decay | by setting := ?nwmax 4:0 < 1:0, we obtain j @#v(t ?q) @#u(t) j n( )q : We refer to Hochreiter's 1991 thesis for additional results  #
$ 3 2 CONSTANT ERROR FLOW: NAIVE APPROACH A single unit  #
$ To avoid vanishing error signals, how can we achieve constant errorflow through a single unit j with a single connection to itself? #
$ According to the rules above, at time t, j's local error back flow is #j(t) = f0 j(netj(t))#j(t + 1)wjj  #
$ To enforce constant errorflow through j, we require f0 j(netj(t))wjj = 1:0: Note the similarity to Mozer's xed time constant system (1992) | a time constant of 1:0 is appropriate for potentially in nite time lags1  #
$ The constant error carrousel  #
$ Integrating the differential equation above, we obtain fj(netj(t)) = netj(t) wjj for arbitrary netj(t)  #
$ This means: fj has to be linear, and unit j's acti- vation has to remain constant: yj(t + 1) = fj(netj(t + 1)) = fj(wjjyj(t)) = yj(t): 1We do not use the expression \time constant" in the differential sense, as, e g , Pearlmutter (1995)  #
$ 5 In the experiments, this will be ensured by using the identity function fj : fj(x) = x;8x, and by setting wjj = 1:0  #
$ We refer to this as the constant error carrousel (CEC)  #
$ CEC will be LSTM's central feature (see Section 4)  #
$ Of course unit j will not only be connected to itself but also to other units  #
$ This invokes two obvious, related problems (also inherent in all other gradient-based approaches): 1  #
$ Input weight con ict: for simplicity, let us focus on a single additional input weight wji  #
$ Assume that the total error can be reduced by switching on unit j in response to a certain input, and keeping it active for a long time (until it helps to compute a desired output)  #
$ Provided i is non-zero, since the same incoming weight has to be used for both storing certain inputs and ignoring others, wji will often receive con icting weight update signals during this time (recall that j is linear): these signals will attempt to make wji participate in (1) storing the input (by switching on j) and (2) protecting the input (by preventing j from being switched o by irrelevant later inputs)  #
$ This con ict makes learning difficult \long time lag errors"  #
$ Again, this con ict makes learning difficult, and calls for a more context-sensitive mechanism for controlling \read operations" through output weights  #
$ Of course, input and output weight conflicts are not specific for long time lags, but occur for short time lags as well  #
$ Their e ects, however, become particularly pronounced in the long time lag case: as the time lag increases, (1) stored information must be protected against perturbation for longer and longer periods, and | especially in advanced stages of learning | (2) more and more already correct outputs also require protection against perturbation  #
$ Due to the problems above the naive approach does not work well except in case of certain simple problems involving local input/output representations and non-repeating input patterns (see Hochreiter 1991 and Silva et al  #
$ 1996)  #
$ The next section shows how to do it right  #
$ 4 LONG SHORT-TERM MEMORY Memory cells and gate units  #
$ To construct an architecture that allows for constant errorflow through special, self-connected units without the disadvantages of the naive approach, we extend the constant error carrousel CEC embodied by the self-connected, linear unit j from Section 3 2 by introducing additional features  #
$ A multiplicative input gate unit is introduced to protect the memory contents stored in j from perturbation by irrelevant inputs  #
$ Likewise, a multiplicative output gate unit is introduced which protects other units from perturbation by currently irrelevant memory contents stored in j  #
$ The resulting, more complex unit is called a memory cell (see Figure 1)  #
$ The j-th memory cell is denoted cj  #
$ Each memory cell is built around a central linear unit with a xed self-connection (the CEC)  #
$ In addition to netcj, cj gets input from a multiplicative unit outj (the \output gate"), and from another multiplicative unit inj (the \input gate")  #
$ inj's activation at time t is denoted by yinj(t), outj's by youtj(t)  #
$ We have youtj(t) = foutj(netoutj(t));yinj(t) = finj(netinj(t)); 6 where netoutj(t) = X u woutjuyu(t ?1); and netinj(t) = X u winjuyu(t?1): We also have netcj(t) = X u wcjuyu(t?1): The summation indices u may stand for input units, gate units, memory cells, or even conventional hidden units if there are any (see also paragraph on \network topology" below)  #
$ All these different types of units may convey useful information about the current state of the net  #
$ For instance, an input gate (output gate) may use inputs from other memory cells to decide whether to store (access) certain information in its memory cell  #
$ There even may be recurrent self-connections like wcjcj  #
$ It is up to the user to de ne the network topology  #
$ See Figure 2 for an example  #
$ At time t, cj's output ycj(t) is computed as ycj(t) = youtj(t)h(scj(t)); where the \internal state" scj(t) is scj(0) = 0;scj(t) = scj(t?1) + yinj(t)g ?netcj(t) for t > 0: The differentiable function g squashes netcj; the differentiable function h scales memory cell outputs computed from the internal state scj  #
$ inj inj outj outj w i c j wic j yc j g h 1 0 net w i w i yinj youtj net c j g yinj = g + sc j sc j yinj h youtj net Figure 1: Architecture of memory cell cj (the box) and its gate units inj;outj  #
$ The self-recurrent connection (with weight 1 0) indicates feedback with a delay of 1 time step  #
$ It builds the basis of the \constant error carrousel" CEC  #
$ The gate units open and close access to CEC  #
$ See text and appendix A 1 for details  #
$ Why gate units? #
$ To avoid input weight conflicts, inj controls the errorflow to memory cell cj's input connections wcji  #
$ To circumvent cj's output weight conflicts, outj controls the errorflow from unit j's output connections  #
$ In other words, the net can use inj to decide when to keep or override information in memory cell cj, and outj to decide when to access memory cell cj and when to prevent other units from being perturbed by cj (see Figure 1)  #
$ Error signals trapped within a memory cell's CEC cannot change { but different error signals flowing into the cell (at different times) via its output gate may get superimposed  #
$ The output gate will have to learn which errors to trap in its CEC, by appropriately scaling them  #
$ The input 7 gate will have to learn when to release errors, again by appropriately scaling them  #
$ Essentially, the multiplicative gate units open and close access to constant errorflow through CEC  #
$ Distributed output representations typically do require output gates  #
$ Not always are both gate types necessary, though | one may be su cient  #
$ For instance, in Experiments 2a and 2b in Section 5, it will be possible to use input gates only  #
$ In fact, output gates are not required in case of local output encoding | preventing memory cells from perturbing already learned outputs can be done by simply setting the corresponding weights to zero  #
$ Even in this case, however, output gates can be beneficial: they prevent the net's attempts at storing long time lag memories (which are usually hard to learn) from perturbing activations representing easily learnable short time lag memories  #
$ (This will prove quite useful in Experiment 1, for instance ) #
$ Network topology  #
$ We use networks with one input layer, one hidden layer, and one output layer  #
$ The (fully) self-connected hidden layer contains memory cells and corresponding gate units (for convenience, we refer to both memory cells and gate units as being located in the hidden layer)  #
$ The hidden layer may also contain \conventional" hidden units providing inputs to gate units and memory cells  #
$ All units (except for gate units) in all layers have directed connections (serve as inputs) to all units in the layer above (or to all higher layers { Experiments 2a and 2b)  #
$ Memory cell blocks  #
$ S memory cells sharing the same input gate and the same output gate form a structure called a \memory cell block of size S"  #
$ Memory cell blocks facilitate information storage | as with conventional neural nets, it is not so easy to code a distributed input within a single cell  #
$ Since each memory cell block has as many gate units as a single memory cell (namely two), the block architecture can be even slightly more e cient (see paragraph \computational complexity")  #
$ A memory cell block of size 1 is just a simple memory cell  #
$ In the experiments (Section 5), we will use memory cell blocks of various sizes  #
$ Learning  #
$ We use a variant of RTRL (e g , Robinson and Fallside 1987) which properly takes into account the altered, multiplicative dynamics caused by input and output gates  #
$ However, to ensure non-decaying error backprop through internal states of memory cells, as with truncated BPTT (e g , Williams and Peng 1990), errors arriving at \memory cell net inputs" (for cell cj, this includes netcj, netinj, netoutj) do not get propagated back further in time (although they do serve to change the incoming weights)  #
$ Only within2 memory cells, errors are propagated back through previous internal states scj  #
$ To visualize this: once an error signal arrives at a memory cell output, it gets scaled by output gate activation and h0  #
$ Then it is within the memory cell's CEC, where it can flow back indefinitely without ever being scaled  #
$ Only when it leaves the memory cell through the input gate and g, it is scaled once more by input gate activation and g0  #
$ It then serves to change the incoming weights before it is truncated (see appendix for explicit formulae)  #
$ Computational complexity  #
$ As with Mozer's focused recurrent backprop algorithm (Mozer 1989), only the derivatives @scj @wil need to be stored and updated  #
$ Hence the LSTM algorithm is very e cient, with an excellent update complexity of O(W), where W the number of weights (see details in appendix A 1)  #
$ Hence, LSTM and BPTT for fully recurrent nets have the same update complexity per time step (while RTRL's is much worse)  #
$ Unlike full BPTT, however, LSTM is local in space and time3: there is no need to store activation values observed during sequence processing in a stack with potentially unlimited size  #
$ Abuse problem and solutions  #
$ In the beginning of the learning phase, error reduction may be possible without storing information over time  #
$ The network will thus tend to abuse memory cells, e g , as bias cells (i e , it might make their activations constant and use the outgoing connections as adaptive thresholds for other units)  #
$ The potential difficulty is: it may take a long time to release abused memory cells and make them available for further learning  #
$ A similar \abuse problem" appears if two memory cells store the same (redundant) information  #
$ There are at least two solutions to the abuse problem: (1) Sequential network construction (e g , Fahlman 1991): a memory cell and the corresponding gate units are added to the network whenever the 2For intra-cellular backprop in a quite different context see also Doya and Yoshizawa (1989)  #
$ 3Following Schmidhuber (1989), we say that a recurrent net algorithm is local in space if the update complexity per time step and weight does not depend on network size  #
$ We say that a method is local in time if its storage requirements do not depend on input sequence length  #
$ For instance, RTRL is local in time but not in space  #
$ BPTT is local in space but not in time  #
$ 8 1 1 2 output hidden input out 1 in 1 out 2 in 2 1 cell block block 1 cell block block 2 cell 2 cell 2 Figure 2: Example of a net with 8 input units, 4 output units, and 2 memory cell blocks of size 2  in1 marks the input gate, out1 marks the output gate, and cell1=block1 marks the rst memory cell of block 1  cell1=block1's architecture is identical to the one in Figure 1, with gate units in1 and out1 (note that by rotating Figure 1 by 90 degrees anti-clockwise, it will match with the corresponding parts of Figure 1)  #
$ The example assumes dense connectivity: each gate unit and each memory cell see all non-output units  #
$ For simplicity, however, outgoing weights of only one type of unit are shown for each layer  #
$ With the e cient, truncated update rule, errorflows only through connections to output units, and through xed self-connections within cell blocks (not shown here | see Figure 1)  #
$ Error flow is truncated once it \wants" to leave memory cells or gate units  #
$ Therefore, no connection shown above serves to propagate error back to the unit from which the connection originates (except for connections to output units), although the connections themselves are modi able  #
$ That's why the truncated LSTM algorithm is so e cient, despite its ability to bridge very long time lags  #
$ See text and appendix A 1 for details  #
$ Figure 2 actually shows the architecture used for Experiment 6a | only the bias of the non-input units is omitted  #
$ error stops decreasing (see Experiment 2 in Section 5)  #
$ (2) Output gate bias: each output gate gets a negative initial bias, to push initial memory cell activations towards zero  #
$ Memory cells with more negative bias automatically get \allocated" later (see Experiments 1, 3, 4, 5, 6 in Section 5)  #
$ Internal state drift and remedies  #
$ If memory cell cj's inputs are mostly positive or mostly negative, then its internal state sj will tend to drift away over time  #
$ This is potentially dangerous, for the h0(sj) will then adopt very small values, and the gradient will vanish  #
$ One way to circumvent this problem is to choose an appropriate function h  But h(x) = x, for instance, has the disadvantage of unrestricted memory cell output range  #
$ Our simple but e ective way of solving drift problems at the beginning of learning is to initially bias the input gate inj towards zero  #
$ Although there is a tradeo between the magnitudes of h0(sj) on the one hand and of yinj and f0 inj on the other, the potential negative e ect of input gate bias is negligible compared to the one of the drifting e ect  #
$ With logistic sigmoid activation functions, there appears to be no need for ne-tuning the initial bias, as con rmed by Experiments 4 and 5 in Section 5 4  #
$ 5 EXPERIMENTS Introduction  #
$ Which tasks are appropriate to demonstrate the quality of a novel long time lag 9 algorithm? #
$ First of all, minimal time lags between relevant input signals and corresponding teacher signals must be long for all training sequences  #
$ In fact, many previous recurrent net algorithms sometimes manage to generalize from very short training sequences to very long test sequences  #
$ See, e g , Pollack (1991)  #
$ But a real long time lag problem does not have any short time lag exemplars in the training set  #
$ For instance, Elman's training procedure, BPTT, o ine RTRL, online RTRL, etc , fail miserably on real long time lag problems  #
$ See, e g , Hochreiter (1991) and Mozer (1992)  #
$ A second important requirement is that the tasks should be complex enough such that they cannot be solved quickly by simple-minded strategies such as random weight guessing  #
$ Guessing can outperform many long time lag algorithms  #
$ Recently we discovered (Schmidhuber and Hochreiter 1996, Hochreiter and Schmidhuber 1996, 1997) that many long time lag tasks used in previous work can be solved more quickly by simple random weight guessing than by the proposed algorithms  #
$ For instance, guessing solved a variant of Bengio and Frasconi's \parity problem" (1994) problem much faster4 than the seven methods tested by Bengio et al  #
$ (1994) and Bengio and Frasconi (1994)  #
$ Similarly for some of Miller and Giles' problems (1993)  #
$ Of course, this does not mean that guessing is a good algorithm  #
$ It just means that some previously used problems are not extremely appropriate to demonstrate the quality of previously proposed algorithms  #
$ What's common to Experiments 1{6  #
$ All our experiments (except for Experiment 1) involve long minimal time lags | there are no short time lag training exemplars facilitating learning  #
$ Solutions to most of our tasks are sparse in weight space  #
$ They require either many parameters/inputs or high weight precision, such that random weight guessing becomes infeasible  #
$ We always use on-line learning (as opposed to batch learning), and logistic sigmoids as activation functions  #
$ For Experiments 1 and 2, initial weights are chosen in the range ?0:2;0:2], for the other experiments in ?0:1;0:1]  #
$ Training sequences are generated randomly according to the various task descriptions  #
$ In slight deviation from the notation in Appendix A1, each discrete time step of each input sequence involves three processing steps: (1) use current input to set the input units  #
$ (2) Compute activations of hidden units (including input gates, output gates, memory cells)  #
$ (3) Compute output unit activations  #
$ Except for Experiments 1, 2a, and 2b, sequence elements are randomly generated on-line, and error signals are generated only at sequence ends  #
$ Net activations are reset after each processed input sequence  #
$ For comparisons with recurrent nets taught by gradient descent, we give results only for RTRL, except for comparison 2a, which also includes BPTT  #
$ Note, however, that untruncated BPTT (see, e g , Williams and Peng 1990) computes exactly the same gradient as o ine RTRL  #
$ With long time lag problems, o ine RTRL (or BPTT) and the online version of RTRL (no activation resets, online weight changes) lead to almost identical, negative results (as con rmed by additional simulations in Hochreiter 1991; see also Mozer 1992)  #
$ This is because o ine RTRL, online RTRL, and full BPTT all su er badly from exponential error decay  #
$ Our LSTM architectures are selected quite arbitrarily  #
$ If nothing is known about the complexity of a given problem, a more systematic approach would be: start with a very small net consisting of one memory cell  #
$ If this does not work, try two cells, etc  #
$ Alternatively, use sequential network construction (e g , Fahlman 1991)  #
$ Outline of experiments  #
$ Experiment 1 focuses on a standard benchmark test for recurrent nets: the embedded Reber grammar  #
$ Since it allows for training sequences with short time lags, it is not a long time lag problem  #
$ We include it because (1) it provides a nice example where LSTM's output gates are truly beneficial, and (2) it is a popular benchmark for recurrent nets that has been used by many authors | we want to include at least one experiment where conventional BPTT and RTRL do not fail completely (LSTM, however, clearly outperforms them)  #
$ The embedded Reber grammar's minimal time lags represent a border case in the sense that it is still possible to learn to bridge them with conventional algorithms  #
$ Only slightly longer 4It should be mentioned, however, that different input representations and different types of noise may lead to worse guessing performance (Yoshua Bengio, personal communication, 1996)  #
$ 10 minimal time lags would make this almost impossible  #
$ The more interesting tasks in our paper, however, are those that RTRL, BPTT, etc  #
$ cannot solve at all  #
$ Experiment 2 focuses on noise-free and noisy sequences involving numerous input symbols distracting from the few important ones  #
$ The most difficult task (Task 2c) involves hundreds of distractor symbols at random positions, and minimal time lags of 1000 steps  #
$ LSTM solves it, while BPTT and RTRL already fail in case of 10-step minimal time lags (see also, e g , Hochreiter 1991 and Mozer 1992)  #
$ For this reason RTRL and BPTT are omitted in the remaining, more complex experiments, all of which involve much longer time lags  #
$ Experiment 3 addresses long time lag problems with noise and signal on the same input line  #
$ Experiments 3a/3b focus on Bengio et al  #
$ 's 1994 \2-sequence problem"  #
$ Because this problem actually can be solved quickly by random weight guessing, we also include a far more difficult 2-sequence problem (3c) which requires to learn real-valued, conditional expectations of noisy targets, given the inputs  #
$ Experiments 4 and 5 involve distributed, continuous-valued input representations and require learning to store precise, real values for very long time periods  #
$ Relevant input signals can occur at quite different positions in input sequences  #
$ Again minimal time lags involve hundreds of steps  #
$ Similar tasks never have been solved by other recurrent net algorithms  #
$ Experiment 6 involves tasks of a different complex type that also has not been solved by other recurrent net algorithms  #
$ Again, relevant input signals can occur at quite different positions in input sequences  #
$ The experiment shows that LSTM can extract information conveyed by the temporal order of widely separated inputs  #
$ Subsection 5 7 will provide a detailed summary of experimental conditions in two tables for reference  #
$ 5 1 EXPERIMENT 1: EMBEDDED REBER GRAMMAR Task  #
$ Our rst task is to learn the \embedded Reber grammar", e g  #
$ Smith and Zipser (1989), Cleeremans et al  #
$ (1989), and Fahlman (1991)  #
$ Since it allows for training sequences with short time lags (of as few as 9 steps), it is not a long time lag problem  #
$ We include it for two reasons: (1) it is a popular recurrent net benchmark used by many authors | we wanted to have at least one experiment where RTRL and BPTT do not fail completely, and (2) it shows nicely how output gates can be beneficial  #
$ B T S X X P V T P V S E Figure 3: Transition diagram for the Reber grammar  #
$ B T P E T P GRAMMAR GRAMMAR REBER REBER Figure 4: Transition diagram for the embedded Reber grammar  #
$ Each box represents a copy of the Reber grammar (see Figure 3)  #
$ Starting at the leftmost node of the directed graph in Figure 4, symbol strings are generated sequentially (beginning with the empty string) by following edges | and appending the associated 11 symbols to the current string | until the rightmost node is reached  #
$ Edges are chosen randomly if there is a choice (probability: 0 5)  #
$ The net's task is to read strings, one symbol at a time, and to permanently predict the next symbol (error signals occur at every time step)  #
$ To correctly predict the symbol before last, the net has to remember the second symbol  #
$ Comparison  #
$ We compare LSTM to \Elman nets trained by Elman's training procedure" (ELM) (results taken from Cleeremans et al  #
$ 1989), Fahlman's \Recurrent Cascade-Correlation" (RCC) (results taken from Fahlman 1991), and RTRL (results taken from Smith and Zipser (1989), where only the few successful trials are listed)  #
$ It should be mentioned that Smith and Zipser actually make the task easier by increasing the probability of short time lag exemplars  #
$ We didn't do this for LSTM  #
$ Training/Testing  #
$ We use a local input/output representation (7 input units, 7 output units)  #
$ Following Fahlman, we use 256 training strings and 256 separate test strings  #
$ The training set is generated randomly; training exemplars are picked randomly from the training set  #
$ Test sequences are generated randomly, too, but sequences already used in the training set are not used for testing  #
$ After string presentation, all activations are reinitialized with zeros  #
$ A trial is considered successful if all string symbols of all sequences in both test set and training set are predicted correctly | that is, if the output unit(s) corresponding to the possible next symbol(s) is(are) always the most active ones  #
$ Architectures  #
$ Architectures for RTRL, ELM, RCC are reported in the references listed above  #
$ For LSTM, we use 3 (4) memory cell blocks  #
$ Each block has 2 (1) memory cells  #
$ The output layer's only incoming connections originate at memory cells  #
$ Each memory cell and each gate unit receives incoming connections from all memory cells and gate units (the hidden layer is fully connected | less connectivity may work as well)  #
$ The input layer has forward connections to all units in the hidden layer  #
$ The gate units are biased  #
$ These architecture parameters make it easy to store at least 3 input signals (architectures 3-2 and 4-1 are employed to obtain comparable numbers of weights for both architectures: 264 for 4-1 and 276 for 3-2)  #
$ Other parameters may be appropriate as well, however  #
$ All sigmoid functions are logistic with output range 0;1], except for h, whose range is ?1;1], and g, whose range is ?2;2]  #
$ All weights are initialized in ?0:2;0:2], except for the output gate biases, which are initialized to -1, -2, and -3, respectively (see abuse problem, solution (2) of Section 4)  #
$ We tried learning rates of 0 1, 0 2 and 0 5  #
$ Results  #
$ We use 3 different, randomly generated pairs of training and test sets  #
$ With each such pair we run 10 trials with different initial weights  #
$ See Table 1 for results (mean of 30 trials)  #
$ Unlike the other methods, LSTM always learns to solve the task  #
$ Even when we ignore the unsuccessful trials of the other approaches, LSTM learns much faster  #
$ Importanceof output gates  #
$ The experiment provides a nice example where the output gate is truly beneficial  #
$ Learning to store the rst T or P should not perturb activations representing the more easily learnable transitions of the original Reber grammar  #
$ This is the job of the output gates  #
$ Without output gates, we did not achieve fast learning  #
$ 5 2 EXPERIMENT 2: NOISE-FREE AND NOISY SEQUENCES Task 2a: noise-free sequences with long time lags  #
$ There are p+1 possible input symbols denoted a1;:::;ap?1;ap = x;ap+1 = y  ai is \locally" represented by the p + 1-dimensional vector whose i-th component is 1 (all other components are 0)  #
$ A net with p + 1 input units and p + 1 output units sequentially observes input symbol sequences, one at a time, permanently trying to predict the next symbol | error signals occur at every single time step  #
$ To emphasize the \long time lag problem", we use a training set consisting of only two very similar sequences: (y;a1;a2;:::;ap?1;y) and (x;a1;a2;:::;ap?1;x)  #
$ Each is selected with probability 0 5  #
$ To predict the nal element, the net has to learn to store a representation of the rst element for p time steps  #
$ We compare \Real-Time Recurrent Learning" for fully recurrent nets (RTRL), \Back-Propagation Through Time" (BPTT), the sometimes very successful 2-net \Neural Sequence Chunker" (CH, Schmidhuber 1992b), and our new method (LSTM)  #
$ In all cases, weights are initialized in -0 2,0 2]  #
$ 1989), \Recurrent Cascade-Correlation" (results taken from Fahlman 1991) and our new approach (LSTM)  #
$ Weight numbers in the rst 4 rows are estimates | the corresponding papers do not provide all the technical details  #
$ Only LSTM almost always learns to solve the task (only two failures out of 150 trials)  #
$ Even when we ignore the unsuccessful trials of the other approaches, LSTM learns much faster (the number of required training examples in the bottom row varies between 3,800 and 24,100)  #
$ tations  #
$ A successful run is one that ful lls the following criterion: after training, during 10,000 successive, randomly chosen input sequences, the maximal absolute error of all output units is always below 0:25  #
$ Architectures  #
$ RTRL: one self-recurrent hidden unit, p+1 non-recurrent output units  #
$ Each layer has connections from all layers below  #
$ All units use the logistic activation function sigmoid in 0,1]  #
$ BPTT: same architecture as the one trained by RTRL  #
$ CH: both net architectures like RTRL's, but one has an additional output for predicting the hidden unit of the other one (see Schmidhuber 1992b for details)  #
$ LSTM: like with RTRL, but the hidden unit is replaced by a memory cell and an input gate (no output gate required)  #
$ g is the logistic sigmoid, and h is the identity function h : h(x) = x;8x  #
$ Memory cell and input gate are added once the error has stopped decreasing (see abuse problem: solution (1) in Section 4)  #
$ Results  #
$ Using RTRL and a short 4 time step delay (p = 4), 7 9 of all trials were successful  #
$ No trial was successful with p = 10  #
$ With long time lags, only the neural sequence chunker and LSTM achieved successful trials, while BPTT and RTRL failed  #
$ With p = 100, the 2-net sequence chunker solved the task in only 1 3 of all trials  #
$ LSTM, however, always learned to solve the task  #
$ Comparing successful trials only, LSTM learned much faster  #
$ See Table 2 for details  #
$ It should be mentioned, however, that a hierarchical chunker can also always quickly solve this task (Schmidhuber 1992c, 1993)  #
$ Task 2b: no local regularities  #
$ With the task above, the chunker sometimes learns to correctly predict the nal element, but only because of predictable local regularities in the input stream that allow for compressing the sequence  #
$ In an additional, more difficult task (involving many more different possible sequences), we remove compressibility by replacing the deterministic subsequence (a1;a2;:::;ap?1) by a random subsequence (of length p ? #
$ 1) over the alphabet a1;a2;:::;ap?1  #
$ We obtain 2 classes (two sets of sequences) f(y;ai1;ai2;:::;aip?1;y) j 1 i1;i2;:::;ip?1 p ?1g and f(x;ai1;ai2;:::;aip?1;x) j 1 i1;i2;:::;ip?1 p ?1g  #
$ Again, every next sequence element has to be predicted  #
$ The only totally predictable targets, however, are x and y, which occur at sequence ends  #
$ Training exemplars are chosen randomly from the 2 classes  #
$ Architectures and parameters are the same as in Experiment 2a  #
$ Table entries refer to means of 18 trials  #
$ With 100 time step delays, only CH and LSTM achieve successful trials  #
$ Even when we ignore the unsuccessful trials of the other approaches, LSTM learns much faster  #
$ sequences, the maximal absolute error of all output units is below 0:25 at sequence end  #
$ Results  #
$ As expected, the chunker failed to solve this task (so did BPTT and RTRL, of course)  #
$ LSTM, however, was always successful  #
$ On average (mean of 18 trials), success for p = 100 was achieved after 5,680 sequence presentations  #
$ This demonstrates that LSTM does not require sequence regularities to work well  #
$ Task 2c: very long time lags | no local regularities  #
$ This is the most difficult task in this subsection  #
$ To our knowledge no other recurrent net algorithm can solve it  #
$ Now there are p+4 possible input symbols denoted a1;:::;ap?1;ap;ap+1 = e;ap+2 = b;ap+3 = x;ap+4 = y  a1;:::;ap are also called \distractor symbols"  #
$ Again, ai is locally represented by the p+4-dimensionalvector whose ith component is 1 (all other components are 0)  #
$ A net with p+4 input units and 2 output units sequentially observes input symbol sequences, one at a time  #
$ Training sequences arerandomly chosen from the union of two very similar subsets of sequences: f(b;y;ai1;ai2;:::;aiq+k;e;y) j 1 i1;i2;:::;iq+k qg and f(b;x;ai1;ai2;:::;aiq+k;e;x) j 1 i1;i2;:::;iq+k qg  #
$ To produce a training sequence, we (1) randomly generate a sequence pre x of length q + 2, (2) randomly generate a sequence su x of additional elements (6= b;e;x;y) with probability 9 10 or, alternatively, an e with probability 1 10  #
$ In the latter case, we (3) conclude the sequence with x or y, depending on the second element  #
$ For a given k, this leads to a uniform distribution on the possible sequences with length q + k + 4  #
$ The minimal sequence length is q + 4; the expected length is 4 + 1 X k=0 1 10( 9 10)k(q + k) = q + 14: The expected number of occurrences of element ai;1 i p, in a sequence is q+10 p q p  The goal is to predict the last symbol, which always occurs after the \trigger symbol" e  Error signals are generated only at sequence ends  #
$ To predict the nal element, the net has to learn to store a representation of the second element for at least q +1 time steps (until it sees the trigger symbol e)  #
$ Success is de ned as \prediction error (for nal sequence element) of both output units always below 0:2, for 10,000 successive, randomly chosen input sequences"  #
$ Architecture/Learning  #
$ The net has p + 4 input units and 2 output units  #
$ Weights are initialized in -0 2,0 2]  #
$ To avoid too much learning time variance due to different weight initializations, the hidden layer gets two memory cells (two cell blocks of size 1 | although one would be su cient)  #
$ There are no other hidden units  #
$ The output layer receives connections only from memory cells  #
$ Memory cells and gate units receive connections from input units, memory cells and gate units (i e , the hidden layer is fully connected)  #
$ No bias weights are used  #
$ h and g are logistic sigmoids with output ranges ?1;1] and ?2;2], respectively  #
$ The learning rate is 0 01  #
$ p is the number of available distractor symbols (p + 4 is the number of input units)  #
$ q p is the expected number of occurrences of a given distractor symbol in a sequence  #
$ The rightmost column lists the number of training sequences required by LSTM (BPTT, RTRL and the other competitors have no chance of solving this task)  #
$ If we let the number of distractor symbols (and weights) increase in proportion to the time lag, learning time increases very slowly  #
$ The lower block illustrates the expected slow-down due to increased frequency of distractor symbols  #
$ Note that the minimal time lag is q +1 | the net never sees short training sequences facilitating the classi cation of long test sequences  #
$ Results  #
$ 20 trials were made for all tested pairs (p;q)  #
$ Table 3 lists the mean of the number of training sequences required by LSTM to achieve success (BPTT and RTRL have no chance of solving non-trivial tasks with minimal time lags of 1000 steps)  #
$ Scaling  #
$ Table 3 shows that if we let the number of input symbols (and weights) increase in proportion to the time lag, learning time increases very slowly  #
$ This is a another remarkable property of LSTM not shared by any other method we are aware of  #
$ Indeed, RTRL and BPTT are far from scaling reasonably | instead, they appear to scale exponentially, and appear quite useless when the time lags exceed as few as 10 steps  #
$ Distractor in uence  #
$ In Table 3, the column headed by q p gives the expected frequency of distractor symbols  #
$ Increasing this frequency decreases learning speed, an e ect due to weight oscillations caused by frequently observed input symbols  #
$ 5 3 EXPERIMENT 3: NOISE AND SIGNAL ON SAME CHANNEL This experiment serves to illustrate that LSTM does not encounter fundamental problems if noise and signal are mixed on the same input line  #
$ We initially focus on Bengio et al  #
$ 's simple 1994 \2-sequence problem"; in Experiment 3c we will then pose a more challenging 2-sequence problem  #
$ Task 3a (\2-sequence problem")  #
$ The task is to observe and then classify input sequences  #
$ There are two classes, each occurring with probability 0 5  #
$ There is only one input line  #
$ Only the rst N real-valued sequence elements convey relevant information about the class  #
$ Sequence elements at positions t > N are generated by a Gaussian with mean zero and variance 0 2  #
$ Case N = 1: the rst sequence element is 1 0 for class 1, and -1 0 for class 2  #
$ Case N = 3: the rst three elements are 1 0 for class 1 and -1 0 for class 2  #
$ The target at the sequence end is 1 0 for class 1 and 0 0 for class 2  #
$ Correct classi cation is de ned as \absolute output error at sequence end below 0 2"  #
$ Given a constant T, the sequence length is randomly selected between T and T + T/10 (a difference to Bengio et al  #
$ 's problem is that they also permit shorter sequences of length T/2)  #
$ Guessing  #
$ Bengio et al  #
$ (1994) and Bengio and Frasconi (1994) tested 7 different methods on the 2-sequence problem  #
$ 's 2-sequence problem  #
$ T is minimal sequence length  #
$ N is the number of information-conveying elements at sequence begin  #
$ The column headed by ST1 (ST2) gives the number of sequence presentations required to achieve stopping criterion ST1 (ST2)  #
$ The rightmost column lists the fraction of misclassified post-training sequences (with absolute error > 0 2) from a test set consisting of 2560 sequences (tested after ST2 was achieved)  #
$ All values are means of 10 trials  #
$ We discovered, however, that this problem is so simple that random weight guessing solves it faster than LSTM and any other method for which there are published results  #
$ forms them all, because the problem is so simple5  #
$ See Schmidhuber and Hochreiter (1996) and Hochreiter and Schmidhuber (1996, 1997) for additional results in this vein  #
$ LSTM architecture  #
$ We use a 3-layer net with 1 input unit, 1 output unit, and 3 cell blocks of size 1  #
$ The output layer receives connections only from memory cells  #
$ Memory cells and gate units receive inputs from input units, memory cells and gate units, and have bias weights  #
$ Gate units and output unit are logistic sigmoid in 0;1], h in ?1;1], and g in ?2;2]  #
$ Training/Testing  #
$ All weights (except the bias weights to gate units) are randomly initialized in the range ?0:1;0:1]  #
$ The rst input gate bias is initialized with ?1:0, the second with ?3:0, and the third with ?5:0  #
$ The rst output gate bias is initialized with ?2:0, the second with ?4:0 and the third with ?6:0  #
$ The precise initialization values hardly matter though, as con rmed by additional experiments  #
$ The learning rate is 1 0  #
$ All activations are reset to zero at the beginning of a new sequence  #
$ We stop training (and judge the task as being solved) according to the following criteria: ST1: none of 256 sequences from a randomly chosen test set is misclassified  #
$ ST2: ST1 is satis ed, and mean absolute test set error is below 0 01  #
$ In case of ST2, an additional test set consisting of 2560 randomly chosen sequences is used to determine the fraction of misclassified sequences  #
$ Results  #
$ See Table 4  #
$ The results are means of 10 trials with different weight initializations in the range ?0:1;0:1]  #
$ LSTM is able to solve this problem, though by far not as fast as random weight guessing (see paragraph \Guessing" above)  #
$ Clearly, this trivial problem does not provide a very good testbed to compare performance of various non-trivial algorithms  #
$ Still, it demonstrates that LSTM does not encounter fundamental problems when faced with signal and noise on the same channel  #
$ Task 3b  #
$ Architecture, parameters, etc  #
$ like in Task 3a, but now with Gaussian noise (mean 0 and variance 0 2) added to the information-conveying elements (t <= N)  #
$ We stop training (and judge the task as being solved) according to the following, slightly rede ned criteria: ST1: less than 6 out of 256 sequences from a randomly chosen test set are misclassified  #
$ ST2: ST1 is satis ed, and mean absolute test set error is below 0 04  #
$ In case of ST2, an additional test set consisting of 2560 randomly chosen sequences is used to determine the fraction of misclassified sequences  #
$ Results  #
$ See Table 5  #
$ The results represent means of 10 trials with different weight initializations  #
$ LSTM easily solves the problem  #
$ Task 3c  #
$ Architecture, parameters, etc  #
$ like in Task 3a, but with a few essential changes that make the task non-trivial: the targets are 0 2 and 0 8 for class 1 and class 2, respectively, and there is Gaussian noise on the targets (mean 0 and variance 0 1; st dev  #
$ 0 32)  #
$ To minimize mean squared error, the system has to learn the conditional expectations of the targets given the inputs  #
$ Misclassi cation is de ned as \absolute difference between output and noise-free target (0 2 for 5It should be mentioned, however, that different input representations and different types of noise may lead to worse guessing performance (Yoshua Bengio, personal communication, 1996)  #
$ 16 T N stop: ST1 stop: ST2 # weights ST2: fraction misclassified 100 3 41,740 43,250 102 0 00828 100 1 74,950 78,430 102 0 01500 1000 1 481,060 485,080 102 0 01207 Table 5: Task 3b: modi ed 2-sequence problem  #
$ Same as in Table 4, but now the information-conveying elements are also perturbed by noise  #
$ T N stop # weights fraction misclassified av  #
$ difference to mean 100 3 269,650 102 0 00558 0 014 100 1 565,640 102 0 00441 0 012 Table 6: Task 3c: modi ed, more challenging 2-sequence problem  #
$ Same as in Table 4, but with noisy real-valued targets  #
$ The system has to learn the conditional expectations of the targets given the inputs  #
$ The rightmost column provides the average difference between network output and expected target  #
$ Unlike 3a and 3b, this task cannot be solved quickly by random weight guessing  #
$ class 1 and 0 8 for class 2) > 0 1  " #
$ The network output is considered acceptable if the mean absolute difference between noise-free target and output is below 0 015  #
$ Since this requires high weight precision, Task 3c (unlike 3a and 3b) cannot be solved quickly by random guessing  #
$ Training/Testing  #
$ The learning rate is 0:1  #
$ We stop training according to the following criterion: none of 256 sequences from a randomly chosen test set is misclassified, and mean absolute difference between noise free target and output is below 0 015  #
$ An additional test set consisting of 2560 randomly chosen sequences is used to determine the fraction of misclassified sequences  #
$ Results  #
$ See Table 6  #
$ The results represent means of 10 trials with different weight initializations  #
$ Despite the noisy targets, LSTM still can solve the problem by learning the expected target values  #
$ 5 4 EXPERIMENT 4: ADDING PROBLEM The difficult task in this section is of a type that has never been solved by other recurrent net algorithms  #
$ It shows that LSTM can solve long time lag problems involving distributed, continuous-valued representations  #
$ Task  #
$ Each element of each input sequence is a pair of components  #
$ The rst component is a real value randomly chosen from the interval ?1;1]; the second is either 1 0, 0 0, or -1 0, and is used as a marker: at the end of each sequence, the task is to output the sum of the rst components of those pairs that are marked by second components equal to 1 0  #
$ Sequences have random lengths between the minimal sequence length T and T + T 10  #
$ In a given sequence exactly two pairs are marked as follows: we rst randomly select and mark one of the rst ten pairs (whose rst component we call X1)  #
$ Then we randomly select and mark one of the rst T 2 ? #
$ 1 still unmarked pairs (whose rst component we call X2)  #
$ The second components of all remaining pairs are zero except for the rst and nal pair, whose second components are -1  #
$ (In the rare case where the rst pair of the sequence gets marked, we set X1 to zero ) #
$ An error signal is generated only at the sequence end: the target is 0:5+ X1+X2 4:0 (the sum X1 +X2 scaled to the interval 0;1])  #
$ A sequence is processed correctly if the absolute error at the sequence end is below 0 04  #
$ Architecture  #
$ We use a 3-layer net with 2 input units, 1 output unit, and 2 cell blocks of size 2  #
$ The output layer receives connections only from memory cells  #
$ Memory cells and gate units receive inputs from memory cells and gate units (i e , the hidden layer is fully connected | less connectivity may work as well)  #
$ The input layer has forward connections to all units in the hidden 17 T minimal lag # weights # wrong predictions Success after 100 50 93 1 out of 2560 74,000 500 250 93 0 out of 2560 209,000 1000 500 93 1 out of 2560 853,000 Table 7: EXPERIMENT 4: Results for the Adding Problem  #
$ T is the minimal sequence length, T=2 the minimal time lag  #
$ \# wrong predictions" is the number of incorrectly processed sequences (error > 0 04) from a test set containing 2560 sequences  #
$ The rightmost column gives the number of training sequences required to achieve the stopping criterion  #
$ All values are means of 10 trials  #
$ For T = 1000 the number of required training examples varies between 370,000 and 2,020,000, exceeding 700,000 in only 3 cases  #
$ layer  #
$ All non-input units have bias weights  #
$ These architecture parameters make it easy to store at least 2 input signals (a cell block size of 1 works well, too)  #
$ All activation functions are logistic with output range 0;1], except for h, whose range is ?1;1], and g, whose range is ?2;2]  #
$ State drift versus initial bias  #
$ Note that the task requires storing the precise values of real numbers for long durations | the system must learn to protect memory cell contents against even minor internal state drift (see Section 4)  #
$ To study the significance of the drift problem, we make the task even more difficult by biasing all non-input units, thus artificially inducing internal state drift  #
$ All weights (including the bias weights) are randomly initialized in the range ?0:1;0:1]  #
$ Following Section 4's remedy for state drifts, the rst input gate bias is initialized with ?3:0, the second with ?6:0 (though the precise values hardly matter, as con rmed by additional experiments)  #
$ Training/Testing  #
$ The learning rate is 0 5  #
$ Training is stopped once the average training error is below 0 01, and the 2000 most recent sequences were processed correctly  #
$ Results  #
$ With a test set consisting of 2560 randomly chosen sequences, the average test set error was always below 0 01, and there were never more than 3 incorrectly processed sequences  #
$ Table 7 shows details  #
$ The experiment demonstrates: (1) LSTM is able to work well with distributed representations  #
$ (2) LSTM is able to learn to perform calculations involving continuous values  #
$ (3) Since the system manages to store continuous values without deterioration for minimal delays of T 2 time steps, there is no significant, harmful internal state drift  #
$ 5 5 EXPERIMENT 5: MULTIPLICATION PROBLEM One may argue that LSTM is a bit biased towards tasks such as the Adding Problem from the previous subsection  #
$ Solutions to the Adding Problem may exploit the CEC's built-in integration capabilities  #
$ Although this CEC property may be viewed as a feature rather than a disadvantage (integration seems to be a natural subtask of many tasks occurring in the real world), the question arises whether LSTM can also solve tasks with inherently non-integrative solutions  #
$ To test this, we change the problem by requiring the nal target to equal the product (instead of the sum) of earlier marked inputs  #
$ Task  #
$ Like the task in Section 5 4, except that the rst component of each pair is a real value randomly chosen from the interval 0;1]  #
$ In the rare case where the rst pair of the input sequence gets marked, we set X1 to 1 0  #
$ The target at sequence end is the product X1 X2  #
$ Architecture  #
$ Like in Section 5 4  #
$ All weights (including the bias weights) are randomly initialized in the range ?0:1;0:1]  #
$ Training/Testing  #
$ The learning rate is 0 1  #
$ We test performance twice: as soon as less than nseq of the 2000 most recent training sequences lead to absolute errors exceeding 0 04, where nseq = 140, and nseq = 13  #
$ Why these values? #
$ nseq = 140 is su cient to learn storage of the relevant inputs  #
$ It is not enough though to ne-tune the precise nal outputs  #
$ nseq = 13, however, 18 T minimal lag # weights nseq # wrong predictions MSE Success after 100 50 93 140 139 out of 2560 0 0223 482,000 100 50 93 13 14 out of 2560 0 0139 1,273,000 Table 8: EXPERIMENT 5: Results for the Multiplication Problem  #
$ T is the minimal sequence length, T=2 the minimal time lag  #
$ We test on a test set containing 2560 sequences as soon as less than nseq of the 2000 most recent training sequences lead to error > 0 04  #
$ \# wrong predictions" is the number of test sequences with error > 0 04  #
$ MSE is the mean squared error on the test set  #
$ The rightmost column lists numbers of training sequences required to achieve the stopping criterion  #
$ All values are means of 10 trials  #
$ leads to quite satisfactory results  #
$ Results  #
$ For nseq = 140 (nseq = 13) with a test set consisting of 2560 randomly chosen sequences, the average test set error was always below 0 026 (0 013), and there were never more than 170 (15) incorrectly processed sequences  #
$ Table 8 shows details  #
$ (A net with additional standard hidden units or with a hidden layer above the memory cells may learn the ne-tuning part more quickly ) #
$ The experiment demonstrates: LSTM can solve tasks involving both continuous-valued representations and non-integrative information processing  #
$ 5 6 EXPERIMENT 6: TEMPORAL ORDER In this subsection, LSTM solves other difficult (but artificial) tasks that have never been solved by previous recurrent net algorithms  #
$ The experiment shows that LSTM is able to extract information conveyed by the temporal order of widely separated inputs  #
$ Task 6a: two relevant, widely separated symbols  #
$ The goal is to classify sequences  #
$ Elements and targets are represented locally (input vectors with only one non-zero bit)  #
$ The sequence starts with an E, ends with a B (the \triggersymbol") and otherwise consists of randomly chosen symbols from the set fa;b;c;dgexcept for two elements at positions t1 and t2 that are either X or Y   #
$ The sequence length is randomly chosen between 100 and 110, t1 is randomly chosen between 10 and 20, and t2 is randomly chosen between 50 and 60  #
$ There are 4 sequence classes Q;R;S;U which depend on the temporal order of X and Y   #
$ The rules are: X;X ! #
$ Q; X;Y ! #
$ R; Y;X ! #
$ S; Y;Y ! #
$ Task 6b: three relevant, widely separated symbols  #
$ Again, the goal is to classify sequences  #
$ Elements/targets are represented locally  #
$ The sequence starts with an E, ends with a B (the \trigger symbol"), and otherwise consists of randomly chosen symbols from the set fa;b;c;dg except for three elements at positions t1;t2 and t3 that are either X or Y   #
$ The sequence length is randomly chosen between 100 and 110, t1 is randomly chosen between 10 and 20, t2 is randomly chosen between 33 and 43, and t3 is randomly chosen between 66 and 76  #
$ There are 8 sequence classes Q;R;S;U;V;A;B;C which depend on the temporal order of the Xs and Ys  #
$ The rules are: X;X;X ! #
$ Q; X;X;Y ! #
$ R; X;Y;X ! #
$ S; X;Y;Y ! #
$ U; Y;X;X ! #
$ V ; Y;X;Y ! #
$ A; Y;Y;X ! #
$ B; Y;Y;Y ! #
$ C  There are as many output units as there are classes  #
$ Each class is locally represented by a binary target vector with one non-zero component  #
$ With both tasks, error signals occur only at the end of a sequence  #
$ The sequence is classi ed correctly if the nal absolute error of all output units is below 0 3  #
$ Architecture  #
$ We use a 3-layer net with 8 input units, 2 (3) cell blocks of size 2 and 4 (8) output units for Task 6a (6b)  #
$ Again all non-input units have bias weights, and the output layer receives connections from memory cells only  #
$ Memory cells and gate units receive inputs from input units, memory cells and gate units (i e , the hidden layer is fully connected | less connectivity may work as well)  #
$ The architecture parameters for Task 6a (6b) make it easy to 19 store at least 2 (3) input signals  #
$ All activation functions are logistic with output range 0;1], except for h, whose range is ?1;1], and g, whose range is ?2;2]  #
$ Training/Testing  #
$ The learning rate is 0 5 (0 1) for Experiment 6a (6b)  #
$ Training is stopped once the average training error falls below 0 1 and the 2000 most recent sequences were classi ed correctly  #
$ All weights are initialized in the range ?0:1;0:1]  #
$ The rst input gate bias is initialized with ?2:0, the second with ?4:0, and (for Experiment 6b) the third with ?6:0 (again, we con rmed by additional experiments that the precise values hardly matter)  #
$ Results  #
$ With a test set consisting of 2560 randomly chosen sequences, the average test set error was always below 0 1, and there were never more than 3 incorrectly classi ed sequences  #
$ Table 9 shows details  #
$ The experiment shows that LSTM is able to extract information conveyed by the temporal order of widely separated inputs  #
$ In Task 6a, for instance, the delays between rst and second relevant input and between second relevant input and sequence end are at least 30 time steps  #
$ task # weights # wrong predictions Success after Task 6a 156 1 out of 2560 31,390 Task 6b 308 2 out of 2560 571,100 Table 9: EXPERIMENT 6: Results for the Temporal Order Problem  #
$ \# wrong predictions" is the number of incorrectly classi ed sequences (error > 0 3 for at least one output unit) from a test set containing 2560 sequences  #
$ The rightmost column gives the number of training sequences required to achieve the stopping criterion  #
$ The results for Task 6a are means of 20 trials; those for Task 6b of 10 trials  #
$ Typical solutions  #
$ In Experiment 6a, how does LSTM distinguish between temporal orders (X;Y ) and (Y;X)? #
$ One of many possible solutions is to store the rst X or Y in cell block 1, and the second X=Y in cell block 2  #
$ Before the rst X=Y occurs, block 1 can see that it is still empty by means of its recurrent connections  #
$ After the rst X=Y, block 1 can close its input gate  #
$ Once block 1 is lled and closed, this fact will become visible to block 2 (recall that all gate units and all memory cells receive connections from all non-output units)  #
$ Typical solutions, however, require only one memory cell block  #
$ The block stores the rst X or Y; once the second X=Y occurs, it changes its state depending on the rst stored symbol  #
$ Solution type 1 exploits the connection between memory cell output and input gate unit | the following events cause different input gate activations: \X occurs in conjunction with a lled block"; \X occurs in conjunction with an empty block"  #
$ Solution type 2 is based on a strong positive connection between memory cell output and memory cell input  #
$ The previous occurrence of X (Y ) is represented by a positive (negative) internal state  #
$ Once the input gate opens for the second time, so does the output gate, and the memory cell output is fed back to its own input  #
$ This causes (X;Y) to be represented by a positive internal state, because X contributes to the new internal state twice (via current internal state and cell output feedback)  #
$ Similarly, (Y;X) gets represented by a negative internal state  #
$ 5 7 SUMMARY OF EXPERIMENTAL CONDITIONS The two tables in this subsection provide an overview of the most important LSTM parameters and architectural details for Experiments 1{6  #
$ The conditions of the simple experiments 2a and 2b differ slightly from those of the other, more systematic experiments, due to historical reasons  #
$ 1st column: task number  #
$ 2nd column: minimal sequence length p  3rd column: minimal number of steps between most recent relevant input information and teacher signal  #
$ 4th column: number of cell blocks b  #
$ 5th column: block size s  6th column: number of input units in  #
$ 7th column: number of output units out  #
$ 8th column: number of weights w  9th column: c describes connectivity: \F" means \output layer receives connections from memory cells; memory cells and gate units receive connections from input units, memory cells and gate units"; \B" means \each layer receives connections from all layers below"  #
$ 10th column: initial output gate bias ogb, where \r" stands for \randomly chosen from the interval ?0:1;0:1]" and \no og" means \no output gate used"  #
$ 11th column: initial input gate bias igb (see 10th column)  #
$ 12th column: which units have bias weights? #
$ \b1" stands for \all hidden units", \ga" for \only gate units", and \all" for \all non-input units"  #
$ 13th column: the function h, where \id" is identity function, \h1" is logistic sigmoid in ?2;2]  #
$ 14th column: the logistic function g, where \g1" is sigmoid in 0;1], \g2" in ?1;1]  #
$ 15th column: learning rate   #
$ 1 2 3 4 5 6 Task select interval test set size stopping criterion success 1 t1 ?0:2;0:2] 256 training & test correctly pred  #
$ see text 2a t1 ?0:2;0:2] no test set after 5 million exemplars ABS(0 25) 2b t2 ?0:2;0:2] 10000 after 5 million exemplars ABS(0 25) 2c t2 ?0:2;0:2] 10000 after 5 million exemplars ABS(0 2) 3a t3 ?0:1;0:1] 2560 ST1 and ST2 (see text) ABS(0 2) 3b t3 ?0:1;0:1] 2560 ST1 and ST2 (see text) ABS(0 2) 3c t3 ?0:1;0:1] 2560 ST1 and ST2 (see text) see text 4 t3 ?0:1;0:1] 2560 ST3(0 01) ABS(0 04) 5 t3 ?0:1;0:1] 2560 see text ABS(0 04) 6a t3 ?0:1;0:1] 2560 ST3(0 1) ABS(0 3) 6b t3 ?0:1;0:1] 2560 ST3(0 1) ABS(0 3) 21 Table 11: Summary of experimental conditions for LSTM, Part II  #
$ 1st column: task number  #
$ 2nd column: training exemplar selection, where \t1" stands for \randomly chosen from training set", \t2" for \randomly chosen from 2 classes", and \t3" for \randomly generated on-line"  #
$ 3rd column: weight initialization interval  #
$ 4th column: test set size  #
$ 5th column: stopping criterion for training, where \ST3( )" stands for \average training error below and the 2000 most recent sequences were processed correctly"  #
$ 6th column: success (correct classi cation) criterion, where \ABS( )" stands for \absolute error of all output units at sequence end is below "  #
$ 6 DISCUSSION Limitations of LSTM  #
$ The particularly e cient truncated backprop version of the LSTM algorithm will not easily solve problems similar to \strongly delayed XOR problems", where the goal is to compute the XOR of two widely separated inputs that previously occurred somewhere in a noisy sequence  #
$ The reason is that storing only one of the inputs will not help to reduce the expected error | the task is non-decomposable in the sense that it is impossible to incrementally reduce the error by rst solving an easier subgoal  #
$ In theory, this limitation can be circumvented by using the full gradient (perhaps with additional conventional hidden units receiving input from the memory cells)  #
$ But we do not recommend computing the full gradient for the following reasons: (1) It increases computational complexity  #
$ (2) Constant errorflow through CECs can be shown only for truncated LSTM  #
$ (3) We actually did conduct a few experiments with non-truncated LSTM  #
$ There was no significant difference to truncated LSTM, exactly because outside the CECs errorflow tends to vanish quickly  #
$ For the same reason full BPTT does not outperform truncated BPTT  #
$ Each memory cell block needs two additional units (input and output gate)  #
$ In comparison to standard recurrent nets, however, this does not increase the number of weights by more than a factor of 9: each conventional hidden unit is replaced by at most 3 units in the LSTM architecture, increasing the number of weights by a factor of 32 in the fully connected case  #
$ Note, however, that our experiments use quite comparable weight numbers for the architectures of LSTM and competing approaches  #
$ Generally speaking, due to its constant errorflow through CECs within memory cells, LSTM runs into problems similar to those of feedforward nets seeing the entire input string at once  #
$ For instance, there are tasks that can be quickly solved by random weight guessing but not by the truncated LSTM algorithm with small weight initializations, such as the 500-step parity problem (see introduction to Section 5)  #
$ Here, LSTM's problems are similar to the ones of a feedforward net with 500 inputs, trying to solve 500-bit parity  #
$ Indeed LSTM typically behaves much like a feedforward net trained by backprop that sees the entire input  #
$ But that's also precisely why it so clearly outperforms previous approaches on many non-trivial tasks with significant search spaces  #
$ LSTM does not have any problems with the notion of \recency" that go beyond those of other approaches  #
$ All gradient-based approaches, however, su er from practical inability to precisely count discrete time steps  #
$ If it makes a difference whether a certain signal occurred 99 or 100 steps ago, then an additional counting mechanism seems necessary  #
$ Easier tasks, however, such as one that only requires to make a difference between, say, 3 and 11 steps, do not pose any problems to LSTM  #
$ For instance, by generating an appropriate negative connection between memory cell output and input, LSTM can give more weight to recent inputs and learn decays where necessary  #
$ 22 Advantages of LSTM  #
$ The constant error backpropagation within memory cells results in LSTM's ability to bridge very long time lags in case of problems similar to those discussed above  #
$ For long time lag problems such as those discussed in this paper, LSTM can handle noise, distributed representations, and continuous values  #
$ In contrast to nite state automata or hidden Markov models LSTM does not require an a priori choice of a nite number of states  #
$ In principle it can deal with unlimited state numbers  #
$ For problems discussed in this paper LSTM generalizes well | even if the positions of widely separated, relevant inputs in the input sequence do not matter  #
$ Unlike previous approaches, ours quickly learns to distinguish between two or more widely separated occurrences of a particular element in an input sequence, without depending on appropriate short time lag training exemplars  #
$ There appears to be no need for parameter ne tuning  #
$ LSTM works well over a broad range of parameters such as learning rate, input gate bias and output gate bias  #
$ For instance, to some readers the learning rates used in our experiments may seem large  #
$ However, a large learning rate pushes the output gates towards zero, thus automatically countermanding its own negative e ects  #
$ The LSTM algorithm's update complexity per weight and time step is essentially that of BPTT, namely O(1)  #
$ This is excellent in comparison to other approaches such as RTRL  #
$ Unlike full BPTT, however, LSTM is local in both space and time  #
$ 7 CONCLUSION Each memory cell's internal architecture guarantees constant errorflow within its constant error carrousel CEC, provided that truncated backprop cuts o errorflow trying to leak out of memory cells  #
$ This represents the basis for bridging very long time lags  #
$ Two gate units learn to open and close access to errorflow within each memory cell's CEC  #
$ The multiplicative input gate a ords protection of the CEC from perturbation by irrelevant inputs  #
$ Likewise, the multiplicative output gate protects other units from perturbation by currently irrelevant memory contents  #
$ Future work  #
$ To nd out about LSTM's practical limitations we intend to apply it to real world data  #
$ Application areas will include (1) time series prediction, (2) music composition, and (3) speech processing  #
$ It will also be interesting to augment sequence chunkers (Schmidhuber 1992b, 1993) by LSTM to combine the advantages of both  #
$ 8 ACKNOWLEDGMENTS Thanks to Mike Mozer, Wilfried Brauer, Nic Schraudolph, and several anonymous referees for valuable comments and suggestions that helped to improve a previous version of this paper (Hochreiter and Schmidhuber 1995)  #
$ This work was supported by DFG grant SCHM 942/3-1 from \Deutsche Forschungsgemeinschaft"  #
$ APPENDIX A 1 ALGORITHM DETAILS In what follows, the index k ranges over output units, i ranges over hidden units, cj stands for the j-th memory cell block, cv j denotes the v-th unit of memory cell block cj, u;l;m stand for arbitrary units, t ranges over all time steps of a given input sequence  #
$ 23 The gate unit logistic sigmoid (with range 0;1]) used in the experiments is f(x) = 1 1 + exp(?x)   #
$ (3) The function h (with range ?1;1]) used in the experiments is h(x) = 2 1 + exp(?x) ?1   #
$ (4) The function g (with range ?2;2]) used in the experiments is g(x) = 4 1 + exp(?x) ?2   #
$ (5) Forward pass  #
$ The net input and the activation of hidden unit i are neti(t) = X u wiuyu(t ?1) (6) yi(t) = fi(neti(t))   #
$ The net input and the activation of inj are netinj(t) = X u winjuyu(t ?1) (7) yinj(t) = finj(netinj(t))   #
$ The net input and the activation of outj are netoutj(t) = X u woutjuyu(t?1) (8) youtj(t) = foutj(netoutj(t))   #
$ The net input netcv j , the internal state scv j , and the output activation ycv j of the v-th memory cell of memory cell block cj are: netcv j (t) = X u wcv juyu(t ?1) (9) scv j (t) = scv j (t ?1) + yinj(t)g netcv j (t) ycv j (t) = youtj(t)h(scv j (t))   #
$ The net input and the activation of output unit k are netk(t) = X u: u not a gate wkuyu(t ?1) yk(t) = fk(netk(t))   #
$ The backward pass to be described later is based on the following truncated backprop formulae  #
$ Approximate derivatives for truncated backprop  #
$ The truncated version (see Section 4) only approximates the partial derivatives, which is reflected by the \ tr" signs in the notation below  #
$ It truncates errorflow once it leaves memory cells or gate units  #
$ Truncation ensures that there are no loops across which an error that left some memory cell through its input or input gate can reenter the cell through its output or output gate  #
$ This in turn ensures constant error flow through the memory cell's CEC  #
$ tr (10) f0 k(netk(t)) 0 @X j Sj X v=1 cv jlwkcv j @ycv j (t ?1) @wlm + X j ? #
$ injl + outjl Sj X v=1 wkcv j @ycv j (t ?1) @wlm + X i: i hidden unit wki @yi(t ?1) @wlm + klym(t ?1) ! #
$ = f0 k(netk(t)) 8 > > > > > < > > > > > : ym(t?1) l = k wkcv j @ycv j (t?1) @wlm l = cv j PSj v=1 wkcv j @ycv j (t?1) @wlm l = inj OR l = outj P i: i hidden unit wki @yi(t?1) @wlm l otherwise , where is the Kronecker delta ( ab = 1 if a = b and 0 otherwise), and Sj is the size of memory cell block cj  #
$ The truncated derivatives of a hidden unit i that is not part of a memory cell are: @yi(t) @wlm = f0 i(neti(t))@neti(t) @wlm tr lif0 i(neti(t))ym(t ?1)   #
$ (11) (Note: here it would be possible to use the full gradient without a ecting constant errorflow through internal states of memory cells ) #
$ 25 Cell block cj's truncated derivatives are: @yinj(t) @wlm = f0 inj(netinj(t)) @netinj(t) @wlm tr injlf0 inj(netinj(t))ym(t ?1)   #
$ (12) @youtj(t) @wlm = f0 outj(netoutj(t)) @netoutj(t) @wlm tr outjlf0 outj(netoutj(t))ym(t ?1)   #
$ @ycv j (t) @wlm = @youtj(t) @wlm h(scv j (t)) + h0(scv j (t)) @scv j (t) @wlm youtj(t) tr (15) outjl @youtj(t) @wlm h(scv j (t)) + injl + cv jl h0(scv j (t)) @scv j (t) @wlm youtj(t)   #
$ To e ciently update the system at time t, the only (truncated) derivatives that need to be stored at time t?1 are @scv j (t?1) @wlm , where l = cv j or l = inj  #
$ Backward pass  #
$ We will describe the backward pass only for the particularly e cient \truncated gradient version" of the LSTM algorithm  #
$ For simplicity we will use equal signs even where approximations are made according to the truncated backprop equations above  #
$ The squared error at time t is given by E(t) = X k: k output unit ?tk(t) ?yk(t) 2 , (16) where tk(t) is output unit k's target at time t  Time t's contribution to wlm's gradient-based update with learning rate is wlm(t) = ? #
$ @E(t) @wlm   #
$ (17) We de ne some unit l's error at time step t by el(t) := ? #
$ @E(t) @netl(t)   #
$ (18) Using (almost) standard backprop, we rst compute updates for weights to output units (l = k), weights to hidden units (l = i) and weights to output gates (l = outj)  #
$ We obtain (compare formulae (10), (11), (13)): l = k (output) : ek(t) = f0 k(netk(t)) ?tk(t) ?yk(t) , (19) l = i (hidden) : ei(t) = f0 i(neti(t)) X k: k output unit wkiek(t) , (20) 26 l = outj (output gates) : (21) eoutj(t) = f0 outj(netoutj(t)) 0 @ Sj X v=1 h(scv j (t)) X k: k output unit wkcv j ek(t) 1 A   #
$ For all possible l time t's contribution to wlm's update is wlm(t) = el(t) ym(t?1)   #
$ (22) The remaining updates for weights to input gates (l = inj) and to cell units (l = cv j) are less conventional  #
$ We de ne some internal state scv j 's error: escv j := ? #
$ @E(t) @scv j (t) = (23) foutj(netoutj(t)) h0(scv j (t)) X k: k output unit wkcv j ek(t)   #
$ We obtain for l = inj or l = cv j; v = 1;:::;Sj ? #
$ @E(t) @wlm = Sj X v=1 escv j (t) @scv j (t) @wlm   #
$ (24) The derivatives of the internal states with respect to weights and the corresponding weight updates are as follows (compare expression (14)): l = inj (input gates) : (25) @scv j (t) @winjm = @scv j (t ?1) @winjm + g(netcv j (t)) f0 inj(netinj(t)) ym(t?1) ; therefore time t's contribution to winjm's update is (compare expression (10)): winjm(t) = Sj X v=1 escv j (t) @scv j (t) @winjm   #
$ (26) Similarly we get (compare expression (14)): l = cv j (memory cells) : (27) @scv j (t) @wcv jm = @scv j (t ?1) @wcv jm + g0(netcv j (t)) finj(netinj(t)) ym(t ?1) ; therefore time t's contribution to wcv jm's update is (compare expression (10)): wcv jm(t) = escv j (t) @scv j (t) @wcv jm   #
$ (28) All we need to implement for the backward pass are equations (19), (20), (21), (22), (23), (25), (26), (27), (28)  #
$ Each weight's total update is the sum of the contributions of all time steps  #
$ Computational complexity  #
$ LSTM's update complexity per time step is O(KH + KCS + HI + CSI) = O(W); (29) where K is the number of output units, C is the number of memory cell blocks, S > 0 is the size of the memory cell blocks, H is the number of hidden units, I is the (maximal) number of units forward-connected to memory cells, gate units and hidden units, and W = KH + KCS + CSI + 2CI + HI = O(KH + KCS + CSI + HI) 27 is the number of weights  #
$ Expression (29) is obtained by considering all computations of the backward pass: equation (19) needs K steps; (20) needs KH steps; (21) needs KSC steps; (22) needs K(H + C) steps for output units, HI steps for hidden units, CI steps for output gates; (23) needs KCS steps; (25) needs CSI steps; (26) needs CSI steps; (27) needs CSI steps; (28) needs CSI steps  #
$ The total is K + 2KH + KC + 2KSC + HI + CI + 4CSI steps, or O(KH +KSC +HI +CSI) steps  #
$ We conclude: LSTM algorithm's update complexity per time step is just like BPTT's for a fully recurrent net  #
$ At a given time step, only the 2CSI most recent @scv j @wlm values from equations (25) and (27) need to be stored  #
$ Hence LSTM's storage complexity also is O(W) | it does not depend on the input sequence length  #
$ A 2 ERROR FLOW We compute how much an error signal is scaled while flowing back through a memory cell for q time steps  #
$ As a by-product, this analysis recon rms that the errorflow within a memory cell's CEC is indeed constant, provided that truncated backprop cuts o errorflow trying to leave memory cells (see also Section 3 2)  #
$ The analysis also highlights a potential for undesirable long-term drifts of scj (see (2) below), as well as the beneficial, countermanding in uence of negatively biased input gates (see (3) below)  #
$ Using the truncated backprop learning rule, we obtain @scj(t ?k) @scj(t ?k ?1) = (30) 1 + @yinj(t ?k) @scj(t ?k ?1)g ?netcj(t?k) + yinj(t ?k)g0 ?netcj(t ?k) @netcj(t?k) @scj(t ?k ?1) = 1 + X u @yinj(t?k) @yu(t ?k ?1) @yu(t ?k ?1) @scj(t ?k ?1) g ?netcj(t ?k) + yinj(t ?k)g0 ?netcj(t ?k) X u @netcj(t ?k) @yu(t ?k ?1) @yu(t ?k ?1) @scj(t?k ?1) tr 1: The tr sign indicates equality due to the fact that truncated backprop replaces by zero the following derivatives: @yinj(t?k) @yu(t?k?1) 8u and @netcj(t?k) @yu(t?k?1) 8u  #
$ In what follows, an error #j(t) starts flowing back at cj's output  #
$ We rede ne #j(t) := X i wicj#i(t+ 1)   #
$ (31) Following the definitions/conventions of Section 3 1, we compute error flow for the truncated backprop learning rule  #
$ The error occurring at the output gate is #outj(t) tr @youtj(t) @netoutj(t) @ycj(t) @youtj(t)#j(t)   #
$ (32) The error occurring at the internal state is #scj (t) = @scj(t+ 1) @scj(t) #scj (t+ 1) + @ycj(t) @scj(t)#j(t)   #
$ (33) Since we use truncated backprop we have #j(t) = P i, i no gate and no memory cell wicj#i(t + 1); therefore we get @#j(t) @#scj (t + 1) = X i wicj @#i(t+ 1) @#scj (t+ 1) tr 0   #
$ (34) 28 The previous equations (33) and (34) imply constant errorflow through internal states of memory cells: @#scj (t) @#scj (t+ 1) = @scj(t + 1) @scj(t) tr 1   #
$ (35) The error occurring at the memory cell input is #cj(t) = @g(netcj(t)) @netcj(t) @scj(t) @g(netcj(t))#scj (t)   #
$ (36) The error occurring at the input gate is #inj(t) tr @yinj(t) @netinj(t) @scj(t) @yinj(t))#scj (t)   #
$ (37) No external errorflow  #
$ Errors are propagated back from units l to unit v along outgoing connections with weights wlv  #
$ This \external error" (note that for conventional units there is nothing but external error) at time t is #e v(t) = @yv(t) @netv(t) X l @netl(t + 1) @yv(t) #l(t + 1)   #
$ (38) We obtain @#e v(t ?1) @#j(t) = (39) @yv(t?1) @netv(t ?1) @#outj(t) @#j(t) @netoutj(t) @yv(t ?1) + @#inj(t) @#j(t) @netinj(t) @yv(t ?1) + @#cj(t) @#j(t) @netcj(t) @yv(t ?1) tr 0   #
$ We observe: the error #j arriving at the memory cell output is not backpropagated to units v via external connections to inj;outj;cj  #
$ Error flow within memory cells  #
$ We now focus on the error back flow within a memory cell's CEC  #
$ This is actually the only type of errorflow that can bridge several time steps  #
$ Suppose error #j(t) arrives at cj's output at time t and is propagated back for q steps until it reaches inj or the memory cell input g(netcj)  #
$ It is scaled by a factor of @#v(t?q) @#j(t) , where v = inj;cj  #
$ We rst compute @#scj (t ?q) @#j(t) tr 8 < : @ycj(t) @scj(t) q = 0 @scj(t?q+1) @scj(t?q) @#scj (t?q+1) @#j(t) q > 0   #
$ (40) Expanding equation (40), we obtain @#v(t ?q) @#j(t) tr @#v(t?q) @#scj (t?q) @#scj (t ?q) @#j(t) tr (41) @#v(t?q) @#scj (t?q) 1 Y m=q @scj(t?m + 1) @scj(t ?m) ! #
$ @ycj(t) @scj(t) tr youtj(t)h0(scj(t)) g0(netcj(t ?q)yinj(t?q) v = cj g(netcj(t?q)f0 inj(netinj(t?q)) v = inj   #
$ Consider the factors in the previous equation's last expression  #
$ Obviously, errorflow is scaled only at times t (when it enters the cell) and t ? #
$ q (when it leaves the cell), but not in between (constant errorflow through the CEC)  #
$ We observe: 29 (1) The output gate's e ect is: youtj(t) scales down those errors that can be reduced early during training without using the memory cell  #
$ Likewise, it scales down those errors resulting from using (activating/deactivating) the memory cell at later training stages | without the output gate, the memory cell might for instance suddenly start causing avoidable errors in situations that already seemed under control (because it was easy to reduce the corresponding errors without memory cells)  #
$ See \output weight con ict" and \abuse problem" in Sections 3/4  #
$ (2) If there are large positive or negative scj(t) values (because scj has drifted since time step t ?q), then h0(scj(t)) may be small (assuming that h is a logistic sigmoid)  #
$ See Section 4  #
$ Drifts of the memory cell's internal state scj can be countermanded by negatively biasing the input gate inj (see Section 4 and next point)  #
$ Recall from Section 4 that the precise bias value does not matter much  #
$ (3) yinj(t ?q) and f0 inj(netinj(t ?q)) are small if the input gate is negatively biased (assume finj is a logistic sigmoid)  #
$ However, the potential significance of this is negligible compared to the potential significance of drifts of the internal state scj  #
$ Some of the factors above may scale down LSTM's overall error flow, but not in a manner that depends on the length of the time lag  #
$ The flow will still be much more effective than an exponentially (of order q) decaying flow without memory cells  #





$ Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation Kyunghyun Cho Bart van Merri enboer Caglar Gulcehre Universite de Montreal firstname lastname@umontreal ca Dzmitry Bahdanau Jacobs University, Germany d bahdanau@jacobs-university de Fethi Bougares Holger Schwenk Universit e du Maine, France firstname lastname@lium univ-lemans fr Yoshua Bengio Universite de Montreal, CIFAR Senior Fellow find me@on the web Abstract In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN)  #
$ One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols  #
$ The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence  #
$ The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model  #
$ Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases  #
$ 1 Introduction Deep neural networks have shown great success in various applications such as objection recognition (see, e g , (Krizhevsky et al , 2012)) and speech recognition (see, e g , (Dahl et al , 2012))  #
$ Furthermore, many recent works showed that neural networks can be successfully used in a number of tasks in natural language processing (NLP)  #
$ These include, but are not limited to, language modeling (Bengio et al , 2003), paraphrase detection (Socher et al , 2011) and word embedding extraction (Mikolov et al , 2013)  #
$ In the field of statistical machine translation (SMT), deep neural networks have begun to show promising results  #
$ (Schwenk, 2012) summarizes a successful usage of feedforward neural networks in the framework of phrase-based SMT system  #
$ Along this line of research on using neural networks for SMT, this paper focuses on a novel neural network architecture that can be used as a part of the conventional phrase-based SMT system  #
$ The proposed neural network architecture, which we will refer to as an RNN Encoder-Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair  #
$ The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence  #
$ The two networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence  #
$ Additionally, we propose to use a rather sophisticated hidden unit in order to improve both the memory capacity and the ease of training  #
$ The proposed RNN Encoder-Decoder with a novel hidden unit is empirically evaluated on the task of translating from English to French  #
$ We train the model to learn the translation probability of an English phrase to a corresponding French phrase  #
$ The model is then used as a part of a standard phrase-based SMT system by scoring each phrase pair in the phrase table  #
$ The empirical evaluation reveals that this approach of scoring phrase pairs with an RNN Encoder-Decoder improves the translation performance  #
$ We qualitatively analyze the trained RNN Encoder-Decoder by comparing its phrase scores with those given by the existing translation model  #
$ The qualitative analysis shows that the RNN Encoder-Decoder is better at capturing the linguistic regularities in the phrase table, indirectly explaining the quantitative improvements in the overall translation performance  #
$ The further analysis of the model reveals that the RNN Encoder-Decoder learns a continuous space representation of a phrase that preserves both the semantic and syntactic structure of the phrase  #
$ arXiv:1406 1078v3 [cs CL] 3 Sep 2014 2 RNN Encoder-Decoder 2 1 Preliminary: Recurrent Neural Networks A recurrent neural network (RNN) is a neural network that consists of a hidden state h and an optional output y which operates on a variable-length sequence x = (x1,   #
$ , xT )  #
$ At each time step t, the hidden state h t of the RNN is updated by h t = f h t-1 , xt , (1) where f is a non-linear activation function  #
$ f may be as simple as an element-wise logistic sigmoid function and as complex as a long short-term memory (LSTM) unit (Hochreiter and Schmidhuber, 1997)  #
$ An RNN can learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence  #
$ In that case, the output at each timestep t is the conditional distribution p(xt | xt-1,   #
$ , x1)  #
$ For example, a multinomial distribution (1-of-K coding) can be output using a softmax activation function p(xt,j = 1 | xt-1,   #
$ , x1) = exp wjh t K j =1 exp wj h t , (2) for all possible symbols j = 1,   #
$ , K, where wj are the rows of a weight matrix W  By combining these probabilities, we can compute the probability of the sequence x using p(x) = T t=1 p(xt | xt-1,   #
$ , x1)  #
$ (3) From this learned distribution, it is straightforward to sample a new sequence by iteratively sam pling a symbol at each time step  #
$ 2 2 RNN Encoder-Decoder In this paper, we propose a novel neural network architecture that learns to encode a variable-length sequence into a fixed-length vector representation and to decode a given fixed-length vector representation back into a variable-length sequence  #
$ From a probabilistic perspective, this new model is a general method to learn the conditional distribution over a variable-length sequence conditioned on yet another variable-length sequence, e g  #
$ p(y1,   #
$ , yT | x1,   #
$ , xT ), where one x1 x2 xT yT' y2 y1 c Decoder Encoder Figure 1: An illustration of the proposed RNN Encoder-Decoder  #
$ should note that the input and output sequence lengths T and T may differ  #
$ The encoder is an RNN that reads each symbol of an input sequence x sequentially  #
$ As it reads each symbol, the hidden state of the RNN changes according to Eq  #
$ (1)  #
$ After reading the end of the sequence (marked by an end-of-sequence symbol), the hidden state of the RNN is a summary c of the whole input sequence  #
$ The decoder of the proposed model is another RNN which is trained to generate the output sequence by predicting the next symbol yt given the hidden state h t   #
$ However, unlike the RNN described in Sec  #
$ 2 1, both yt and h t are also conditioned on yt-1 and on the summary c of the input sequence  #
$ Hence, the hidden state of the decoder at time t is computed by, h t = f h t-1 , yt-1, c , and similarly, the conditional distribution of the next symbol is P(yt|yt-1, yt-2,   #
$ , y1, c) = g h t , yt-1, c   #
$ for given activation functions f and g (the latter must produce valid probabilities, e g  #
$ with a softmax)  #
$ See Fig  #
$ 1 for a graphical depiction of the proposed model architecture  #
$ The two components of the proposed RNN Encoder-Decoder are jointly trained to maximize the conditional log-likelihood max 1 N N n=1 log p(yn | xn), (4) where  is the set of the model parameters and each (xn, yn) is an (input sequence, output sequence) pair from the training set  #
$ In our case, as the output of the decoder, starting from the input, is differentiable, we can use a gradient-based algorithm to estimate the model parameters  #
$ Once the RNN Encoder-Decoder is trained, the model can be used in two ways  #
$ One way is to use the model to generate a target sequence given an input sequence  #
$ On the other hand, the model can be used to score a given pair of input and output sequences, where the score is simply a probability p(y | x) from Eqs  #
$ (3) and (4)  #
$ 2 3 Hidden Unit that Adaptively Remembers and Forgets In addition to a novel model architecture, we also propose a new type of hidden unit (f in Eq  #
$ (1)) that has been motivated by the LSTM unit but is much simpler to compute and implement 1 Fig  #
$ 2 shows the graphical depiction of the proposed hidden unit  #
$ Let us describe how the activation of the j-th hidden unit is computed  #
$ First, the reset gate rj is computed by rj =  [Wrx]j + Urh t-1 j , (5) where  is the logistic sigmoid function, and [  #
$ ]j denotes the j-th element of a vector  #
$ x and ht-1 are the input and the previous hidden state, respectively  #
$ Wr and Ur are weight matrices which are learned  #
$ Similarly, the update gate zj is computed by zj =  [Wzx]j + Uzh t-1 j   #
$ (6) The actual activation of the proposed unit hj is then computed by h t j = zjh t-1 j + (1 - zj) h t j , (7) where h t j =  [Wx]j + U r h t-1 j   #
$ (8) In this formulation, when the reset gate is close to 0, the hidden state is forced to ignore the previous hidden state and reset with the current input 1 The LSTM unit, which has shown impressive results in several applications such as speech recognition, has a memory cell and four gating units that adaptively control the information flow inside the unit, compared to only two gating units in the proposed hidden unit  #
$ For details on LSTM networks, see, e g , (Graves, 2012)  #
$ z r h h ~ x Figure 2: An illustration of the proposed hidden activation function  #
$ The update gate z selects whether the hidden state is to be updated with a new hidden state h  The reset gate r decides whether the previous hidden state is ignored  #
$ See Eqs  #
$ (5)-(8) for the detailed equations of r, z, h and h   only  #
$ This effectively allows the hidden state to drop any information that is found to be irrelevant later in the future, thus, allowing a more compact representation  #
$ On the other hand, the update gate controls how much information from the previous hidden state will carry over to the current hidden state  #
$ This acts similarly to the memory cell in the LSTM network and helps the RNN to remember long-term information  #
$ Furthermore, this may be considered an adaptive variant of a leaky-integration unit (Bengio et al , 2013)  #
$ As each hidden unit has separate reset and update gates, each hidden unit will learn to capture dependencies over different time scales  #
$ Those units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active  #
$ In our preliminary experiments, we found that it is crucial to use this new unit with gating units  #
$ We were not able to get meaningful result with an oft-used tanh unit without any gating  #
$ 3 Statistical Machine Translation In a commonly used statistical machine translation system (SMT), the goal of the system (decoder, specifically) is to find a translation f given a source sentence e, which maximizes p(f | e)  p(e | f)p(f), where the first term at the right hand side is called translation model and the latter language model (see, e g , (Koehn, 2005))  #
$ In practice, however, most SMT systems model log p(f | e) as a log-linear model with additional features and corresponding weights: log p(f | e) = N n=1 wnfn(f, e) + log Z(e), (9) where fn and wn are the n-th feature and weight, respectively  #
$ Z(e) is a normalization constant that does not depend on the weights  #
$ The weights are often optimized to maximize the BLEU score on a development set  #
$ In the phrase-based SMT framework introduced in (Koehn et al , 2003) and (Marcu and Wong, 2002), the translation model log p(e | f) is factorized into the translation probabilities of matching phrases in the source and target sentences 2 These probabilities are once again considered additional features in the log-linear model (see Eq  #
$ (9)) and are weighted accordingly to maximize the BLEU score  #
$ Since the neural net language model was proposed in (Bengio et al , 2003), neural networks have been used widely in SMT systems  #
$ In many cases, neural networks have been used to rescore translation hypotheses (n-best lists) (see, e g , (Schwenk et al , 2006))  #
$ Recently, however, there has been interest in training neural networks to score the translated sentence (or phrase pairs) using a representation of the source sentence as an additional input  #
$ See, e g , (Schwenk, 2012), (Son et al , 2012) and (Zou et al , 2013)  #
$ 3 1 Scoring Phrase Pairs with RNN Encoder-Decoder Here we propose to train the RNN Encoder-Decoder (see Sec  #
$ 2 2) on a table of phrase pairs and use its scores as additional features in the log-linear model in Eq  #
$ (9) when tuning the SMT decoder  #
$ When we train the RNN Encoder-Decoder, we ignore the (normalized) frequencies of each phrase pair in the original corpora  #
$ This measure was taken in order (1) to reduce the computational expense of randomly selecting phrase pairs from a large phrase table according to the normalized frequencies and (2) to ensure that the RNN Encoder-Decoder does not simply learn to rank the phrase pairs according to their numbers of occurrences  #
$ One underlying reason for this choice was that the existing translation probability in the phrase table already reflects the frequencies of the phrase 2 Without loss of generality, from here on, we refer to p(e | f) for each phrase pair as a translation model as well pairs in the original corpus  #
$ With a fixed capacity of the RNN Encoder-Decoder, we try to ensure that most of the capacity of the model is focused toward learning linguistic regularities, i e , distinguishing between plausible and implausible translations, or learning the "manifold" (region of probability concentration) of plausible translations  #
$ Once the RNN Encoder-Decoder is trained, we add a new score for each phrase pair to the existing phrase table  #
$ This allows the new scores to enter into the existing tuning algorithm with minimal additional overhead in computation  #
$ As Schwenk pointed out in (Schwenk, 2012), it is possible to completely replace the existing phrase table with the proposed RNN Encoder-Decoder  #
$ In that case, for a given source phrase, the RNN Encoder-Decoder will need to generate a list of (good) target phrases  #
$ This requires, however, an expensive sampling procedure to be performed repeatedly  #
$ In this paper, thus, we only consider rescoring the phrase pairs in the phrase table  #
$ 3 2 Related Approaches: Neural Networks in Machine Translation Before presenting the empirical results, we discuss a number of recent works that have proposed to use neural networks in the context of SMT  #
$ Schwenk in (Schwenk, 2012) proposed a similar approach of scoring phrase pairs  #
$ Instead of the RNN-based neural network, he used a feedforward neural network that has fixed-size inputs (7 words in his case, with zero-padding for shorter phrases) and fixed-size outputs (7 words in the target language)  #
$ When it is used specifically for scoring phrases for the SMT system, the maximum phrase length is often chosen to be small  #
$ However, as the length of phrases increases or as we apply neural networks to other variable-length sequence data, it is important that the neural network can handle variable-length input and output  #
$ The pro- posed RNN Encoder-Decoder is well-suited for these applications  #
$ Similar to (Schwenk, 2012), Devlin et al  #
$ (Devlin et al , 2014) proposed to use a feedforward neural network to model a translation model, however, by predicting one word in a target phrase at a time  #
$ They reported an impressive improvement, but their approach still requires the maxi- mum length of the input phrase (or context words) to be fixed a priori  #
$ Although it is not exactly a neural network they train, the authors of (Zou et al , 2013) proposed to learn a bilingual embedding of words/phrases  #
$ They use the learned embedding to compute the distance between a pair of phrases which is used as an additional score of the phrase pair in an SMT system  #
$ In (Chandar et al , 2014), a feedforward neural network was trained to learn a mapping from a bag-of-words representation of an input phrase to an output phrase  #
$ This is closely related to both the proposed RNN Encoder-Decoder and the model proposed in (Schwenk, 2012), except that their input representation of a phrase is a bag-of-words  #
$ A similar approach of using bag-of-words representations was proposed in (Gao et al , 2013) as well  #
$ Earlier, a similar encoder-decoder model using two recursive neural networks was proposed in (Socher et al , 2011), but their model was restricted to a monolingual setting, i e  #
$ the model reconstructs an input sentence  #
$ More recently, another encoder-decoder model using an RNN was proposed in (Auli et al , 2013), where the decoder is conditioned on a representation of either a source sentence or a source context  #
$ One important difference between the proposed RNN Encoder-Decoder and the approaches in (Zou et al , 2013) and (Chandar et al , 2014) is that the order of the words in source and target phrases is taken into account  #
$ The RNN Encoder-Decoder naturally distinguishes between sequences that have the same words but in a differ- ent order, whereas the aforementioned approaches effectively ignore order information  #
$ The closest approach related to the proposed RNN Encoder-Decoder is the Recurrent Continuous Translation Model (Model 2) proposed in (Kalchbrenner and Blunsom, 2013)  #
$ In their paper, they proposed a similar model that consists of an encoder and decoder  #
$ The difference with our model is that they used a convolutional n-gram model (CGM) for the encoder and the hybrid of an inverse CGM and a recurrent neural network for the decoder  #
$ They, however, evaluated their model on rescoring the n-best list proposed by the conventional SMT system and computing the perplexity of the gold standard translations  #
$ 4 Experiments We evaluate our approach on the English/French translation task of the WMT'14 workshop  #
$ 4 1 Data and Baseline System Large amounts of resources are available to build an English/French SMT system in the framework of the WMT'14 translation task  #
$ The bilingual corpora include Europarl (61M words), news commentary (5 5M), UN (421M), and two crawled corpora of 90M and 780M words respectively  #
$ The last two corpora are quite noisy  #
$ To train the French language model, about 712M words of crawled newspaper material is available in addition to the target side of the bitexts  #
$ All the word counts refer to French words after tokenization  #
$ It is commonly acknowledged that training statistical models on the concatenation of all this data does not necessarily lead to optimal performance, and results in extremely large models which are difficult to handle  #
$ Instead, one should focus on the most relevant subset of the data for a given task  #
$ We have done so by applying the data selection method proposed in (Moore and Lewis, 2010), and its extension to bi-texts (Axelrod et al , 2011)  #
$ By these means we selected a subset of 418M words out of more than 2G words for language modeling and a subset of 348M out of 850M words for training the RNN Encoder-Decoder  #
$ We used the test set newstest2012 and 2013 for data selection and weight tuning with MERT, and newstest2014 as our test set  #
$ Each set has more than 70 thousand words and a single reference translation  #
$ For training the neural networks, including the proposed RNN Encoder-Decoder, we limited the source and target vocabulary to the most frequent 15,000 words for both English and French  #
$ This covers approximately 93% of the dataset  #
$ All the out-of-vocabulary words were mapped to a special token ([UNK])  #
$ The baseline phrase-based SMT system was built using Moses with default settings  #
$ This system achieves a BLEU score of 30 64 and 33 3 on the development and test sets, respectively (see Table 1)  #
$ 4 1 1 RNN Encoder-Decoder The RNN Encoder-Decoder used in the experiment had 1000 hidden units with the proposed gates at the encoder and at the decoder  #
$ The input matrix between each input symbol x t and the hidden unit is approximated with two lower-rank matrices, and the output matrix is approximated Models BLEU dev test Baseline 30 64 33 30 RNN 31 20 33 87 CSLM + RNN 31 48 34 64 CSLM + RNN + WP 31 50 34 54 Table 1: BLEU scores computed on the development and test sets using different combinations of approaches  #
$ WP denotes a word penalty, where we penalizes the number of unknown words to neural networks  #
$ similarly  #
$ We used rank-100 matrices, equivalent to learning an embedding of dimension 100 for each word  #
$ The activation function used for h in Eq  #
$ (8) is a hyperbolic tangent function  #
$ The computation from the hidden state in the decoder to the output is implemented as a deep neural network (Pascanu et al , 2014) with a single intermediate layer having 500 maxout units each pooling 2 inputs (Goodfellow et al , 2013)  #
$ All the weight parameters in the RNN Encoder-Decoder were initialized by sampling from an isotropic zero-mean (white) Gaussian distribution with its standard deviation fixed to 0 01, except for the recurrent weight parameters  #
$ For the recurrent weight matrices, we first sampled from a white Gaussian distribution and used its left singular vectors matrix, following (Saxe et al , 2014)  #
$ We used Adadelta and stochastic gradient descent to train the RNN Encoder-Decoder with hyperparameters = 10-6 and  = 0 95 (Zeiler, 2012)  #
$ At each update, we used 64 randomly selected phrase pairs from a phrase table (which was created from 348M words)  #
$ The model was trained for approximately three days  #
$ Details of the architecture used in the experiments are explained in more depth in the supplementary material  #
$ 4 1 2 Neural Language Model In order to assess the effectiveness of scoring phrase pairs with the proposed RNN Encoder-Decoder, we also tried a more traditional approach of using a neural network for learning a target language model (CSLM) (Schwenk, 2007)  #
$ Especially, the comparison between the SMT system using CSLM and that using the proposed approach of phrase scoring by RNN Encoder-Decoder will clarify whether the contributions from multiple neural networks in different parts of the SMT system add up or are redundant  #
$ We trained the CSLM model on 7-grams from the target corpus  #
$ Each input word was projected into the embedding space R512, and they were concatenated to form a 3072-dimensional vector  #
$ The concatenated vector was fed through two rectified layers (of size 1536 and 1024) (Glorot et al , 2011)  #
$ The output layer was a simple softmax layer (see Eq  #
$ (2))  #
$ All the weight parameters were initialized uniformly between -0 01 and 0 01, and the model was trained until the validation perplexity did not improve for 10 epochs  #
$ After training, the language model achieved a perplexity of 45 80  #
$ The validation set was a random selection of 0 1% of the corpus  #
$ The model was used to score partial translations during the decoding process, which generally leads to higher gains in BLEU score than n-best list rescoring (Vaswani et al , 2013)  #
$ To address the computational complexity of using a CSLM in the decoder a buffer was used to aggregate n-grams during the stack-search performed by the decoder  #
$ Only when the buffer is full, or a stack is about to be pruned, the n-grams are scored by the CSLM  #
$ This allows us to perform fast matrix-matrix multiplication on GPU using Theano (Bergstra et al , 2010; Bastien et al , 2012)  #
$ RNN Scores (log) TM Scores (log) Figure 3: The visualization of phrase pairs accord- ing to their scores (log-probabilities) by the RNN Encoder-Decoder and the translation model  #
$ 4 2 Quantitative Analysis We tried the following combinations: 1  #
$ Baseline configuration 2  #
$ Baseline + RNN 3  #
$ Baseline + CSLM + RNN 4  #
$ Baseline + CSLM + RNN + Word penalty Source Translation Model RNN Encoder-Decoder Table 2: The top scoring target phrases for a small set of source phrases according to the translation model (direct translation probability) and by the RNN Encoder-Decoder  #
$ Source phrases were randomly selected from phrases with 4 or more words  #
$ denotes an incomplete (partial) character  #
$ r is a Cyrillic letter ghe  #
$ The results are presented in Table 1  #
$ As expected, adding features computed by neural networks consistently improves the performance over the baseline performance  #
$ The best performance was achieved when we used both CSLM and the phrase scores from the RNN Encoder-Decoder  #
$ This suggests that the contributions of the CSLM and the RNN Encoder-Decoder are not too correlated and that one can expect better results by improving each method independently  #
$ Furthermore, we tried penalizing the number of words that are unknown to the neural networks (i e  #
$ words which are not in the short-list)  #
$ We do so by simply adding the number of unknown words as an additional feature the log-linear model in Eq  #
$ (9) 3 However, in this case we 3 To understand the effect of the penalty, consider the set of all words in the 15,000 large shortlist, SL  #
$ All words xi / SL are replaced by a special token [UNK] before being scored by the neural networks  #
$ Hence, the conditional probability of any xi t / SL is actually given by the model as p (xt = [UNK] | x<t) = p (xt / SL | x<t) = x j t / SL p xj t | x<t  p xi t | x<t , where x<t is a shorthand notation for xt-1,   #
$ , x1  #
$ were not able to achieve better performance on the test set, but only on the development set  #
$ 4 3 Qualitative Analysis In order to understand where the performance improvement comes from, we analyze the phrase pair scores computed by the RNN Encoder-Decoder against the corresponding p(f | e) from the translation model  #
$ Since the existing translation model relies solely on the statistics of the phrase pairs in the corpus, we expect its scores to be better estimated for the frequent phrases but badly estimated for rare phrases  #
$ Also, as we mentioned earlier in Sec  #
$ 3 1, we further expect the RNN Encoder-Decoder which was trained without any frequency information to score the phrase pairs based rather on the linguistic regularities than on the statistics of their occurrences in the corpus  #
$ We focus on those pairs whose source phrase is long (more than 3 words per source phrase) and As a result, the probability of words not in the shortlist is always overestimated  #
$ It is possible to address this issue by backing off to an existing model that contain non-shortlisted words (see (Schwenk, 2007)) In this paper, however, we opt for introducing a word penalty instead, which counteracts the word probability overestimation  #
$ Source Samples from RNN Encoder-Decoder Table 3: Samples generated from the RNN Encoder-Decoder for each source phrase used in Table 2  #
$ We show the top-5 target phrases out of 50 samples  #
$ They are sorted by the RNN Encoder-Decoder scores  #
$ Figure 4: 2-D embedding of the learned word representation  #
$ The left one shows the full embedding space, while the right one shows a zoomed-in view of one region (color-coded)  #
$ For more plots, see the supplementary material  #
$ frequent  #
$ For each such source phrase, we look at the target phrases that have been scored high either by the translation probability p(f | e) or by the RNN Encoder-Decoder  #
$ Similarly, we perform the same procedure with those pairs whose source phrase is long but rare in the corpus  #
$ Table 2 lists the top-3 target phrases per source phrase favored either by the translation model or by the RNN Encoder-Decoder  #
$ The source phrases were randomly chosen among long ones having more than 4 or 5 words  #
$ In most cases, the choices of the target phrases by the RNN Encoder-Decoder are closer to actual or literal translations  #
$ We can observe that the RNN Encoder-Decoder prefers shorter phrases in general  #
$ Interestingly, many phrase pairs were scored similarly by both the translation model and the RNN Encoder-Decoder, but there were as many other phrase pairs that were scored radically different (see Fig  #
$ 3)  #
$ This could arise from the proposed approach of training the RNN Encoder-Decoder on a set of unique phrase pairs, discouraging the RNN Encoder-Decoder from learning simply the frequencies of the phrase pairs from the corpus, as explained earlier  #
$ Furthermore, in Table 3, we show for each of the source phrases in Table 2, the generated samples from the RNN Encoder-Decoder  #
$ For each source phrase, we generated 50 samples and show the top-five phrases accordingly to their scores  #
$ We can see that the RNN Encoder-Decoder is able to propose well-formed target phrases without looking at the actual phrase table  #
$ Importantly, the generated phrases do not overlap completely with the target phrases from the phrase table  #
$ This encourages us to further investigate the possibility of replacing the whole or a part of the phrase table Figure 5: 2-D embedding of the learned phrase representation  #
$ The top left one shows the full representation space (5000 randomly selected points), while the other three figures show the zoomed-in view of specific regions (color-coded)  #
$ with the proposed RNN Encoder-Decoder in the future  #
$ 4 4 Word and Phrase Representations Since the proposed RNN Encoder-Decoder is not specifically designed only for the task of machine translation, here we briefly look at the properties of the trained model  #
$ It has been known for some time that continuous space language models using neural networks are able to learn semantically meaningful embeddings (See, e g , (Bengio et al , 2003; Mikolov et al , 2013))  #
$ Since the proposed RNN Encoder-Decoder also projects to and maps back from a sequence of words into a continuous space vector, we expect to see a similar property with the proposed model as well  #
$ The left plot in Fig  #
$ 4 shows the 2-D embedding of the words using the word embedding matrix learned by the RNN Encoder-Decoder  #
$ The projection was done by the recently proposed Barnes-Hut-SNE (van der Maaten, 2013)  #
$ We can clearly see that semantically similar words are clustered with each other (see the zoomed-in plots in Fig  #
$ 4)  #
$ The proposed RNN Encoder-Decoder naturally generates a continuous-space representation of a phrase  #
$ The representation (c in Fig  #
$ 1) in this case is a 1000-dimensional vector  #
$ Similarly to the word representations, we visualize the representations of the phrases that consists of four or more words using the Barnes-Hut-SNE in Fig  #
$ From the visualization, it is clear that the RNN Encoder-Decoder captures both semantic and syntactic structures of the phrases  #
$ For instance, in the bottom-left plot, most of the phrases are about the duration of time, while those phrases that are syntactically similar are clustered together  #
$ The bottom-right plot shows the cluster of phrases that are semantically similar (countries or regions)  #
$ On the other hand, the top-right plot shows the phrases that are syntactically similar  #
$ 5 Conclusion In this paper, we proposed a new neural network architecture, called an RNN Encoder-Decoder that is able to learn the mapping from a sequence of an arbitrary length to another sequence, possibly from a different set, of an arbitrary length  #
$ The proposed RNN Encoder-Decoder is able to either score a pair of sequences (in terms of a conditional probability) or generate a target sequence given a source sequence  #
$ Along with the new architecture, we proposed a novel hidden unit that includes a reset gate and an update gate that adaptively control how much each hidden unit remembers or forgets while reading/generating a sequence  #
$ We evaluated the proposed model with the task of statistical machine translation, where we used the RNN Encoder-Decoder to score each phrase pair in the phrase table  #
$ Qualitatively, we were able to show that the new model is able to capture linguistic regularities in the phrase pairs well and also that the RNN Encoder-Decoder is able to propose well-formed target phrases  #
$ The scores by the RNN Encoder-Decoder were found to improve the overall translation performance in terms of BLEU scores  #
$ Also, we found that the contribution by the RNN Encoder-Decoder is rather orthogonal to the existing approach of using neural networks in the SMT system, so that we can improve further the performance by using, for instance, the RNN Encoder-Decoder and the neural net language model together  #
$ Our qualitative analysis of the trained model shows that it indeed captures the linguistic regularities in multiple levels i e  #
$ at the word level as well as phrase level  #
$ This suggests that there may be more natural language related applications that may benefit from the proposed RNN Encoder-Decoder  #
$ The proposed architecture has large potential for further improvement and analysis  #
$ One approach that was not investigated here is to replace the whole, or a part of the phrase table by letting the RNN Encoder-Decoder propose target phrases  #
$ Also, noting that the proposed model is not limited to being used with written language, it will be an important future research to apply the proposed architecture to other applications such as speech transcription  #
